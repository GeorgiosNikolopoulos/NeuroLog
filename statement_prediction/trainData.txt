"Multiple hibernate.cfg.xml files found in the persistence bundle.  Using the first one discovered."
"prepending / to %s. It should be placed in the root of the classpath rather than in a package.",configurationResourceName
"Creating EhCacheRegionFactory from a specified resource: %s.  Resolved to URL: %s",configurationResourceName,url
"Unknown multi tenancy strategy [ " + strategyName + " ], using MultiTenancyStrategy.NONE."
message,this
"Value generator specified column value in reference to multi-column attribute [%s -> %s]; ignoring",mappingProperty.getPersistentClass(),mappingProperty.getName()
"Standard access requested to CDI BeanManager : " + beanManager
"Error resolving CDI bean [%s] - using fallback"
"Error destroying managed bean instance [%s] - the context is not active anymore." + " The instance must have been destroyed already - ignoring.",instance,e
"Error resolving CDI bean [%s] - using fallback"
"Error resolving CDI bean [%s] - using fallback"
"Extended access requested to CDI BeanManager : " + beanManager
"Creating ManagedBean(%s) using direct instantiation",beanType.getName()
"Closing logical connection"
"Logical connection closed"
"Manually reconnected logical connection"
"Registering statement [%s]",statement
"Releasing statement [{0}]",statement
"Releasing result set [%s]",resultSet
"Closing result set [%s]",resultSet
"Unable to release JDBC result set [%s]",e.getMessage()
"Closing prepared statement [%s]",statement
"Exception clearing maxRows/queryTimeout [%s]",sqle.getMessage()
"Unable to release JDBC statement [%s]",e.getMessage()
"Registering result set [%s]",resultSet
"ResultSet statement was not registered (on register)"
"Request to release Blob, but appears no Blobs have ever been registered"
"Request to release Clob, but appears no Clobs have ever been registered"
"Request to release NClob, but appears no NClobs have ever been registered"
"Releasing JDBC resources"
"Unable to free JDBC Blob reference [%s]",e.getMessage()
"Unable to free JDBC Clob reference [%s]",e.getMessage()
"Unable to free JDBC NClob reference [%s]",e.getMessage()
"`hibernate.connection.provider_disables_autocommit` was enabled.  This setting should only be " + "enabled when you are certain that the Connections given to Hibernate by the " + "ConnectionProvider have auto-commit disabled.  Enabling this setting when the " + "Connections do not have auto-commit disabled will lead to Hibernate executing " + "SQL operations outside of any JDBC/SQL transaction."
"Skipping aggressive release of JDBC Connection after-statement due to held resources"
"Initiating JDBC connection release from afterStatement"
"Closing logical connection"
"Logical connection closed"
"LogicalConnection#afterStatement"
"LogicalConnection#afterTransaction"
"Preparing to begin transaction via JDBC Connection.setAutoCommit(false)"
"Transaction begun via JDBC Connection.setAutoCommit(false)"
"Preparing to commit transaction via JDBC Connection.commit()"
"Transaction committed via JDBC Connection.commit()"
"re-enabling auto-commit on JDBC Connection after completion of JDBC-based transaction"
"Could not re-enable auto-commit on JDBC Connection after completion of JDBC-based transaction : " + e
"Preparing to rollback transaction via JDBC Connection.rollback()"
"Transaction rolled-back via JDBC Connection.rollback()"
"Unable to ascertain initial auto-commit state of provided connection; assuming auto-commit"
"SynchronizationRegistryStandardImpl.notifySynchronizationsBeforeTransactionCompletion"
"SynchronizationRegistryStandardImpl.notifySynchronizationsAfterTransactionCompletion(%s)",status
"Clearing local Synchronizations"
"Calling UserTransaction#begin"
"Called UserTransaction#begin"
"Skipping TransactionManager#begin due to already active transaction"
"Calling UserTransaction#commit"
"Called UserTransaction#commit"
"Skipping TransactionManager#commit due to not being initiator"
"Calling UserTransaction#rollback"
"Called UserTransaction#rollback"
"Surrounding JTA transaction suspended [%s]",surroundingTransaction
"Surrounding JTA transaction resumed [%s]",surroundingTransaction
"Calling TransactionManager#begin"
"Called TransactionManager#begin"
"Skipping TransactionManager#begin due to already active transaction"
"Calling TransactionManager#commit"
"Called TransactionManager#commit"
"Skipping TransactionManager#commit due to not being initiator"
"Calling TransactionManager#rollback"
"Called TransactionManager#rollback"
"JTA platform says we cannot currently register synchronization; skipping"
"Hibernate RegisteredSynchronization successfully registered with JTA platform"
"JTA transaction was already joined (RegisteredSynchronization already registered)"
"Unable to access UserTransaction, attempting to use TransactionManager instead"
"Unable to access TransactionManager, attempting to use UserTransaction instead"
"JtaPlatform#retrieveUserTransaction returned null"
"JtaPlatform#retrieveUserTransaction threw an exception [%s]",ignore.getMessage()
"JtaPlatform#retrieveTransactionManager returned null"
"JtaPlatform#retrieveTransactionManager threw an exception [%s]",ignore.getMessage()
"DdlTransactionIsolatorJtaImpl#prepare: JtaPlatform -> %s",jtaPlatform
"DdlTransactionIsolatorJtaImpl#prepare: TransactionManager -> %s",tm
"DdlTransactionIsolatorJtaImpl#prepare: suspended Transaction -> %s",this.suspendedTransaction
"Synchronization coordinator: afterCompletion(status=%s)",status
"Registered JTA Synchronization : beforeCompletion()"
"Registered JTA Synchronization : afterCompletion(%s)",status
"Synchronization coordinator: beforeCompletion()"
"Synchronization coordinator: afterCompletion(status=%s)",status
"Synchronization coordinator: doAfterCompletion(successful=%s, delayed=%s)",successful,delayed
"was unable to reset connection back to auto-commit"
"ResourceLocalTransactionCoordinatorImpl#afterBeginCallback"
"ResourceLocalTransactionCoordinatorImpl#beforeCompletionCallback"
"ResourceLocalTransactionCoordinatorImpl#afterCompletionCallback(%s)",successful
"On commit, transaction was marked for roll-back only, rolling back"
"Throwing RollbackException on roll-back of transaction marked rollback-only on commit"
"Encountered failure rolling back failed commit",e
"Encountered failure rolling back failed commit",e2
"JDBC transaction marked for rollback-only (exception provided for stack trace)",NEWException("exception just for purpose of providing stack trace")
"Making sub-graph : ( (%s) %s )",type.getName(),getAttributeName()
"Adding sub-graph : ( (%s) %s )",subGraph.getGraphedType().getName(),getAttributeName()
"Adding sub-graph [%s] over-wrote existing [%]",subGraph,previous
"Making key sub-graph : ( (%s) %s )",type.getName(),getAttributeName()
"Adding key sub-graph : ( (%s) %s )",subType.getName(),getAttributeName()
"Adding key sub-graph [%s] over-wrote existing [%]",subGraph,previous
"Unable to save generated class %1$s",unloadedClass.getTypeDescription().getName(),e
"Reflection optimizer disabled for %s [%s: %s (property %s)]",clazz.getName(),StringHelper.unqualify(t.getClass().getName()),t.getMessage(),setterNamesLBRACKETindexRBRACKET
"Reflection optimizer disabled for %s [%s: %s]",clazz.getName(),StringHelper.unqualify(t.getClass().getName()),t.getMessage()
"Persistent fields for entity %s: %s",managedCtClass.getName(),Arrays.toString(orderedFields)
"Found @MappedSuperclass %s to collectPersistenceFields",managedCtSuperclass
"Skipping enhancement of [%s]: it's an interface!",managedCtClass.getName()
"Skipping enhancement of [%s]: already enhanced",managedCtClass.getName()
"Enhancing [%s] as Entity",managedCtClass.getName()
"Enhancing [%s] as Composite",managedCtClass.getName()
"Enhancing [%s] as MappedSuperclass",managedCtClass.getName()
"Extended enhancement of [%s]",managedCtClass.getName()
"Skipping enhancement of [%s]: not entity or composite",managedCtClass.getName()
"Weaving in PersistentAttributeInterceptable implementation on [%s]",managedCtClass.getName()
"Could not find bi-directional association for field [%s#%s]",managedCtClass.getName(),persistentField.getName()
"Bi-directional association for field [%s#%s] not managed: @ManyToMany in java.util.Map attribute not supported ",managedCtClass.getName(),persistentField.getName()
"Could not find type of bi-directional association for field [%s#%s]",managedCtClass.getName(),persistentField.getName()
"mappedBy association for field [%s#%s] is [%s#%s]",target.getDeclaringType().asErasure().getName(),target.getName(),targetEntity.getName(),f.getName()
"Extended enhancement: Transforming access to field [%s.%s] from method [%s#%s]",field.getType().asErasure(),field.getName(),field.getName(),name
"Writing getter method [%s] into [%s] for field [%s]",name,target.getName(),field
"Writing setter method [%s] into [%s] for field [%s]",name,target.getName(),field
"mappedBy association for field [%s#%s] is [%s#%s]",persistentField.getDeclaringClass().getName(),persistentField.getName(),targetEntity.getName(),f.getName()
"Persistent fields for entity %s: %s",managedCtClass.getName(),Arrays.toString(orderedFields)
"Found @MappedSuperclass %s to collectPersistenceFields",managedCtSuperclass.getName()
"Could not find the superclass of %s",managedCtClass
"Could not find type of bi-directional association for field [%s#%s]",managedCtClass.getName(),persistentField.getName()
"Could not find bi-directional association for field [%s#%s]",managedCtClass.getName(),persistentField.getName()
"Bi-directional association for field [%s#%s] not managed: @ManyToMany in java.util.Map attribute not supported ",managedCtClass.getName(),persistentField.getName()
"Extended enhancement: Transforming access to field [%s.%s] from method [%s#%s]",fieldClassName,fieldName,aCtClass.getName(),methodName
"Wrote field into [%s]: @Transient private transient %s %s;",target.getName(),type.getName(),name
"An error occurs closing InputStream for class [%s]",aClass.getName()
"Skipping enhancement of [%s]: it's an interface!",managedCtClass.getName()
"Skipping enhancement of [%s]: already enhanced",managedCtClass.getName()
"Enhancing [%s] as Entity",managedCtClass.getName()
"Enhancing [%s] as Composite",managedCtClass.getName()
"Enhancing [%s] as MappedSuperclass",managedCtClass.getName()
"Extended enhancement of [%s]",managedCtClass.getName()
"Skipping enhancement of [%s]: not entity or composite",managedCtClass.getName()
"Weaving in PersistentAttributeInterceptable implementation on [%s]",managedCtClass.getName()
"Forcing column [%s] to be non-null as it is part of the primary key for table [%s]",column.getCanonicalName(),getTableNameForLogging(column)
"Forcing column [%s] to be non-null as it is part of the primary key for table [%s]",column.getCanonicalName(),getNameIdentifier().getCanonicalName()
"No alter strings for table : %s",getQuotedName()
"version is BinaryType; changing to RowVersionType"
"Unable to close temporary session used to load lazy collection associated to no session"
"Statistics#queryCacheHit( `%s`, `%s` )",hql,regionName
"Statistics#queryCacheMiss( `%s`, `%s` )",hql,regionName
"Statistics#queryCachePut( `%s`, `%s` )",hql,regionName
"Statistics initialized [enabled=%s]",enabled
"Fetching initial value: %s",sql
"First free id: %s",previousValueHolder.makeValue()
"ForeignGenerator detected a transient entity [%s]",foreignValueSourceType.getAssociatedEntityName()
"Use of 'parameters' config setting is no longer supported; " + "to specify initial-value or increment use the " + "org.hibernate.id.enhanced.SequenceStyleGenerator generator instead."
"Sequence identifier generated: %s",result
"Natively generated identity: %s",id
LOG.unableToUpdateHiValue(tableName),sqle
"Creating hilo optimizer (legacy) with [incrementSize={0}; returnClass={1}]",incrementSize,returnClass.getName()
"Sequence value obtained: %s",value.makeValue()
"No optimizer specified, using NONE as default"
"Unknown optimizer key [%s]; returning null assuming Optimizer impl class name",externalName
"Creating pooled optimizer with [incrementSize={0}; returnClass={1}]",incrementSize,returnClass.getName()
err
"could not read a hi value",sqle
"Creating hilo optimizer with [incrementSize={0}; returnClass={1}]",incrementSize,returnClass.getName()
"binding parameter [%s] - [%s]",1,segmentValue
"Registering IdentifierGenerator strategy [%s] -> [%s]",strategy,generatorClass.getName()
"    - overriding [%s]",previous.getName()
"Wrapping up metadata context..."
"Starting entity [" + safeMapping.getEntityName() + ]
"Completed entity [" + safeMapping.getEntityName() + ]
"Starting mapped superclass [" + safeMapping.getMappedClass().getName() + ]
"Completed mapped superclass [" + safeMapping.getMappedClass().getName() + ]
"Building old-school composite identifier [" + ownerType.getJavaType().getName() + ]
"Building attribute [" + ownerType.getName() + "." + property.getName() + "]"
"Building identifier attribute [" + ownerType.getName() + "." + property.getName() + "]"
"Building version attribute [ownerType.getTypeName()" + "." + "property.getName()]"
"Starting attribute metadata determination [" + attributeContext.getPropertyMapping().getName() + "]"
"    Determined member [" + member + "]"
"    Determined type [name=" + type.getName() + ", class=" + type.getClass().getName() + "]"
"Applying named entity graph [name=%s, entity-name=%s, jpa-entity-name=%s",definition.getRegisteredName(),definition.getEntityName(),definition.getJpaEntityName()
"EntityGraph being replaced on EntityManagerFactory for name %s",graphName
"Cache region factory : %s",regionFactory.getClass().getName()
"Cannot default RegionFactory based on registered strategies as `%s` RegionFactory strategies were registered",implementors
"Evicting second-level cache: %s",MessageHelper.infoString(entityDescriptor,identifier,sessionFactory)
"Evicting entity cache: %s",navigableRole.getFullPath()
"Evicting natural-id cache: %s",rootEntityRole.getFullPath()
"Evicting second-level cache: %s",MessageHelper.collectionInfoString(collectionDescriptor,ownerIdentifier,sessionFactory)
"Evicting second-level cache: %s",navigableRole.getFullPath()
"Evicting query cache, region: %s",cache.getRegion().getName()
"Evicting cache of all query regions."
"Caching query results in region: %s; timestamp=%s",cacheRegion.getName(),session.getTransactionStartTimestamp()
"key.hashCode=" + key.hashCode()
"querySpaces=" + querySpaces
"Unexpected returnTypes is " + (returnTypesEQEQnullQUES"null":"empty") + "! result" + (resultEQEQnullQUES" is null":".size()=" + result.size())
"unexpected returnTypes is " + returnTypeInfo.toString() + "! result"
"Checking cached query results in region: %s",cacheRegion.getName()
"Query results were not found in cache"
"Cached query results were not up-to-date"
"Returning cached query results"
"tuple is null; returnTypes is %s",returnTypesEQEQnullQUES"null":"Type[" + returnTypes.length + "]"
"Unexpected result tuple! tuple is null; should be Object[" + returnTypes.length + "]!"
"Unexpected result tuple! tuple is null; returnTypes is " + (returnTypesEQEQnullQUES"null":"empty")
"tuple is Object[%s]; returnTypes is %s",tuple.length,returnTypesEQEQnullQUES"null":"Type[" + returnTypes.length + "]"
"Unexpected tuple length! transformer= expected=" + returnTypes.length + " got=" + tuple.length
"Unexpected tuple value type! transformer= expected=" + returnTypesLBRACKETjRBRACKET.getReturnedClass().getName() + " got=" + tupleLBRACKETjRBRACKET.getClass().getName()
"RegionFactory impl [%s] did not provide constructor accepting Properties",strategyClass.getName()
"RegionFactory impl [%s] did not provide constructor accepting Properties",strategyClass.getName()
"Evict CollectionRegion " + role
"Evict CollectionRegion " + collectionPersister.getRole() + " for id " + id
"TimestampsRegionAccess#preInvalidate - disabled"
"TimestampsRegionAccess#invalidate - disabled"
"TimestampsRegionAccess#isUpToDate - disabled"
"Pre-invalidating space [%s], timestamp: %s",space,ts
"Invalidating space [%s], timestamp: %s",space,ts
"[%s] last update timestamp: %s",space,lastUpdate + ", result set timestamp: " + timestamp
"Clearing cache data map [region=`%s`]",region.getName()
"DomainDataRegion created [%s]; key-factory = %s",regionConfig.getRegionName(),effectiveKeysFactory
"Generating entity cache access [%s] : %s",accessType.getExternalName(),namedEntityRole
"Generating entity natural-id access [%s] : %s",accessType.getExternalName(),namedEntityRole
"Generating collection cache access : %s",namedCollectionRole
"Getting cached data from region [`%s` (%s)] by key [%s]",getRegion().getName(),getAccessType(),key
"Cache miss : region = `%s`, key = `%s`",getRegion().getName(),key
"Cache hit : region = `%s`, key = `%s`",getRegion().getName(),key
"Cache hit, but item is unreadable/invalid : region = `%s`, key = `%s`",getRegion().getName(),key
"Caching data from load [region=`%s` (%s)] : key[%s] -> value[%s]",getRegion().getName(),getAccessType(),key,value
"Cache put-from-load [region=`%s` (%s), key=`%s`, value=`%s`] failed due to being non-writable",getAccessType(),getRegion().getName(),key,value
"Locking cache item [region=`%s` (%s)] : `%s` (timeout=%s, version=%s)",getRegion().getName(),getAccessType(),key,timeout,version
"Unlocking cache item [region=`%s` (%s)] : %s",getRegion().getName(),getAccessType(),key
"Cached entry expired : " + key
"Skipping #remove call in read-write access to maintain SoftLock : %s",key
"Checking readability of read-write cache item [timestamp=`%s`, version=`%s`] : txTimestamp=`%s`",(Object)timestamp,version,txTimestamp
"Checking writeability of read-write cache item [timestamp=`%s`, version=`%s`] : txTimestamp=`%s`, newVersion=`%s`",timestamp,version,txTimestamp,newVersion
"Checking writeability of read-write cache lock [timeout=`%s`, lockId=`%s`, version=`%s`, sourceUuid=%s, multiplicity=`%s`, unlockTimestamp=`%s`] : txTimestamp=`%s`, newVersion=`%s`",timeout,lockId,version,sourceUuid,multiplicity,unlockTimestamp,txTimestamp,newVersion
"Illegal attempt to update item cached as read-only [%s]",key
"Illegal attempt to update item cached as read-only [%s]",key
"Interpreting BatchFetchStyle from setting : %s",setting
"Unable to interpret given setting [%s] as BatchFetchStyle",setting
"Result set row: %s",count
"Done processing result set ({0} rows)",count
"Total objects hydrated: {0}",hydratedObjectsSize
"Found row of collection: %s",MessageHelper.collectionInfoString(persister,collectionRowKey,getFactory())
"Result set contains (possibly empty) collection: %s",MessageHelper.collectionInfoString(persister,optionalKey,getFactory())
"Result set contains (possibly empty) collection: %s",MessageHelper.collectionInfoString(collectionPersister,key,getFactory())
"Result row: %s",StringHelper.toString(keys)
"Initializing object from ResultSet: %s",MessageHelper.infoString(persister,id,getFactory())
"Lock timeout [%s] requested but dialect reported to not support lock timeouts",lockOptions.getTimeOut()
"Bound [{0}] parameters total",col
"bindNamedParameters() %s -> %s [%s]",typedValue.getValue(),name,loc + startIndex
"Wrapping result set [%s]",rs
"Loading entity: %s",MessageHelper.infoString(persister,id,identifierType,getFactory())
"Done entity load"
"Loading collection element by index"
"Done entity load"
"Batch loading entity: %s",MessageHelper.infoString(persister,ids,getFactory())
"Done entity batch load"
"Loading collection: %s",MessageHelper.collectionInfoString(getCollectionPersisters()LBRACKET0RBRACKET,id,getFactory())
"Done loading collection"
"Batch loading collection: %s",MessageHelper.collectionInfoString(getCollectionPersisters()LBRACKET0RBRACKET,ids,getFactory())
"Done batch load"
"Unexpected querySpaces is {0}",(querySpacesEQEQnullQUESquerySpaces:"empty")
"querySpaces is {0}",querySpaces
"Static select for collection %s: %s",collectionPersister.getRole(),getSQLString()
"SQL-template for dynamic collection [%s] batch-fetching : %s",collectionPersister.getRole(),sqlTemplate
"Batch loading collection: %s",MessageHelper.collectionInfoString(getCollectionPersisters()LBRACKET0RBRACKET,ids,getFactory())
"Done batch load"
"Static select for one-to-many %s: %s",oneToManyPersister.getRole(),getSQLString()
"Loading collection: %s",MessageHelper.collectionInfoString(collectionPersister,id,getFactory())
"Done loading collection"
"Static select for collection %s: %s",collectionPersister.getRole(),getStaticLoadQuery().getSqlStatement()
"Found row of collection: %s",MessageHelper.collectionInfoString(collectionReference.getCollectionPersister(),collectionRowKey,context.getSession().getFactory())
"Result set contains (possibly empty) collection: %s",MessageHelper.collectionInfoString(collectionReference.getCollectionPersister(),optionalKey,context.getSession().getFactory())
"On call to EntityIdentifierReaderImpl#resolve, EntityKey was already known; " + "should only happen on root returns with an optional identifier specified"
"Initializing object from ResultSet: {0}",MessageHelper.infoString(concreteEntityPersister,id,session.getFactory())
"extracted ROWID value: {0}",rowId
"Total objects hydrated: {0}",numberOfHydratedObjects
"Skipping create subselects because there are fewer than 2 results, so query by key is more efficient.",getClass().getName()
"Limiting ResultSet processing to just %s rows",maxRows
"Processing result set"
"Starting ResultSet row #%s",count
"Done processing result set ({0} rows)",count
"Preparing collection initializer : %s",MessageHelper.collectionInfoString(persister,key,session.getFactory())
"processing queryspace " + querySpace.getUid()
NEWString(byteArrayOutputStream.toByteArray())
"Lock timeout [%s] requested but dialect reported to not support lock timeouts",lockOptions.getTimeOut()
"Bound [{0}] parameters total",col
"bindNamedParameters() %s -> %s [%s]",typedval.getValue(),name,loc + startIndex
"Wrapping result set [%s]",rs
"Building columnName->columnIndex cache"
"Building LoadPlan..."
"Pushing fetch source to stack : " + fetchSource
"Popped fetch owner from stack : " + last
"%s Finished root entity : %s",StringHelper.repeat("<<",fetchSourceStack.size()),entityDefinition.getEntityPersister().getEntityName()
"%s Starting entity identifier : %s",StringHelper.repeat(">>",fetchSourceStack.size()),entityIdentifierDefinition.getEntityDefinition().getEntityPersister().getEntityName()
"%s Finished entity identifier : %s",StringHelper.repeat("<<",fetchSourceStack.size()),entityIdentifierDefinition.getEntityDefinition().getEntityPersister().getEntityName()
"Pushing collection reference to stack : " + collectionReference
"Popped collection reference from stack : " + last
"%s Starting root collection : %s",StringHelper.repeat(">>",fetchSourceStack.size()),collectionDefinition.getCollectionPersister().getRole()
"%s Finished root collection : %s",StringHelper.repeat("<<",fetchSourceStack.size()),collectionDefinition.getCollectionPersister().getRole()
"%s Starting collection index graph : %s",StringHelper.repeat(">>",fetchSourceStack.size()),indexDefinition.getCollectionDefinition().getCollectionPersister().getRole()
"%s Finished collection index graph : %s",StringHelper.repeat("<<",fetchSourceStack.size()),indexDefinition.getCollectionDefinition().getCollectionPersister().getRole()
"%s Starting collection element graph : %s",StringHelper.repeat(">>",fetchSourceStack.size()),elementDefinition.getCollectionDefinition().getCollectionPersister().getRole()
"%s Finished collection element graph : %s",StringHelper.repeat("<<",fetchSourceStack.size()),elementDefinition.getCollectionDefinition().getCollectionPersister().getRole()
"%s Starting composite : %s",StringHelper.repeat(">>",fetchSourceStack.size()),compositionDefinition.getName()
"%s Starting attribute %s",StringHelper.repeat(">>",fetchSourceStack.size()),attributeDefinition
"%s Finishing up attribute : %s",StringHelper.repeat("<<",fetchSourceStack.size()),attributeDefinition
"Building LoadPlan..."
"Adding QuerySpace : uid = %s -> %s]",querySpace.getUid(),querySpace
toString(loadPlan,aliasResolutionContext)
"Starting processing of sql query [{0}]",sqlQuery
"Mapping alias [{0}] to entity-suffix [{1}]",alias,suffix
"Mapping alias [{0}] to collection-suffix [{1}]",alias,suffix
"Static select for entity %s [%s]: %s",entityName,lockMode,getSQLString()
"Static select for entity %s [%s:%s]: %s",entityName,lockOptions.getLockMode(),lockOptions.getTimeOut(),getSQLString()
"Static select for entity %s: %s",entityName,getSQLString()
"Batch loading entity: %s",MessageHelper.infoString(persister(),idsToLoad,session.getFactory())
"SQL-template for dynamic entity [%s] batch-fetching [%s] : %s",entityName,lockMode,sqlTemplate
"Done batch load"
"Static select for action %s on entity %s: %s",action,entityName,getSQLString()
"Batch loading entity: %s",MessageHelper.infoString(persister,ids,session.getFactory())
"Done entity batch load"
"Load request found matching entity in context, but it is scheduled for removal; returning null"
"Load request found matching entity in context, but the matched entity was of an inconsistent return type; returning null"
"Converting second-level cache entry [%s] into entity : %s",entry,MessageHelper.infoString(persister,entityId,factory)
"Cached Version : %s",version
"Static select for entity %s [%s:%s]: %s",getEntityName(),buildingParameters.getLockOptions().getLockMode(),buildingParameters.getLockOptions().getTimeOut(),getStaticLoadQuery().getSqlStatement()
"Static select for entity %s [%s]: %s",getEntityName(),buildingParameters.getLockMode(),getStaticLoadQuery().getSqlStatement()
"Static select for entity %s [%s:%s]: %s",getEntityName(),buildingParameters.getLockOptions().getLockMode(),buildingParameters.getLockOptions().getTimeOut(),getStaticLoadQuery().getSqlStatement()
"Static select for entity %s [%s]: %s",getEntityName(),buildingParameters.getLockMode(),getStaticLoadQuery().getSqlStatement()
"Batch loading entity: %s",MessageHelper.infoString(persister,ids,session.getFactory())
"Done entity batch load"
"Batch loading entity: %s",MessageHelper.infoString(persister,ids,getFactory())
"Done entity batch load"
"Done entity load : %s#%s",getEntityName(),id
"Translating order-by fragment [%s] for collection role : %s",collectionBinding.getOrderBy(),getRole()
"Translating many-to-many order-by fragment [%s] for collection role : %s",collectionBinding.getOrderBy(),getRole()
"Static SQL for collection: %s",getRole()
" Row insert: %s",getSQLInsertRowString()
" Row update: %s",getSQLUpdateRowString()
" Row delete: %s",getSQLDeleteRowString()
" One-shot delete: %s",getSQLDeleteString()
"Deleting collection: %s",MessageHelper.collectionInfoString(this,id,getFactory())
"Done deleting collection"
"Inserting collection: %s",MessageHelper.collectionInfoString(this,collection,id,session)
"Done inserting collection: %s rows inserted",count
"Collection was empty"
"Deleting rows of collection: %s",MessageHelper.collectionInfoString(this,collection,id,session)
"Done deleting collection rows: %s deleted",count
"No rows to delete"
"Inserting rows of collection: %s",MessageHelper.collectionInfoString(this,collection,id,session)
"Done inserting rows: %s inserted",count
"Updating rows of collection: %s#%s",navigableRole.getFullPath(),id
"Done updating rows: %s updated",count
"Initializing collection: %s using named query: %s",persister.getRole(),queryName
"Visiting attribute path : " + subPath.getFullPath()
"Property path deemed to be circular : " + subPath.getFullPath()
"Visiting index for collection :  " + currentPropertyPath.getFullPath()
"Encountered explicit discriminator mapping for joined inheritance"
"Skipping duplicate registration of path [{0}], existing type = [{1}], incoming type = [{2}]",path,existingType,type
"Skipped adding attribute [{1}] to base-type [{0}] as more than one sub-type defined the attribute using incompatible types (strictly speaking the attributes are not inherited); existing type = [{2}], incoming type = [{3}]",getEntityName(),path,existingType,type
"Initializing lazy properties of: {0}, field access: {1}",MessageHelper.infoString(this,id,getFactory()),fieldName
"Initializing lazy properties from datastore (triggered for `%s`)",fieldName
"Done initializing lazy properties"
"Initializing lazy properties from second-level cache"
"Done initializing lazy properties"
"Getting current persistent state for: {0}",MessageHelper.infoString(this,id,getFactory())
"resolving unique key [%s] to identifier for entity [%s]",key,getEntityName()
"Forcing version increment [" + MessageHelper.infoString(this,id,getFactory()) + "; " + getVersionType().toLoggableString(currentVersion,getFactory()) + " -> " + getVersionType().toLoggableString(nextVersion,getFactory()) + "]"
"Getting version: {0}",MessageHelper.infoString(this,id,getFactory())
"Dehydrating entity: {0}",MessageHelper.infoString(this,id,getFactory())
String.format("binding parameter [%s] as ROWID - [%s]",index,rowId)
"Hydrating entity: {0}",MessageHelper.infoString(this,id,getFactory())
"Inserting entity: {0} (native id)",getEntityName()
"Version: {0}",Versioning.getVersion(fields,this)
"Inserting entity: {0}",MessageHelper.infoString(this,id,getFactory())
"Version: {0}",Versioning.getVersion(fields,this)
"Updating entity: {0}",MessageHelper.infoString(this,id,getFactory())
"Existing version: {0} -> New version:{1}",oldVersion,fieldsLBRACKETgetVersionProperty()RBRACKET
"Deleting entity: {0}",MessageHelper.infoString(this,id,getFactory())
"Version: {0}",version
"Delete handled by foreign key constraint: {0}",getTableName(j)
"Static SQL for entity: %s",getEntityName()
" Lazy select (%s) : %s",entry.getKey(),entry.getValue()
" Version select: %s",sqlVersionSelectString
" Snapshot select: %s",sqlSnapshotSelectString
" Insert %s: %s",j,getSQLInsertStrings()LBRACKETjRBRACKET
" Update %s: %s",j,getSQLUpdateStrings()LBRACKETjRBRACKET
" Delete %s: %s",j,getSQLDeleteStrings()LBRACKETjRBRACKET
" Identity insert: %s",sqlIdentityInsertString
" Update by row id (all fields): %s",sqlUpdateByRowIdString
" Update by row id (non-lazy fields): %s",sqlLazyUpdateByRowIdString
" Insert-generated property select: %s",sqlInsertGeneratedValuesSelectString
" Update-generated property select: %s",sqlUpdateGeneratedValuesSelectString
"Fetching entity: {0}",MessageHelper.infoString(this,id,getFactory())
StringHelper.qualify(getEntityName(),propertyName) + " is dirty"
"Getting current natural-id snapshot state for: {0}",MessageHelper.infoString(this,id,getFactory())
"Resolving natural-id [%s] to id : %s ",Arrays.asList(naturalIdValues),MessageHelper.infoString(this)
"Ignoring subclass attribute definition [%s.%s] because it is defined in a superclass ",entityMetamodel.getName(),attributeDefinition.getName()
"Ignoring lock-options passed to named query loader"
"Loading entity: %s using named query: %s",persister.getEntityName(),queryName
"Refreshing transient {0}",MessageHelper.infoString(persister,id,this.getFactory())
"Initializing SessionFactoryRegistry : %s",this
"Registering SessionFactory: %s (%s)",uuid,nameEQEQnullQUES"<unnamed>":name
"Not binding SessionFactory to JNDI, no JNDI name configured"
"Attempting to bind SessionFactory [%s] to JNDI",name
"Unbinding SessionFactory from JNDI : %s",name
"Lookup: name=%s",name
"Lookup: uid=%s",uuid
"Not found: %s",uuid
sessionFactoryMap.toString()
"A factory was successfully bound to name: %s",evt.getNewBinding().getName()
"JNDI lookup: %s",name
"Resolved to UUID = %s",uuid
"Session creation specified 'autoJoinTransactions', which is invalid in conjunction " + "with sharing JDBC connection between sessions; ignoring"
"Session creation specified 'PhysicalConnectionHandlingMode which is invalid in conjunction " + "with sharing JDBC connection between sessions; ignoring"
"Skiping unhandled NativeSQLQueryReturn type : " + nativeSQLQueryReturn
"Serializing " + getClass().getSimpleName() + " ["
"Deserializing " + getClass().getSimpleName()
"Building session factory"
"Session factory constructed with filter configurations : %s",filters
"Instantiating session factory with properties: %s",properties
"Instantiated session factory"
"Already closed"
"Opening Hibernate Session.  tenant=%s",tenantIdentifier
"Serializing: %s",getUuid()
"Serialized"
"Deserializing"
"Deserialized: %s",getUuid()
"Resolving serialized SessionFactory"
"Resolved SessionFactory by UUID [%s]",uuid
"Resolved SessionFactory by name [%s]",name
"Deserializing SessionFactory from Session"
"Exception trying to cleanup load context : {0}",ignore.getMessage()
"Closing iterator"
"Attempting to retrieve next results"
"Exhausted results"
"Retrieved next results"
"Assembling results"
"Returning current results"
"Opened Session [%s] at timestamp: %s",getSessionIdentifier(),getTimestamp()
"Already closed"
"Closing session [%s]",getSessionIdentifier()
"On close, shared Session had before/after transaction actions that have not yet been processed"
"Forcing Session/EntityManager closed as SessionFactory/EntityManagerFactory has been closed"
"Skipping auto-flush due to session closed"
"Automatically flushing session"
"Automatically closing session"
"Disconnecting session"
"Reconnecting session"
"%s remove orphan before updates: [%s]",timing,entityEntryEQEQnullQUESentityName:MessageHelper.infoString(entityName,entityEntry.getId())
"Initializing proxy: %s",MessageHelper.infoString(persister,id,getFactory())
"Clearing effective entity graph for subsequent-select"
"Checking session dirtiness"
"Session dirty (scheduled updates and insertions)"
"Flushing to force deletion of re-saved object: %s",MessageHelper.infoString(entityEntry.getPersister(),entityEntry.getId(),getFactory())
"Scroll SQL query: {0}",customQuery.getSQL()
"SQL query: {0}",customQuery.getSQL()
"SessionImpl#beforeTransactionCompletion()"
"SessionImpl#afterTransactionCompletion(successful=%s, delayed=%s)",successful,delayed
"Cached natural-id/pk resolution linked to null EntityEntry in persistence context : " + MessageHelper.infoString(entityPersister,pk,getFactory())
"Property '" + propertyName + "' is not serializable, value won't be set."
"Property having key null is illegal; value won't be set."
"Serializing Session [%s]",getSessionIdentifier()
"Deserializing Session [%s]",getSessionIdentifier()
msg
"Listing entities:"
"More......"
toString(entityKeyAndEntity.getKey().getEntityName(),entityKeyAndEntity.getValue())
"Starting clone through serialization"
"Starting serialization of object [{0}]",obj
"Starting serialization of [uninitialized proxy]"
"Starting deserialization of object"
"Attempting to locate class [{0}]",className
"Unable to locate class using given classloader"
"Unable to locate class using given classloader"
"Unable to locate class using given classloader"
"Trying to resolve system-id [%s]",systemId
"Recognized hibernate namespace; attempting to resolve on classpath under org/hibernate/"
"Attempting to resolve on classpath under org/hibernate/"
"Recognized local namespace; attempting to resolve on classpath"
"Unable to locate [%s] on classpath",systemId
"Located [%s] in classpath",systemId
"Unable to locate [%s] on classpath",systemId
"Located [%s] in classpath",systemId
"Adding Integrator [%s].",integrator.getClass().getName()
"Adding insert with non-nullable, transient entities; insert=[{0}], dependencies=[{1}]",insert,dependencies.toLoggableString(insert.getSession())
"No entity insert actions have non-nullable, transient entity dependencies."
"No unresolved entity inserts that depended on [{0}]",MessageHelper.infoString(entityEntry.getEntityName(),entityEntry.getId())
"Unresolved inserts before resolving [{0}]: [{1}]",MessageHelper.infoString(entityEntry.getEntityName(),entityEntry.getId()),toString()
"Resolving insert [{0}] dependency on [{1}]",MessageHelper.infoString(dependentAction.getEntityName(),dependentAction.getId()),MessageHelper.infoString(entityEntry.getEntityName(),entityEntry.getId())
"Resolving insert [{0}] (only depended on [{1}])",dependentAction,MessageHelper.infoString(entityEntry.getEntityName(),entityEntry.getId())
"Unresolved inserts after resolving [{0}]: [{1}]",MessageHelper.infoString(entityEntry.getEntityName(),entityEntry.getId()),toString()
"Starting serialization of [{0}] unresolved insert entries",queueSize
"Starting deserialization of [{0}] unresolved insert entries",queueSize
"Unexpected ServiceRegistry type [%s] encountered during building of MetadataSources; may cause " + "problems later attempting to construct MetadataBuilder",serviceRegistry.getClass().getName()
"adding resource mappings from class convention : %s",entityClass.getName()
"Seeking mapping documents in jar file : %s",jar.getName()
"found mapping document : %s",zipEntry.getName()
"Problem closing schema stream [%s]",e.toString()
"Unable to close cfg.xml resource stream",e
"Unable to close cfg.xml URL stream",e
String.format("Unable to close properties file [%s] stream",resourceName),e
String.format("Unable to close properties file [%s] stream",file.getAbsolutePath()),e
"Listener [%s] defined as part of a group also defined event type",listener.getClazz()
"More than one cfg.xml file attempted to supply SessionFactory name: [%s], [%s].  Keeping initially discovered one [%s]",getSessionFactoryName(),incoming.getSessionFactoryName(),getSessionFactoryName()
"Performing JAXB binding of hbm.xml document : %s",origin.toString()
"Unable to close StAX reader",e
"Unable to close StAX reader",e
"Writing cache file for: %s to: %s",xmlFile.getAbsolutePath(),serFile.getAbsolutePath()
"Could not update cacheable hbm.xml bin file timestamp"
"Was unable to close input stream"
"Problem closing schema stream - %s",e.toString()
"Problem closing schema stream - %s",e.toString()
"In resolveEntity(%s, %s, %s, %s)",publicID,systemID,baseURI,namespace
"Interpreting namespace : %s",namespace
"Interpreting public/system identifier : [%s] - [%s]",publicID,systemID
"Recognized hibernate-mapping identifier; attempting to resolve on classpath under org/hibernate/"
"Recognized legacy hibernate-mapping identifier; attempting to resolve on classpath under org/hibernate/"
"Recognized legacy hibernate-mapping identifier; attempting to resolve on classpath under org/hibernate/"
"Recognized hibernate-configuration identifier; attempting to resolve on classpath under org/hibernate/"
"Recognized hibernate-configuration identifier; attempting to resolve on classpath under org/hibernate/"
"Recognized `classpath:` identifier; attempting to resolve on classpath [%s]",systemID
"Unable to resolve [%s] on classpath",systemID
"Resolved [%s] on classpath",systemID
"Created database namespace [logicalName=%s, physicalName=%s]",name.toString(),physicalName.toString()
"Checking auto-apply AttributeConverter [%s] (domain-type=%s) for match against %s : %s.%s (type=%s)",descriptor.getAttributeConverterClass().getName(),descriptor.getDomainValueResolvedType().getSignature(),conversionSite.getSiteDescriptor(),xProperty.getDeclaringClass().getName(),xProperty.getName(),xProperty.getType().getName()
"Unable to resolve class [%s] named in persistence unit [%s]",unresolvedListedClassName,scanEnvironment.getRootUrl()
"Calling MetadataContributor : %s",contributor
"Calling AdditionalJaxbMappingProducer : %s",producer
"filter-def [name=%s, origin=%s] defined multiple conditions, accepting arbitrary one",jaxbFilterDefinitionMapping.getName(),context.getOrigin().toString()
"Processed filter definition : %s",jaxbFilterDefinitionMapping.getName()
"Import (query rename): %s -> %s",rename,name
"Mapping class: %s -> %s",rootEntityDescriptor.getEntityName(),primaryTable.getName()
"Mapping joined-subclass: %s -> %s",entityDescriptor.getEntityName(),primaryTable.getName()
"Mapping union-subclass: %s -> %s",entityDescriptor.getEntityName(),primaryTable.getName()
"Mapping entity secondary-table: %s -> %s",persistentClass.getEntityName(),secondaryTable.getName()
"Property [%s.%s] referenced by property-ref [%s] was available - no need for delayed handling",referencedEntityName,referencedPropertyName,sourceElementSynopsis
"Property [%s.%s] referenced by property-ref [%s] was not yet available - creating delayed handler",handler.referencedEntityName,handler.referencedPropertyName,handler.sourceElementSynopsis
"Property [%s] specified %s generation, setting insertable to false : %s",propertySource.getName(),generationTiming.name(),mappingDocument.getOrigin()
"Property [%s] specified ALWAYS generation, setting updateable to false : %s",propertySource.getName(),mappingDocument.getOrigin()
message.toString()
"Binding virtual component [%s] to owner class [%s]",role,componentBinding.getOwner().getClassName()
"Binding virtual component [%s] as dynamic",role
"Binding component [%s]",role
"Binding component [%s] to explicitly specified class",role,explicitComponentClassName
"Attempting to determine component class by reflection %s",role
"Unable to determine component class name via reflection, and explicit " + "class name not given; role=[%s]",role
"Performing delayed property-ref handling [%s, %s, %s]",referencedEntityName,referencedPropertyName,sourceElementSynopsis
"Mapped collection : " + getPluralAttributeSource().getAttributeRole().getFullPath()
"   + table -> " + getCollectionBinding().getTable().getName()
"   + key -> " + columns(getCollectionBinding().getKey())
"   + index -> " + columns(((IndexedCollection)getCollectionBinding()).getIndex())
"   + one-to-many -> " + ((OneToMany)getCollectionBinding().getElement()).getReferencedEntityName()
"   + element -> " + columns(getCollectionBinding().getElement())
"Mapping collection: %s -> %s",collectionBinding.getRole(),collectionBinding.getCollectionTable().getName()
"Added virtual backref property [%s] : %s",prop.getName(),pluralAttributeSource.getAttributeRole().getFullPath()
"Binding [%s] element type for a [%s]",getPluralAttributeSource().getElementSource().getNature(),getPluralAttributeSource().getNature()
"Encountered filter with no name associated with many-to-many [%s]; skipping",getPluralAttributeSource().getAttributeRole().getFullPath()
"Applying many-to-many filter [%s] as [%s] to collection [%s]",filterSource.getName(),filterSource.getCondition(),getPluralAttributeSource().getAttributeRole().getFullPath()
"Binding natural-id UniqueKey for entity : " + entityBinding.getEntityName()
"Creating FetchProfile : %s",fetchProfileBinding.getName()
"Skipping HBM processing of entity hierarchy [%s], as at least one entity [%s] has been processed",entityHierarchy.getRoot().getEntityNamingSource().getEntityName(),entityName
"Entity super-type named as extends [%s] for subclass [%s:%s] not found",waitingListEntry.getKey(),waitingEntry.sourceMappingDocument.getOrigin(),waitingEntry.sourceMappingDocument.determineEntityName(waitingEntry.jaxbSubEntityMapping)
"Indexing mapping document [%s] for purpose of building entity hierarchy ordering",mappingDocument.getOrigin()
"Processing <identifier-generator/> : %s",identifierGenerator.getName()
"Processed type-definition : %s -> %s",definition.getName(),definition.getTypeImplementorClass().getName()
"Encountered a non-categorized annotated class [%s]; ignoring",annotatedClass.getName()
"Skipping annotated class processing of entity [%s], as it has already been processed",clazz
"Injecting JPA temp ClassLoader [%s] into BootstrapContext; was [%s]",jpaTempClassLoader,this.getJpaTempClassLoader()
"Injecting ScanOptions [%s] into BootstrapContext; was [%s]",scanOptions,this.scanOptions
"Injecting ScanEnvironment [%s] into BootstrapContext; was [%s]",scanEnvironment,this.scanEnvironment
"Injecting Scanner [%s] into BootstrapContext; was [%s]",scanner,this.scannerSetting
"Injecting ArchiveDescriptorFactory [%s] into BootstrapContext; was [%s]",factory,this.archiveDescriptorFactory
"Injecting Jandex IndexView [%s] into BootstrapContext; was [%s]",jandexView,this.jandexView
"ServiceRegistry passed to MetadataBuilder was a BootstrapServiceRegistry; this likely wont end well" + "if attempt is made to build SessionFactory"
e.getMessage() + "  Ignoring"
"Duplicate typedef name [%s] now -> %s",registrationKey,typeDefinition.getTypeImplementorClass().getName()
"Duplicated fetch profile with same name [" + profile.getName() + "] found."
"Import: {0} -> {1}",importName,entityName
"import name [" + importName + "] overrode previous [{" + old + "}]"
"Resolving reference to class: %s",referencedEntityName
"Processing association property references"
"ClassLoaderAccessImpl#injectTempClassLoader(%s) [was %s]",jpaTempClassLoader,this.jpaTempClassLoader
"Not known whether passed class name [%s] is safe",name
"No temp ClassLoader provided; using live ClassLoader " + "for loading potentially unsafe class : %s",name
"Temp ClassLoader was provided, so we will use that : %s",name
"reading mappings from resource : %s",resource
"reading mappings from file : %s",origin.getName()
"reading mappings from InputStreamAccess : %s",xmlInputStreamAccess.getStreamName()
"Unable to close InputStream obtained from InputStreamAccess : %s",xmlInputStreamAccess.getStreamName()
"reading mappings from InputStream"
"Reading mapping document from URL : %s",urlExternalForm
"JAR URL from URL Entry: " + url + " >> " + jarUrl
String.format("Registering named strategy selector [%s] : [%s] -> [%s]",strategy.getName(),name,implementation.getName())
String.format("Registering named strategy selector [%s] : [%s] -> [%s] (replacing [%s])",strategy.getName(),name,implementation.getName(),old.getName())
"Named strategy map did not exist on call to un-register"
"Child ServiceRegistry [%s] was already registered; this will end badly later...",child
"Implicitly destroying Boot-strap registry on de-registration " + "of all child ServiceRegistries"
"Skipping implicitly destroying Boot-strap registry on de-registration " + "of all child ServiceRegistries"
"trying via [new URL(\"%s\")]",name
"trying via [ClassLoader.getResourceAsStream(\"%s\")]",name
"trying via [new URL(\"%s\")]",stripped
"trying via [ClassLoader.getResourceAsStream(\"%s\")]",stripped
"Unable to close temporary session used to load lazy proxy associated to no session"
"Initialization failure [" + entityName + "#" + id + "]",e
logMessage,e
logMessage,t
message,t
logMessage,e
logMessage,t
message,t
"Using %s-based conversion for Enum %s",isOrdinal()QUES"ORDINAL":"NAMED",enumClass.getName()
"Created collection wrapper: %s",MessageHelper.collectionInfoString(persister,collection,key,session)
"Incoming session was null; using current jvm time"
"Falling back to vm-based timestamp, as dialect does not support current timestamp selection"
"Current timestamp retrieved from db : {0} (nanos={1}, time={2})",ts,ts.getNanos(),ts.getTime()
"Current timestamp retrieved from db : {0} (nanos={1}, time={2})",ts,ts.getNanos(),ts.getTime()
"Adding type registration %s -> %s",key,type
"JavaTypeDescriptorRegistry entry replaced : %s -> %s (was %s)",descriptor.getJavaType(),descriptor,old
"Could not find matching JavaTypeDescriptor for requested Java class [%s]; using fallback.  " + "This means Hibernate does not know how to perform certain basic operations in relation to this Java type." + "",cls.getName()
"Could not find matching scoped JavaTypeDescriptor for requested Java class [%s]; " + "falling back to static registry",javaType.getName()
"JavaTypeDescriptorRegistry entry replaced : %s -> %s (was %s)",descriptor.getJavaType(),descriptor,old
"Using  cached JavaTypeDescriptor instance for Java class [%s]",cls.getName()
"Created AttributeConverterTypeAdapter -> " + name
"Converted value on binding : %s -> %s",value,convertedValue
"Converted value on binding : %s -> %s",value,convertedValue
"Converted value on extraction: %s -> %s",extractedValue,convertedValue
"Unable to locate nationalized jdbc-code equivalent for given jdbc code : " + jdbcCode
String.format(NULL_BIND_MSG_TEMPLATE,index,JdbcTypeNameMapper.getTypeName(getSqlDescriptor().getSqlType()))
String.format(BIND_MSG_TEMPLATE,index,JdbcTypeNameMapper.getTypeName(sqlDescriptor.getSqlType()),getJavaDescriptor().extractLoggableRepresentation(value))
String.format(NULL_BIND_MSG_TEMPLATE,name,JdbcTypeNameMapper.getTypeName(getSqlDescriptor().getSqlType()))
String.format(BIND_MSG_TEMPLATE,name,JdbcTypeNameMapper.getTypeName(sqlDescriptor.getSqlType()),getJavaDescriptor().extractLoggableRepresentation(value))
"JDBC type code mapping not known for class [" + cls.getName() + "]; using custom code [" + specialCode + "]"
"Java Class mapping not known for JDBC type code [%s]; using java.lang.Object",typeCode
"extracted value ([%s] : [%s]) - [null]",name,JdbcTypeNameMapper.getTypeName(getSqlDescriptor().getSqlType())
"extracted value ([%s] : [%s]) - [%s]",name,JdbcTypeNameMapper.getTypeName(getSqlDescriptor().getSqlType()),getJavaDescriptor().extractLoggableRepresentation(value)
"extracted procedure output  parameter ([%s] : [%s]) - [null]",index,JdbcTypeNameMapper.getTypeName(getSqlDescriptor().getSqlType())
"extracted procedure output  parameter ([%s] : [%s]) - [%s]",index,JdbcTypeNameMapper.getTypeName(getSqlDescriptor().getSqlType()),getJavaDescriptor().extractLoggableRepresentation(value)
"extracted named procedure output  parameter ([%s] : [%s]) - [null]",paramName,JdbcTypeNameMapper.getTypeName(getSqlDescriptor().getSqlType())
"extracted named procedure output  parameter ([%s] : [%s]) - [%s]",paramName,JdbcTypeNameMapper.getTypeName(getSqlDescriptor().getSqlType()),getJavaDescriptor().extractLoggableRepresentation(value)
"A standard JDBC type code [%s] was not defined in SqlTypeDescriptorRegistry",jdbcTypeCode
"A standard JDBC type code [%s] was not defined in SqlTypeDescriptorRegistry",potentialAlternateTypeCode
"Different TypeConfiguration [%s] passed to #deregisterTypeConfiguration than previously registered [%s] under that UUID [%s]",typeConfiguration,existing,typeConfiguration.getUuid()
"Scoping TypeConfiguration [%s] to MetadataBuildingContext [%s]",this,metadataBuildingContext
"Scoping TypeConfiguration [%s] to SessionFactoryImpl [%s]",this,sessionFactory
"Handling #sessionFactoryClosed from [%s] for TypeConfiguration",factory
e,"Cannot register [%s] Hibernate Type contribution",basicType.getName()
"Un-scoping TypeConfiguration [%s] from SessionFactory [%s]",this,factory
"Resolving serialized TypeConfiguration - readResolve"
"Binding formula %s",formulaString
"Binding column: %s",toString()
"Column(s) overridden for property %s",inferredData.getPropertyName()
"Could not perform @ColumnDefault lookup as 'PropertyData' did not give access to XProperty"
"Building session factory using provided StandardServiceRegistry"
"Building session factory using internal StandardServiceRegistryBuilder"
"#makeIdGenerator(%s, %s, %s, %s, ...)",id,idXProperty,generatorType,generatorName
"Could not resolve explicit IdentifierGeneratorDefinition - using implicit interpretation (%s)",name
"Building implicit generic IdentifierGeneratorDefinition (%s) : %s",name,strategyName
"Binding Any Meta definition: %s",defAnn.name()
"SessionFactory name : %s",sessionFactoryOptions.getSessionFactoryName()
"Automatic flush during beforeCompletion(): %s",enabledDisabled(sessionFactoryOptions.isFlushBeforeCompletionEnabled())
"Automatic session close at end of transaction: %s",enabledDisabled(sessionFactoryOptions.isAutoCloseSessionEnabled())
"Statistics: %s",enabledDisabled(sessionFactoryOptions.isStatisticsEnabled())
"Deleted entity synthetic identifier rollback: %s",enabledDisabled(sessionFactoryOptions.isIdentifierRollbackEnabled())
"Default entity-mode: %s",sessionFactoryOptions.getDefaultEntityMode()
"Check Nullability in Core (should be disabled when Bean Validation is on): %s",enabledDisabled(sessionFactoryOptions.isCheckNullability())
"Allow initialization of lazy state outside session : %s",enabledDisabled(sessionFactoryOptions.isInitializeLazyStateOutsideTransactionsEnabled())
"Using BatchFetchStyle : %",sessionFactoryOptions.getBatchFetchStyle().name()
"Default batch fetch size: %s",sessionFactoryOptions.getDefaultBatchFetchSize()
"Maximum outer join fetch depth: %s",sessionFactoryOptions.getMaximumFetchDepth()
"Default null ordering: %s",sessionFactoryOptions.getDefaultNullPrecedence()
"Order SQL updates by primary key: %s",enabledDisabled(sessionFactoryOptions.isOrderUpdatesEnabled())
"Order SQL inserts for batching: %s",enabledDisabled(sessionFactoryOptions.isOrderInsertsEnabled())
"multi-tenancy strategy : %s",sessionFactoryOptions.getMultiTenancyStrategy()
"JTA Track by Thread: %s",enabledDisabled(sessionFactoryOptions.isJtaTrackByThread())
"Query language substitutions: %s",sessionFactoryOptions.getQuerySubstitutions()
"Named query checking : %s",enabledDisabled(sessionFactoryOptions.isNamedQueryStartupCheckingEnabled())
"Second-level cache: %s",enabledDisabled(sessionFactoryOptions.isSecondLevelCacheEnabled())
"Second-level query cache: %s",enabledDisabled(sessionFactoryOptions.isQueryCacheEnabled())
"Second-level query cache factory: %s",sessionFactoryOptions.getTimestampsCacheFactory()
"Second-level cache region prefix: %s",sessionFactoryOptions.getCacheRegionPrefix()
"Optimize second-level cache for minimal puts: %s",enabledDisabled(sessionFactoryOptions.isMinimalPutsEnabled())
"Structured second-level cache entries: %s",enabledDisabled(sessionFactoryOptions.isStructuredCacheEntriesEnabled())
"Second-level cache direct-reference entries: %s",enabledDisabled(sessionFactoryOptions.isDirectReferenceCacheEntriesEnabled())
"Automatic eviction of collection cache: %s",enabledDisabled(sessionFactoryOptions.isAutoEvictCollectionCache())
"JDBC batch size: %s",sessionFactoryOptions.getJdbcBatchSize()
"JDBC batch updates for versioned data: %s",enabledDisabled(sessionFactoryOptions.isJdbcBatchVersionedData())
"Scrollable result sets: %s",enabledDisabled(sessionFactoryOptions.isScrollableResultSetsEnabled())
"Wrap result sets: %s",enabledDisabled(sessionFactoryOptions.isWrapResultSetsEnabled())
"JDBC3 getGeneratedKeys(): %s",enabledDisabled(sessionFactoryOptions.isGetGeneratedKeysEnabled())
"JDBC result set fetch size: %s",sessionFactoryOptions.getJdbcFetchSize()
"Connection release mode: %s",sessionFactoryOptions.getConnectionReleaseMode()
"Generate SQL with comments: %s",enabledDisabled(sessionFactoryOptions.isCommentsEnabled())
"JPA compliance - query : ",enabledDisabled(sessionFactoryOptions.getJpaCompliance().isJpaQueryComplianceEnabled())
"JPA compliance - closed-handling : ",enabledDisabled(sessionFactoryOptions.getJpaCompliance().isJpaClosedComplianceEnabled())
"JPA compliance - lists : ",enabledDisabled(sessionFactoryOptions.getJpaCompliance().isJpaListComplianceEnabled())
"JPA compliance - transactions : ",enabledDisabled(sessionFactoryOptions.getJpaCompliance().isJpaTransactionComplianceEnabled())
"Add sequence generator with name: {0}",idGen.getName()
"Add table generator with name: {0}",definitionBuilder.getName()
"Add sequence generator with name: {0}",definitionBuilder.getName()
"Add generic generator with name: {0}",definitionBuilder.getName()
"Binding entity from annotated class: %s",clazzToProcess.getName()
"Ignoring explicit DiscriminatorColumn annotation on ",clazzToProcess.getName()
"Applying implicit DiscriminatorColumn using DiscriminatorColumn defaults"
"Ignoring implicit (absent) DiscriminatorColumn"
"Subclass joined column(s) created"
"Binding filter definition: %s",def.getFilterName()
typeBindMessageF,defAnn.name()
typeBindMessageF,defAnn.defaultForType().getName()
"Setting discriminator for entity {0}",rootClass.getEntityName()
"Skipping attribute [%s : %s] as it was already processed as part of super hierarchy",inferredData.getClassOrElementName(),inferredData.getPropertyName()
"Processing annotations of {0}.{1}",propertyHolder.getEntityName(),inferredData.getPropertyName()
"{0} is a version property",inferredData.getPropertyName()
"Version name: {0}, unsavedValue: {1}",rootClass.getVersion().getName(),((SimpleValue)rootClass.getVersion().getValue()).getNullValue()
"Bind {0} on {1}",(isComponentQUES"@EmbeddedId":"@Id"),inferredData.getPropertyName()
"Binding component with path: {0}",subpath
"Fetching {0} with {1}",propertyName,fetchMode
"Resolving XML entity {0} : {1}",publicId,systemId
"Unable to locate [{0}] on classpath",systemId
"Located [{0}] in classpath",systemId
"Recognized JPA ORM namespace; attempting to resolve on classpath under org/hibernate/jpa"
"Attempting to locate auto-apply AttributeConverter for collection element [%s]",collection.getRole()
"Unable to resolve XClass [%s] to Class for collection elements [%s]",elementXClass.getName(),collection.getRole()
"Attempting to locate auto-apply AttributeConverter for collection key [%s]",collection.getRole()
"Unable to resolve XClass [%s] to Class for collection key [%s]",keyXClass.getName(),collection.getRole()
"Encountered formula definition; skipping"
"Second pass for collection: %s",collection.getRole()
msg
"Attempting to locate auto-apply AttributeConverter for property [%s:%s]",path,property.getName()
violation
"Unable to acquire Bean Validation ValidatorFactory, skipping activation"
"Skipping application of relational constraints from legacy Hibernate Validator"
"@NotNull was applied to attribute [%s] which is defined (at least partially) " + "by formula(s); formula portions will be skipped",property.getName()
"Collection role: %s",role
"Encountered deprecated @Sort annotation; use @SortNatural or @SortComparator instead."
"Binding a OneToMany: %s.%s through a foreign key",propertyHolder.getEntityName(),propertyName
"Mapping collection: %s -> %s",collection.getRole(),collection.getCollectionTable().getName()
"Binding a OneToMany: %s through an association table",path
"Binding as ManyToMany: %s",path
"Binding a ManyToAny: %s",path
"Binding a collection of element: %s",path
"MetadataSourceProcessor property %s with lazy=%s",name,lazy
"Building property %s",name
"Cascading {0} with {1}",name,cascade
"Binding result set mapping: %s",definition.getName()
"No column in the identifier!"
"Starting applyAttributeConverter [%s:%s]",persistentClassName,property.getName()
"Skipping AttributeConverter checks for Id attribute [%s]",property.getName()
"Skipping AttributeConverter checks for version attribute [%s]",property.getName()
"Skipping AttributeConverter checks for Temporal attribute [%s]",property.getName()
"Skipping AttributeConverter checks for map-key annotated as MapKeyTemporal [%s]",property.getName()
"Skipping AttributeConverter checks for Enumerated attribute [%s]",property.getName()
"Skipping AttributeConverter checks for map-key annotated as MapKeyEnumerated [%s]",property.getName()
"Skipping AttributeConverter checks for association attribute [%s]",property.getName()
"building SimpleValue for %s",propertyName
"Starting fillSimpleValue for %s",propertyName
"Applying JPA AttributeConverter [%s] to [%s:%s]",attributeConverterDescriptor,persistentClassName,propertyName
"Binding named query: %s => %s",queryDefinition.getName(),queryDefinition.getQueryString()
"Binding named native query: %s => %s",queryAnn.name(),queryAnn.query()
"Binding named native query: %s => %s",query.getName(),queryAnn.query()
"Binding named query: %s => %s",query.getName(),query.getQueryString()
"Bound named stored procedure query : %s => %s",def.getRegisteredName(),def.getProcedureName()
"Import with entity name %s",name
"Bind entity %s on table %s",persistentClass.getEntityName(),table.getName()
"Adding XML overriding information for %s",className
"Adding XML overriding information for listeners: %s",localAddedClasses
"Unable to close PreparedStatement",e
"Building Return [isResultSet=%s, updateCount=%s, extendedReturn=%s",isResultSet(),getUpdateCount(),hasExtendedReturns()
"Unable to locate %s implementation class %s",expected.getName(),candidate.toString()
"Unable to instantiate %s class %s",expected.getName(),target.getName()
"Binding : %s",name
"Intermediate lookup: {0}",intermediateContextName
"Found intermediate context: {0}",intermediateContextName
"Creating sub-context: {0}",intermediateContextName
"Binding : {0}",name
"Bound name: %s",name
"Not all entities were loaded."
"Entities of type [" + persister.getEntityName() + "] not found; IDs: " + idSet
"Seeding: %s",seed
"Using initial version: {0}",initialVersion
"Incrementing: %s to %s",versionType.toLoggableString(version,session.getFactory()),versionType.toLoggableString(next,session.getFactory())
"Setting proxy identifier: %s",id
"Initializing non-lazy collections"
"Detached object being merged (corresponding with a managed entity) has a collection that [%s] the detached child.",(foundQUES"contains":"does not contain")
"Detached proxy being merged has a collection that [%s] the managed child.",(foundQUES"contains":"does not contain")
"Detached proxy being merged has a collection that [%s] the detached child being merged..",(foundQUES"contains":"does not contain")
"A detached object being merged (corresponding to a parent in parentsByChild) has an indexed collection that [%s] the detached child being merged. ",(indexBANGEQnullQUES"contains":"does not contain")
"A detached object being merged (corresponding to a managed entity) has an indexed collection that [%s] the detached child being merged. ",(indexBANGEQnullQUES"contains":"does not contain")
"Serializing persistence-context"
"Starting serialization of [" + entitiesByKey.size() + "] entitiesByKey entries"
"Starting serialization of [" + entitiesByUniqueKey.size() + "] entitiesByUniqueKey entries"
"Starting serialization of [" + proxiesByKey.size() + "] proxiesByKey entries"
"Starting serialization of [" + entitySnapshotsByKey.size() + "] entitySnapshotsByKey entries"
"Starting serialization of [" + collectionsByKey.size() + "] collectionsByKey entries"
"Starting serialization of [" + collectionEntries.size() + "] collectionEntries entries"
"Starting serialization of [" + arrayHolders.size() + "] arrayHolders entries"
"Starting serialization of [" + size + "] nullifiableEntityKey entries"
"Deserializing persistence-context"
"Starting deserialization of [" + count + "] entitiesByKey entries"
"Starting deserialization of [" + count + "] entitiesByUniqueKey entries"
"Starting deserialization of [" + count + "] proxiesByKey entries"
"Encountered pruned proxy"
"Starting deserialization of [" + count + "] entitySnapshotsByKey entries"
"Starting deserialization of [" + count + "] collectionsByKey entries"
"Starting deserialization of [" + count + "] collectionEntries entries"
"Starting deserialization of [" + count + "] arrayHolders entries"
"Starting deserialization of [" + count + "] nullifiableEntityKey entries"
"Unexpected CachedNaturalIdValueSource [" + source + "]"
"Starting serialization of [%s] EntityEntry entries",count
"Starting deserialization of [%s] EntityEntry entries",count
"Enable to deserialize [%s]",entityEntryClassName
"Enable to deserialize [%s]",entityEntryClassName
"Enable to deserialize [%s]",entityEntryClassName
"Processing cascade {0} for: {1}",action,persister.getEntityName()
"Done processing cascade {0} for: {1}",action,persister.getEntityName()
"Deleting orphaned entity instance: {0}",description
"Cascade {0} for collection: {1}",action,collectionType.getRole()
"Done cascade {0} for collection: {1}",action,collectionType.getRole()
"Deleting orphans for collection: {0}",collectionType.getRole()
"Done deleting orphans for collection: {0}",collectionType.getRole()
"Deleting orphaned entity instance: {0}",entityName
"Session Metrics {\n" + "    %s nanoseconds spent acquiring %s JDBC connections;\n" + "    %s nanoseconds spent releasing %s JDBC connections;\n" + "    %s nanoseconds spent preparing %s JDBC statements;\n" + "    %s nanoseconds spent executing %s JDBC statements;\n" + "    %s nanoseconds spent executing %s JDBC batches;\n" + "    %s nanoseconds spent performing %s L2C puts;\n" + "    %s nanoseconds spent performing %s L2C hits;\n" + "    %s nanoseconds spent performing %s L2C misses;\n" + "    %s nanoseconds spent executing %s flushes (flushing a total of %s entities and %s collections);\n" + "    %s nanoseconds spent executing %s partial-flushes (flushing a total of %s entities and %s collections)\n" + "}",jdbcConnectionAcquisitionTime,jdbcConnectionAcquisitionCount,jdbcConnectionReleaseTime,jdbcConnectionReleaseCount,jdbcPrepareStatementTime,jdbcPrepareStatementCount,jdbcExecuteStatementTime,jdbcExecuteStatementCount,jdbcExecuteBatchTime,jdbcExecuteBatchCount,cachePutTime,cachePutCount,cacheHitTime,cacheHitCount,cacheMissTime,cacheMissCount,flushTime,flushCount,flushEntityCount,flushCollectionCount,partialFlushTime,partialFlushCount,partialFlushEntityCount,partialFlushCollectionCount
"Version: %s",versionStr
"Resolving attributes for %s",MessageHelper.infoString(persister,id,session.getFactory())
"Processing attribute `%s` : value = %s",propertyNamesLBRACKETiRBRACKET,valueEQEQLazyPropertyInitializer.UNFETCHED_PROPERTYQUES"<un-fetched>":valueEQEQPropertyAccessStrategyBackRefImpl.UNKNOWNQUES"<unknown>":value
"Resolving <un-fetched> attribute : `%s`",propertyNamesLBRACKETiRBRACKET
"Attribute (`%s`)  - enhanced for lazy-loading? - %s",propertyNamesLBRACKETiRBRACKET,isLazyEnhanced
"Skipping <unknown> attribute : `%s`",propertyNamesLBRACKETiRBRACKET
"Adding entity to second-level cache: %s",MessageHelper.infoString(persister,id,session.getFactory())
"Done materializing entity %s",MessageHelper.infoString(persister,id,session.getFactory())
"Overriding eager fetching using active fetch profile. EntityName: %s, associationName: %s, eager fetching: %s",entityName,associationName,overridingEager
"bindNamedParameters() %s -> %s [%s]",typedVal.getValue(),name,location + start
"Resolved natural key -> primary key resolution in session cache: " + persister.getRootEntityName() + "#[" + Arrays.toString(naturalIdValues) + "]"
"Collection dereferenced: %s",MessageHelper.collectionInfoString(loadedPersister,coll,entry.getLoadedKey(),session)
"Found collection with unloaded owner: %s",MessageHelper.collectionInfoString(entry.getLoadedPersister(),coll,entry.getLoadedKey(),session)
"Skipping uninitialized bytecode-lazy collection: %s",MessageHelper.collectionInfoString(persister,collection,ce.getCurrentKey(),session)
"Collection found: %s, was: %s (initialized)",MessageHelper.collectionInfoString(persister,collection,ce.getCurrentKey(),session),MessageHelper.collectionInfoString(ce.getLoadedPersister(),collection,ce.getLoadedKey(),session)
"Collection found: %s, was: %s (uninitialized)",MessageHelper.collectionInfoString(persister,collection,ce.getCurrentKey(),session),MessageHelper.collectionInfoString(ce.getLoadedPersister(),collection,ce.getLoadedKey(),session)
"Forcing collection initialization"
"Registering external cascade style [%s : %s]",name,cascadeStyle
"External cascade style registration [%s : %s] overrode base registration [%s]",name,cascadeStyle,old
"Collection dirty: %s",MessageHelper.collectionInfoString(loadedPersister.getRole(),getLoadedKey())
"Reset storedSnapshot to %s for %s",storedSnapshot,this
"Version unsaved-value strategy NULL"
"Version unsaved-value strategy UNDEFINED"
"Version unsaved-value strategy NEGATIVE"
"Version unsaved-value: {0}",value
"SubselectFetch query fragment: %s",subselectQueryFragment
"Adding an EntityInsertAction for [{0}] object",action.getEntityName()
"Adding insert with no non-nullable, transient entities: [{0}]",insert
"Adding insert with non-nullable, transient entities; insert=[{0}], dependencies=[{1}]",insert,nonNullableTransientDependencies.toLoggableString(insert.getSession())
"Executing insertions before resolved early-insert"
"Executing identity-insert immediately"
"Adding resolved non-early insert action."
"Adding an EntityIdentityInsertAction for [{0}] object",action.getEntityName()
"Changes must be flushed to space: %s",actionSpace
"Changes must be flushed to space: %s",space
"Serializing action-queue"
"Deserializing action-queue"
"Deserialized [{0}] entries",l.size()
"The batch containing " + latestBatches.size() + " statements could not be sorted after " + maxIterations + " iterations. " + "This might indicate a circular entity relationship."
"Parameters: {0}",print.toString(positionalParameterTypes,positionalParameterValues)
"Named parameters: {0}",print.toString(namedParameters)
"Setting effective graph state [%s] : %s",semantic.name(),graph
"Neither LOAD nor FETCH graph were found in properties"
"ID unsaved-value strategy ANY"
"ID unsaved-value strategy NONE"
"ID unsaved-value strategy NULL"
"ID unsaved-value strategy UNDEFINED"
"ID unsaved-value: {0}",value
"Cascading to delete: {0}",entityName
"Cascading to lock: {0}",entityName
"Cascading to refresh: {0}",entityName
"Cascading to evict: {0}",entityName
"Cascading to save or update: {0}",entityName
"Cascading to merge: {0}",entityName
"Cascading to persist: {0}",entityName
"Cascading to persist on flush: {0}",entityName
"Cascading to replicate: {0}",entityName
"Constructing collection load context for result set [{0}]",resultSet
"Returning loading collection: %s",MessageHelper.collectionInfoString(persister,key.getKey(),getSession().getFactory())
"Attempting to locate loading collection entry [{0}] in any result-set context",key
"Collection [{0}] not located in load context",key
"Collection [{0}] located in load context",key
"Starting attempt to find loading collection [{0}]",MessageHelper.collectionInfoString(persister.getRole(),key)
"Collection already initialized; ignoring"
"Collection not yet initialized; initializing"
"Found loading collection bound to current result set processing; reading row"
"Removing collection load entry [{0}]",lce
"No collections were found in result set for role: %s",persister.getRole()
"%s collections were found in result set for role: %s",count,persister.getRole()
"%s collections initialized for role: %s",count,persister.getRole()
"Ending loading collection [{0}]",lce
"Collection fully initialized: %s",MessageHelper.collectionInfoString(persister,loadingCollection,lce.getKey(),session)
"Caching collection: %s",MessageHelper.collectionInfoString(persister,lce.getCollection(),lce.getKey(),session)
"Refusing to add to cache due to enabled filters"
"Unable to resolve connection default schema",e
"Could not fetch the SequenceInformation from the database",e
"Database ->\n" + "       name : %s\n" + "    version : %s\n" + "      major : %s\n" + "      minor : %s",dbmd.getDatabaseProductName(),dbmd.getDatabaseProductVersion(),dbmd.getDatabaseMajorVersion(),dbmd.getDatabaseMinorVersion()
"Driver ->\n" + "       name : %s\n" + "    version : %s\n" + "      major : %s\n" + "      minor : %s",dbmd.getDriverName(),dbmd.getDriverVersion(),dbmd.getDriverMajorVersion(),dbmd.getDriverMinorVersion()
"JDBC version : %s.%s",dbmd.getJDBCMajorVersion(),dbmd.getJDBCMinorVersion()
"Unable to free CLOB created to test createClob() implementation : %s",ignore
"Unable to use Java 1.7 Connection#getSchema"
"Unable to use Java 1.7 Connection#getSchema"
"Unable to use Java 1.7 Connection#getSchema : An error occurred trying to resolve the connection default schema resolver: " + ignore.getMessage()
"Normalizing identifier quoting [%s]",identifier
"Forcing identifier [%s] to quoted for global quoting",identifier
"Forcing identifier [%s] to quoted as recognized reserved word",identifier
"Normalizing identifier quoting for catalog name [%s]",identifier
"Environment does not support catalogs; returning null"
"Rendering quoted identifier [%s] in upper case for use in DatabaseMetaData",identifier
"Rendering quoted identifier [%s] in lower case for use in DatabaseMetaData",identifier
"Rendering unquoted identifier [%s] in mixed case for use in DatabaseMetaData",identifier
"Rendering unquoted identifier [%s] in lower case for use in DatabaseMetaData",identifier
"Normalizing identifier quoting for schema name [%s]",identifier
"Normalizing identifier quoting for object name [%s]",identifier
"JDBC driver metadata reported database stores unquoted identifiers in neither upper, lower nor mixed case"
"JDBC driver metadata reported database stores unquoted identifiers in more than one case"
"JDBC driver metadata reported database stores quoted identifiers in neither upper, lower nor mixed case"
"JDBC driver metadata reported database stores quoted identifiers in more than one case"
"IdentifierCaseStrategy for both quoted and unquoted identifiers was set " + "to the same strategy [%s]; that will likely lead to problems in schema update " + "and validation if using quoted identifiers",unquotedCaseStrategy.name()
"Unable to free CLOB created to test createClob() implementation : %s",ignore
"Closing JDBC container [{0}]",this
"Starting after statement execution processing [{0}]",getConnectionReleaseMode()
"Skipping aggressive release due to manual disabling"
"Skipping aggressive release due to registered resources"
"Connection provider reports to not support aggressive release; overriding"
"Registering last query statement [{0}]",statement
"Closing prepared statement [{0}]",statement
"Exception clearing maxRows/queryTimeout [%s]",sqle.getMessage()
"Unable to release JDBC statement [%s]",e.getMessage()
"Closing result set [{0}]",resultSet
"Unable to release JDBC result set [%s]",e.getMessage()
"JDBC DatabaseMetaData class does not define supportsRefCursors method..."
"Unexpected error trying to gauge level of JDBC REF_CURSOR support : " + e.getMessage()
statement
message,sqlException
warnMessage
sqlException.getMessage()
introMessage,warning
description
message
"could not clear warnings",sqle
"could not clear warnings",sqle
"Unable to locate specified class [" + className + "]",cle
"Unable to instantiate specified class [" + implClass.getName() + "]",e
msg
"No driver class specified"
"Initializing Connection pool with %s Connections",builder.initialSize
"Adding %s Connections to the pool",numberToBeAdded
"Removing %s Connections from the pool",numberToBeRemoved
"Connection leak detected: there are " + allocationCount + " unclosed connections upon shutting down pool " + getUrl()
"Reusing batch statement"
"SQLException escaped proxy",e
"No batched statements to execute"
"Executing batch size: %s",batchPosition
"Find: {0}",getSourceQuery()
"Iterate: {0}",getSourceQuery()
"Iterate: {0}",getSourceQuery()
"Execute update: {0}",getSourceQuery()
"Unable to locate HQL query plan in cache; generating ({0})",queryString
"Located HQL query plan in cache ({0})",queryString
"Unable to locate collection-filter query plan in cache; generating ({0} : {1} )",collectionRole,filterString
"Located collection-filter query plan in cache ({0} : {1})",collectionRole,filterString
"Unable to locate native-sql query plan in cache; generating ({0})",spec.getQueryString()
"Located native-sql query plan in cache ({0})",spec.getQueryString()
"Cleaning QueryPlan Cache"
"No JtaPlatform was specified, checking resolver"
"No JtaPlatform was specified, checking resolver"
"No JtaPlatformResolver was specified, using default [%s]",StandardJtaPlatformResolver.CLASS.getName()
"Located JtaPlatformProvider [%s] provided JtaPlaform : %s",provider,providedPlatform
"WebSphere version : %s",webSphereEnvironment.getWebSphereVersion()
"TransactionImpl created on closed Session/EntityManager"
"On TransactionImpl creation, JpaCompliance#isJpaTransactionComplianceEnabled == %s",jpaCompliance.isJpaTransactionComplianceEnabled()
"begin"
"committing"
"rolling back"
"#setRollbackOnly called on a not-active transaction"
"Handling request to add collection fetch [{0}]",fetchAssociactionRole
"Skipping select-value with non-unique alias"
prefix + traceText
prefix + ruleName
prefix + ruleName
prefix + ruleName
msg
"Beginning parsing of order-by fragment : ",fragment
TokenPrinters.ORDERBY_FRAGMENT_PRINTER.showAsString(parser.getAST(),"--- {order-by fragment} ---")
"Initializing service [role={0}]",serviceBinding.getServiceRole().getName()
"NPE injecting service deps : " + service.getClass().getName()
"Child ServiceRegistry [%s] was already registered; this will end badly later...",child
"Implicitly destroying ServiceRegistry on de-registration " + "of all child ServiceRegistries"
"Skipping implicitly destroying ServiceRegistry on de-registration " + "of all child ServiceRegistries"
"Overriding existing service binding [" + serviceRole.getName() + "]"
"Preparing procedure call : %s",call
"Starting attempt resolve named result-set-mapping : %s",resultSetMappingName
"Found result-set-mapping : %s",mapping.traceLoggableFormat()
"Using heuristic type [%s] based on bind value [%s] as `bindType`",hibernateType,value
"Using explicit type [%s] as `bindType`",hibernateType,value
"Using type [%s] (based on TemporalType [%s] as `bindType`",hibernateType,clarifiedTemporalType
"Transformed: %s rows to: %s distinct results",list.size(),result.size()
"QueryTranslatorFactory: " + factory
prefix + traceText
prefix + ruleName
"compile() : The query is already compiled, skipping..."
"HQL: %s",hql
"SQL: %s",sql
TokenPrinters.SQL_TOKEN_PRINTER.showAsString(w.getAST(),"--- SQL AST ---")
"parse() - HQL: %s",hql
TokenPrinters.HQL_TOKEN_PRINTER.showAsString(hqlAst,"--- HQL AST ---")
e.toString(),e
message
message
"throwQueryException() : no errors"
prefix + ruleName
prefix + ruleName
"handleDotIdent() : new LT(3) token - %s",LT(1)
"weakKeywords() : new LT(1) token - %s",LT(1)
"weakKeywords() : new LT(1) token - %s",LT(1)
"Converting keyword [%s] following COLON to IDENT as an expected parameter name",nextToken.getText()
"handleDotIdent() : new LT(2) token - %s",LT(1)
"Registering discovered request to treat(%s as %s)",path,subclassName
prefix + traceText
prefix + ruleName
"createFromFilterElement() : processed filter FROM element."
"createFromJoinElement() : " + getASTPrinter().showAsString(fromElement,"-- join tree --")
"Creating entity-join FromElement [%s -> %s]",alias,entityPersister.getEntityName()
"handleWithFragment() : " + getASTPrinter().showAsString(hqlSqlWithNode,"-- with clause --")
"lookupProperty() %s => %s(%s)",dotNode.getPath(),rhs.getText(),lhs.getPath()
"Attempting to resolve property [{0}] as a non-qualified ref",identText
"processQuery() : %s",query.toStringTree()
"Derived SELECT clause created."
"Resolved : %s -> %s",this.getPath(),this.getText()
"collectionProperty() :  name=%s type=%s",name,type
"Creating elements for %s",path
"createFromElementInSubselect() : path = %s",path
"createFromElementInSubselect() : creating a new FROM element..."
"createFromElementInSubselect() : %s -> %s",path,fromElement
"createEntityJoin() : Implied multi-table entity join"
"createEntityAssociation() : One to many - path = %s role = %s associatedEntityName = %s",path,role,associatedEntityName
"createManyToMany() : path = %s role = %s associatedEntityName = %s",path,role,associatedEntityName
"addJoinByPathMap() : %s -> %s",path,destination.getDisplayText()
"Promoting [%s] to [%s]",elem,this
"addCollectionJoinFromElementByPath() : %s -> %s",path,destination
"Rendered scalar ID select column(s): " + buf
"Using non-qualified column reference [{0} -> ({1})]",path,ArrayHelper.toString(columns)
"toColumns(%s,%s) : subquery = %s",tableAlias,propertyName,subquery
"getOrderByClause() : Creating a new ORDER BY clause"
"%s : %s (%s) -> %s",fromClause,className,classAliasEQEQnullQUES"<no alias>":classAlias,tableAlias
"Attempt to disable subclass-inclusions : ",NEWException("Stack-trace source")
"Handling property dereference [{0} ({1}) -> {2} ({3})]",persister.getEntityName(),getClassAlias(),propertyName,propertyDeclarer
"Creating join for many-to-many elements for %s",path
"No FROM element found for the elements of collection join path %s, created %s",path,elem
"FROM element found for collection join path %s",path
"dereferenceCollection() : Created new FROM element for %s : %s",propName,elem
"dereferenceEntityJoin() : generating join for %s in %s (%s) parent = %s",propertyName,getFromElement().getClassName(),classAliasEQEQnullQUES"<no alias>":classAlias,ASTUtil.getDebugString(parent)
"%s() : correlated subquery",methodName
"dereferenceShortcut() : property %s in %s does not require a join.",propertyName,getFromElement().getClassName()
"Unresolved property path is now '%s'",dotNode.propertyPath
"Terminal getPropertyPath = [%s]",propertyPath
"getDataType() : %s -> %s",propertyPath,propertyType
"This dialect is unable to cascade the delete into the many-to-many join table" + " when the entity has multiple primary keys.  Either properly setup cascading on" + " the constraints or manually clear the associations prior to deleting the entities."
"setSQLValue() %s -> %s",text,value
"setConstantValue() %s -> %s %s",text,value,value.getClass().getName()
"processConstant() : Replacing '%s' with '%s'",constant.getText(),replacement
"Could not format incoming text [{0}] as a NUM_INT; assuming numeric overflow and attempting as NUM_LONG",text
String.format("Finding of query referenced tables is skipped because the feature is disabled. See %s",AvailableSettings.OMIT_JOIN_OF_SUPERCLASS_TABLES)
"Finding of query referenced tables is skipped because filters are enabled."
TokenPrinters.REFERENCED_TABLES_PRINTER.showAsString(query,"Tables referenced from query nodes:")
"Using FROM fragment [%s]",fromFragment
"Using unprocessed WHERE-fragment [%s]",whereFragment
"Using processed WHERE-fragment [%s]",fragment.getText()
"parsePath() : %s -> %s",path,ASTUtil.getDebugString(lhs)
"Compiling query"
"Unexpected query compilation problem",e
"HQL: %s",hql
"SQL: %s",sql
"Error attempting to export id-table [%s] : %s",creationStatement,e.getMessage()
"Unable to use JDBC Connection to create Statement",e
"Unable obtain JDBC Connection",e
"Error attempting to cleanup id-table : [%s]",e.getMessage()
"Unable to use JDBC Connection to create Statement",e
"Unable obtain JDBC Connection",e
"Generated ID-INSERT-SELECT SQL (multi-table delete) : {0}",idInsertSelect
"Generated ID-INSERT-SELECT SQL (multi-table update) : {0}",idInsertSelect
"unable to create temporary id table [" + e.getMessage() + "]"
description
message
"unable to drop temporary id table after use [" + e.getMessage() + "]"
"Success of batch update unknown: %s",batchPosition
"Unable to release generated current-session on failed synch registration",ignore
"Unable to rollback transaction for orphaned session",t
"Unable to close orphaned session",t
"Allowing invocation [%s] to proceed to real (non-transacted) session",methodName
"Allowing proxy invocation [%s] to proceed to real session",methodName
"Rendered criteria query -> %s",jpaqlString
"Binding for parameter [%s] did not define type",key
"Converting QueryParameterBinding to QueryParameterListBinding for given QueryParameter : %s",queryParameter
"registering named query definition [%s] overriding previously registered definition [%s]",name,previous
"registering named SQL query definition [%s] overriding previously registered definition [%s]",name,previous
"registering named procedure call definition [%s] overriding previously registered definition [%s]",name,previous
"Checking named query: %s",namedQueryDefinition.getName()
"Checking named SQL query: %s",namedSQLQueryDefinition.getName()
"Stored procedure [%s] IN/INOUT parameter [%s] not bound and `passNulls` was set to true; binding NULL",procedureCall.getProcedureName(),this
"Stored procedure [%s] IN/INOUT parameter [%s] not bound and `passNulls` was set to false; assuming procedure defines default value",procedureCall.getProcedureName(),this
"Starting createEntityManagerFactory for persistenceUnitName %s",persistenceUnitName
"Could not obtain matching EntityManagerFactoryBuilder, returning null"
"Attempting to obtain correct EntityManagerFactoryBuilder for persistenceUnitName : %s",persistenceUnitName
"Unable to locate persistence units",e
"Located and parsed %s persistence units; checking each",units.size()
"Checking persistence-unit [name=%s, explicit-provider=%s] against incoming persistence unit name [%s]",persistenceUnit.getName(),persistenceUnit.getProviderClassName(),persistenceUnitName
"Excluding from consideration due to name mis-match"
"Excluding from consideration due to provider mis-match"
"Found no matching persistence units"
"Starting createContainerEntityManagerFactory : %s",info.getPersistenceUnitName()
"Starting generateSchema : PUI.name=%s",info.getPersistenceUnitName()
"Starting generateSchema for persistenceUnitName %s",persistenceUnitName
"Could not obtain matching EntityManagerFactoryBuilder, returning false"
"Session was closed; nothing to do"
"javax.persistence.PersistenceUnitUtil.getIdentifier is only intended to work with enhanced entities " + "(although Hibernate also adapts this support to its proxies); " + "however the passed entity was not enhanced (nor a proxy).. may not be able to read identifier"
"Interpreting Hibernate FlushMode#ALWAYS to JPA FlushModeType#AUTO; may cause problems if relying on FlushMode#ALWAYS-specific behavior"
"Interpreting Hibernate FlushMode#MANUAL to JPA FlushModeType#COMMIT; may cause problems if relying on FlushMode#MANUAL-specific behavior"
"Attempting to interpret external setting [" + externalName + "] as FlushMode name"
"Attempting to interpret external setting [" + externalName + "] as FlushModeType name"
sb.toString()
"Found use of deprecated `%s` setting; use `%s` instead.",org.hibernate.cfg.AvailableSettings.APP_CLASSLOADER,org.hibernate.cfg.AvailableSettings.CLASSLOADERS
"Found JACC permission grant [%s] in properties, but no JACC context id was specified; ignoring"
"Attempting to parse persistence.xml file : %s",xmlUrl.toExternalForm()
"Persistence unit name from persistence.xml : %s",name
extractInfo(warn)
"Checking requested PersistenceProvider name [%s] against Hibernate provider names",requestedProviderName
"Integration provided explicit PersistenceProvider [%s]",integrationProviderName
"Persistence-unit [%s] requested PersistenceProvider [%s]",persistenceUnit.getName(),persistenceUnitRequestedProvider
"CallbackRegistry reported that Class [%s] already had %s callbacks registered; " + "assuming this means the class was mapped twice " + "(using hbm.xml entity-name support) - skipping subsequent registrations",entityClassName,callbackType.getCallbackAnnotation().getSimpleName()
"Adding %s as %s callback for entity %s",methodName,callbackType.getCallbackAnnotation().getSimpleName(),beanClass.getName()
"Adding %s as %s callback for entity %s",methodName,callbackType.getCallbackAnnotation().getSimpleName(),beanClass.getName()
"Adding %s as %s callback for entity %s",methodName,callbackType.getCallbackAnnotation().getSimpleName(),embeddableXClass.getName()
"Adding permission [%s] to role [%s]",grantedAction,permissionDeclaration.getRole()
"Ignoring call to addPermission on disabled JACC service"
"Ignoring call to checkPermission on disabled JACC service"
"Skipping JACC integration as it was not enabled"
"Using dialect defined converter"
"Attempting to construct instance of specified SQLExceptionConverter [{0}]",converterClassName
"Unregistering registered MBean [ON={0}]",objectName
"Unable to unregsiter registered MBean [ON=%s] : %s",objectName,e.toString()
"Attempting to release created MBeanServer"
"Changing the query timeout from " + timeoutInMilliseconds + " ms to " + timeoutInSeconds + " s, because HANA requires the timeout in seconds"
"An error occurred while trying to determine the value of the HANA parameter indexserver.ini / session / max_lob_prefetch_size. Using the default value " + maxLobPrefetchSizeDefault,e
"Unable to resolve Oracle CURSOR JDBC type code",e
"Loading entity: {0}",MessageHelper.infoString(persister,event.getEntityId(),factory)
"Entity proxy found in session cache"
"Ignoring NO_PROXY for to-one association with subclasses to honor laziness"
"Entity proxy found in session cache"
"Entity found in session cache"
"Creating new proxy for entity"
"Attempting to resolve: {0}",MessageHelper.infoString(persister,event.getEntityId(),session.getFactory())
"Resolved object in second-level cache: {0}",MessageHelper.infoString(persister,event.getEntityId(),session.getFactory())
"Object not resolved in any cache: {0}",MessageHelper.infoString(persister,event.getEntityId(),session.getFactory())
"Initializing collection {0}",MessageHelper.collectionInfoString(ceLoadedPersister,collection,ce.getLoadedKey(),source)
"Checking second-level cache"
"Collection initialized from cache"
"Collection not cached"
"Collection initialized"
"Disregarding cached version (if any) of collection due to enabled filters"
String.format("More than one representation of the same persistent entity being merged for: %s",MessageHelper.infoString(entityName,session.getIdentifier(managedEntity)))
String.format("Summary: number of %s entities with multiple representations merged: %d",entityName,count)
"No entity copies merged."
sb.toString()
"Session dirty"
"Session not dirty"
"Flushing session"
"Flushed: %s insertions, %s updates, %s deletions to %s objects",session.getActionQueue().numberOfInsertions(),session.getActionQueue().numberOfUpdates(),session.getActionQueue().numberOfDeletions(),persistenceContext.getNumberOfManagedEntities()
"Flushed: %s (re)creations, %s updates, %s removals to %s collections",session.getActionQueue().numberOfCollectionCreations(),session.getActionQueue().numberOfCollectionUpdates(),session.getActionQueue().numberOfCollectionRemovals(),persistenceContext.getCollectionEntriesSize()
"Processing flush-time cascades"
"Flushing entities and processing referenced collections"
"Processing unreferenced collections"
"Executing flush"
"Post flush"
"Resetting entity id attribute to null for foreign generator"
"Ignoring persistent instance"
"Saving transient instance"
"un-scheduling entity deletion [%s]",MessageHelper.infoString(persister,persister.getIdentifier(entity,source),source.getFactory())
"Generated identifier: %s, using strategy: %s",persister.getIdentifierType().toLoggableString(generatedId,source.getFactory()),persister.getIdentifierGenerator().getClass().getName()
"Saving {0}",MessageHelper.infoString(persister,id,source.getFactory())
"Calling onSave()"
"Insertion vetoed by onSave()"
"Persistent instance of: {0}",getLoggableName(entityName,entity)
"Deleted instance of: {0}",getLoggableName(entityName,entity)
"Transient instance of: {0}",getLoggableName(entityName,entity)
"Detached instance of: {0}",getLoggableName(entityName,entity)
"Updating immutable, deleted entity: {0}",MessageHelper.infoString(persister,entry.getId(),session.getFactory())
"Updating non-modifiable, deleted entity: {0}",MessageHelper.infoString(persister,entry.getId(),session.getFactory())
"Updating deleted entity: ",MessageHelper.infoString(persister,entry.getId(),session.getFactory())
"Updating entity: {0}",MessageHelper.infoString(persister,entry.getId(),session.getFactory())
"Found dirty properties [{0}] : {1}",MessageHelper.infoString(persister.getEntityName(),id),Arrays.toString(dirtyPropertyNames)
"Configured EntityCopyObserver strategy: %s",EntityCopyNotAllowedObserver.SHORT_NAME
"Configured EntityCopyObserver strategy: %s",EntityCopyAllowedObserver.SHORT_NAME
"Configured EntityCopyObserver strategy: %s",EntityCopyAllowedLoggedObserver.SHORT_NAME
"Configured EntityCopyObserver is a custom implementation of type %s",observerType.getName()
"Locking {0} in mode: {1}",MessageHelper.infoString(persister,entry.getId(),source.getFactory()),requestedLockMode
"Reassociating transient instance: {0}",MessageHelper.infoString(persister,id,event.getSession().getFactory())
"Entity was not persistent in delete processing"
"Deleting a persistent instance"
"Object was already deleted"
"Already handled transient entity; skipping"
"Deleting {0}",MessageHelper.infoString(persister,entityEntry.getId(),session.getFactory())
"Calling onDelete()"
"Deletion vetoed by onDelete()"
"Already refreshed"
"Refreshing transient {0}",MessageHelper.infoString(persister,id,source.getFactory())
"Refreshing ",MessageHelper.infoString(e.getPersister(),e.getId(),source.getFactory())
"Evicting {0}",MessageHelper.infoString(persister)
"Reassociated uninitialized proxy"
"Ignoring persistent instance"
"Object already associated with session: {0}",MessageHelper.infoString(entityEntry.getPersister(),savedId,factory)
"Saving transient instance"
"Updating detached instance"
"Immutable instance passed to performUpdate()"
"Updating {0}",MessageHelper.infoString(persister,event.getRequestedId(),event.getSession().getFactory())
"Updating {0}",MessageHelper.infoString(persister,event.getRequestedId(),source.getFactory())
"Calling onUpdate()"
"Update vetoed by onUpdate()"
"Uninitialized proxy passed to replicate()"
"Ignoring persistent instance passed to replicate()"
"Found existing row for {0}",MessageHelper.infoString(persister,id,source.getFactory())
"No need to replicate"
"No existing row, replicating new instance {0}",MessageHelper.infoString(persister,id,source.getFactory())
"Replicating changes to {0}",MessageHelper.infoString(persister,id,source.getFactory())
"Need to execute flush"
"Don't need to execute flush"
"Attempting to resolve: {0}#{1}",MessageHelper.infoString(persister),event.getNaturalIdValues()
"Resolved object in cache: {0}#{1}",MessageHelper.infoString(persister),event.getNaturalIdValues()
"Object not resolved in any cache: {0}#{1}",MessageHelper.infoString(persister),event.getNaturalIdValues()
"Evicting collection: %s",MessageHelper.collectionInfoString(ce.getLoadedPersister(),collection,ce.getLoadedKey(),getSession())
"Ignoring uninitialized proxy"
"Ignoring uninitialized enhanced-proxy"
"Already in merge process"
"Already in copyCache; setting in merge process"
"Ignoring persistent instance"
"Merging transient instance"
"Merging detached instance"
"Collection dereferenced while transient {0}",MessageHelper.collectionInfoString(role,ownerIdentifier,source.getFactory())
"Wrapped collection in role: {0}",collectionType.getRole()
"Encountered event listener [%s] for post-commit event [%s] " + "which did not implement the corresponding extended " + "listener contract [%s]",listener.getClass().getName(),getEventType().eventName(),extendedListenerContract.getName()
"Skipping SchemaExport as Action.NONE was passed"
"Skipping SchemaExport as no targets were specified"
"--drop or --create was used; prefer --action=none|create|drop|drop-and-create instead"
"--text or --quiet was used; prefer --target=none|(stdout|database|script)*"
"Interpreting UniqueConstraintSchemaUpdateStrategy from setting : %s",setting
"Unable to interpret given setting [%s] as UniqueConstraintSchemaUpdateStrategy",setting
"Skipping SchemaExport as no targets were specified"
"--text or --quiet was used; prefer --target=none|(stdout|database|script)*"
"Unable to resolve indicated Dialect resolution info (%s, %s, %s)",explicitDbName,explicitDbMajor,explicitDbMinor
"Problem releasing DatabaseInformation : " + e.getMessage()
"Attempting to resolve script source setting : %s",scriptSourceSettingString
"Attempting to resolve script source setting : %s",scriptTargetSettingString
"Problem releasing GenerationTarget [%s] : %s",target,e.getMessage()
"Problem releasing GenerationTarget [%s] : %s",target,e.getMessage()
e,"Error performing delayed DROP command [%s]",command
exception,"GenerationTarget encountered exception accepting command : %s",exception.getMessage()
"Problem releasing GenerationTarget [%s] : %s",target,e.getMessage()
"Problem releasing DatabaseInformation : " + e.getMessage()
"Unable to close file reader for generation script source"
"Specified schema generation script file [%s] did not exist for reading",file
"Unable to close file reader for generation script source"
"wasInitiallyAutoCommit=%s",wasInitiallyAutoCommit
"Was unable to reset JDBC connection to no longer be in auto-commit mode"
"Attempting to resolve writer for URL : " + url
"Exception calling File#createNewFile : " + e.toString()
"wasInitiallyAutoCommit=%s",wasInitiallyAutoCommit
"Was unable to reset JDBC connection to no longer be in auto-commit mode"
"Multiple schemas found with that name [%s.%s]",catalogName,schemaName
"%s << begin [level=%s, statement=%s]",statementName,level,this.statementTypeName
"%s : finishing up [level=%s, statement=%s]",statementName,level,this.statementTypeName
"%s >> end [level=%s, statement=%s]",statementName,level,this.statementTypeName
msg
msg
msg,e
msg
msg,e
msg,e
LOG.unableToInstantiateC3p0ConnectionPool(),e
"*** skipping test - " + message,NEWException()
"Cleaning up unfinished transaction"
BeforeClass.CLASS.getSimpleName() + ": " + getName()
Test.CLASS.getSimpleName() + ": " + method.getName()
"adding test " + Helper.extractTestName(frameworkMethod) + " [#" + testCount + "]"
"Overriding TCCL [%s] -> [%s]",originalTCCL,isolatedClassLoader
"Reverting TCCL [%s] -> [%s]",isolatedClassLoader,originalTCCL
"#inSession(action)"
"#inSession(action)"
"#inSession(action)"
"#inTransaction(action)"
"#inTransaction(action)"
"#inTransaction(action)"
"Open SessionFactory instances found prior to start of test class [%s]",testClass.getName()
"Open SessionFactory instances found after completion of test class [%s]; closing them",testClass.getName()
"org.hibernate.testing.cache.CachingRegionFactory should be only used for testing."
"Byteman rule triggered: sleeping a second"
"unexpected interruption",e
"Increment call count"
ignore.getMessage()
"Rollback failure",e
"Rollback failure",e
"Rollback failure",e
"Rollback failure",e
"Rollback failure",e
"Rollback failure",e
"Rollback failure",e
"Rollback failure",e
"Rollback failure",e
"Rollback failure",e
e.getMessage()
e,"Statement [%s] execution failed!",statement
"#inSession(SF,action)"
"Session opened, calling action"
"called action"
"Session closed (AutoCloseable)"
"#inSession(SF,action)"
"Session opened, calling action"
"Session closed (AutoCloseable)"
"#inTransaction(factory, action)"
"#inTransaction(factory, action)"
"inTransaction(session,action)"
"Started transaction"
"Calling action in txn"
"Called action - in txn"
ACTION_COMPLETED_TXN,e
"Rolling back transaction due to action error"
"Rolled back transaction due to action error"
"Rolling back transaction due to action error failed; throwing original error"
"Committing transaction after successful action execution - success"
"Committing transaction after successful action execution - failure"
"inTransaction(session,action)"
"Started transaction"
"Calling action in txn"
"Called action - in txn"
ACTION_COMPLETED_TXN,e
"Rolling back transaction due to action error"
"Rolled back transaction due to action error"
"Rolling back transaction due to action error failed; throwing original error"
"Committing transaction after successful action execution - success"
"Committing transaction after successful action execution - failure"
"#inSession(SF,action)"
"StatelessSession opened, calling action"
"called action"
"Session closed (AutoCloseable)"
"#inTransaction(factory, action)"
"inTransaction(session,action)"
"Started transaction"
"Calling action in txn"
"Called action - in txn"
ACTION_COMPLETED_TXN,e
"Rolling back transaction due to action error"
"Rolled back transaction due to action error"
"Rolling back transaction due to action error failed; throwing original error"
"Committing transaction after successful action execution - success"
"Committing transaction after successful action execution - failure"
"Skipping Hibernate bytecode enhancement plugin execution since no feature is enabled"
"Skipping Hibernate enhancement plugin execution since there is no classes dir " + dir
"Skipping Hibernate enhancement plugin execution since there are no classes to enhance on " + dir
"Starting Hibernate enhancement for classes on " + dir
"Extended enhancement is enabled. Classes other than entities may be modified. You should consider access the entities using getter/setter methods and disable this property. Use at your own risk."
"Successfully enhanced class [" + file + "]"
"Adding classpath entry for classes root " + file.getAbsolutePath()
msg
"Adding classpath entry for dependency " + a.getId()
msg
"Non-insertable property %s.%s will be made insertable because a matching @AuditMappedBy was found in the %s entity",referencedEntityName,propertyName,entityName
"Generating first-pass auditing mapping for entity %s",entityName
"Generating second-pass auditing mapping for entity %s",entityName
"Adding audit mapping for property %s.%s: one-to-many collection, using a join column on the referenced entity",referencingEntityName,propertyName
"Adding audit mapping for property %s.%s: collection with a join table",referencingEntityName,propertyName
"Using join table name: %s",auditMiddleTableName
"Going to search the mapped by attribute for %s in superclasses of entity: %s",propertyName,referencedClass.getClassName()
"Searching in superclass: %s",tempClass.getSuperclass().getClassName()
"Resolving object from First Level Cache: EntityName:%s - primaryKey:%s - revision:%s",entityName,id,revision
"Caching entity on First Level Cache:  - primaryKey:%s - revision:%s - entityName:%s",id,revision,entityName
"Caching entityName on First Level Cache:  - primaryKey:%s - revision:%s - entity:%s -> entityName:%s",id,revision,entity.getClass().getName(),entityName
"Trying to resolve entityName from First Level Cache: - primaryKey:%s - revision:%s - entity:%s",id,revision,entity
"Skipping envers transaction hook due to non-active (most likely marked-rollback-only) transaction"
"Envers-generate entity mapping -----------------------------\n%s",baos.toString()
"------------------------------------------------------------"
"Skipping Envers listener registrations : EnversService disabled"
"Skipping Envers listener registrations : Listener auto-registration disabled"
"Skipping Envers listener registrations : No audited entities found"
"Encountered deprecated Envers setting [%s]; use [%s] or [%s] instead",LEGACY_AUTO_REGISTER,INTEGRATION_ENABLED,EnversIntegrator.AUTO_REGISTER
"Envers integration enabled? : %s",integrationEnabled
"Failed to execute router: " + getUrl() + ", cause: " + t.getMessage(),t
"route error, rule has been ignored. rule: " + rule + ", method:" + invocation.getMethodName() + ", url: " + RpcContext.getContext().getUrl(),e
"Notification of tag rule, change type is: " + event.getChangeType() + ", raw rule is:\n " + event.getValue()
"Failed to parse the raw tag router rule and it will not take effect, please check if the " + "rule matches with the template, the raw rule is:\n ",e
"TagRouter must getConfig from or subscribe to a specific application, but the application " + "in this TagRouter is not specified."
"The current consumer in the service blacklist. consumer: " + NetUtils.getLocalHost() + ", service: " + url.getServiceKey()
"The route result is empty and force execute. consumer: " + NetUtils.getLocalHost() + ", service: " + url.getServiceKey() + ", router: " + url.getParameterAndDecoded(Constants.RULE_KEY)
"Failed to execute condition router rule: " + getUrl() + ", invokers: " + invokers + ", cause: " + t.getMessage(),t
"Notification of condition rule, change type is: " + event.getChangeType() + ", raw rule is:\n " + event.getValue()
"Failed to parse the raw condition rule and it will not take effect, please check " + "if the condition rule matches with the template, the raw rule is:\n " + event.getValue(),e
"Failback background works error,invocation->" + invocation + ", exception: " + e.getMessage()
"Failback to invoke method " + invocation.getMethodName() + ", wait for retry in background. Ignored exception: " + e.getMessage() + ", ",e
"Failed retry to invoke method " + invocation.getMethodName() + ", waiting again.",e
"Failed retry times exceed threshold (" + retries + "), We have to abandon, invocation->" + invocation
"No available provider for service" + directory.getUrl().getServiceKey() + " on group " + invoker.getUrl().getParameter(Constants.GROUP_KEY) + ", will continue to try another group."
"Invoke " + getGroupDescFromServiceKey(entry.getKey()) + " failed: " + r.getException().getMessage(),r.getException()
"Failsafe ignore exception: " + e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
"Although retry the method " + methodName + " in the service " + getInterface().getName() + " was successful by the provider " + invoker.getUrl().getAddress() + ", but there have been failed providers " + providers + " (" + providers.size() + "/" + copyInvokers.size() + ") from the registry " + directory.getUrl().getAddress() + " on the consumer " + NetUtils.getLocalHost() + " using the dubbo version " + Version.getVersion() + ". Last error is: " + le.getMessage(),le
e.getMessage() + " may because invokers list dynamic change, ignore.",e
"cluster reselect fail reason is :" + t.getMessage() + " if can not solve, you can set cluster.availablecheck=false in url",t
"force-mock: " + invocation.getMethodName() + " force-mock enabled , url : " + directory.getUrl()
"fail-mock: " + invocation.getMethodName() + " fail-mock enabled , url : " + directory.getUrl(),e
"Exception when try to invoke mock. Get mock invokers error for service:" + directory.getUrl().getServiceInterface() + ", method:" + invocation.getMethodName() + ", will contruct a new mock with 'new MockInvoker()'.",e
type + " has no zero-arg constructor and this will affect the serialization performance"
"qos won't be started because it is disabled. " + "Please check dubbo.application.qos.enable is configured either in system property, " + "dubbo.properties or XML/spring-boot configuration."
"Fail to start qos server: ",throwable
"qos-server bind localhost:" + port
"qos-server can not bind localhost:" + port,throwable
"qos-server stopped."
"can not found command " + commandContext,ex
"execute commandContext got exception " + commandContext,ex
"can not found commandContext url: " + msg.getUri()
"can not find commandContext: " + commandContext,ex
"execute commandContext: " + commandContext + " got exception",qosEx
"receive online command"
"receive offline command"
e.getMessage(),e
e.getMessage(),e
"Close grizzly channel " + connection
e.getMessage(),e
t.getMessage()
e.getMessage(),e
"RemotingException on channel " + channel,e
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
"Close old mina channel " + oldSession + " on create new mina channel " + newSession
"Close new mina channel " + newSession + ", because the client closed."
t.getMessage()
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
"CLose mina channel " + session
e.getMessage(),e
t.getMessage(),t
e.getMessage(),e
t.getMessage(),t
t.getMessage(),t
e.getMessage(),e
e.getMessage(),e
t.getMessage(),t
t.getMessage(),t
t.getMessage(),t
e.getMessage(),e
"Please set -Dserver=127.0.0.1:9911"
"Please set -Dserver=127.0.0.1:9911"
"Please set -Dserver=127.0.0.1:9911"
"restart times:" + i
"server closed:" + server
"Please set -Dport=9911"
"Please set -Dserver=127.0.0.1:9911"
e
t.getMessage(),t
t.getMessage(),t
t.getMessage(),t
NEWException("Close channel " + channel + " on exit command: " + Arrays.toString((byteLBRACKETRBRACKET)command))
NEWException("Close channel " + channel + " on exit command " + command)
"Skip input stream " + is.available()
e.getMessage(),e
"Failed to send bad_response info back: " + res + ", cause: " + e.getMessage(),e
"Start " + getClass().getSimpleName() + " " + NetUtils.getLocalAddress() + " connect to the server " + getRemoteAddress()
"Failed to start " + getClass().getSimpleName() + " " + NetUtils.getLocalAddress() + " connect to the server " + getRemoteAddress() + " (check == false, ignore and retry later!), cause: " + t.getMessage(),t
"Successed connect to server " + getRemoteAddress() + " from " + getClass().getSimpleName() + " " + NetUtils.getLocalHost() + " using dubbo version " + Version.getVersion() + ", channel is " + this.getChannel()
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
"Serialization extension " + serialization.getClass().getName() + " has duplicate id to Serialization extension " + ID_SERIALIZATION_MAP.get(idByte).getClass().getName() + ", ignore this Serialization extension"
e
"Start " + getClass().getSimpleName() + " bind " + getBindAddress() + ", export " + getLocalAddress()
t.getMessage(),t
t.getMessage(),t
t.getMessage(),t
"Close " + getClass().getSimpleName() + " bind " + getBindAddress() + ", export " + getLocalAddress()
e.getMessage(),e
e.getMessage(),e
"Close new channel " + ch + ", cause: server is closing or has been closed. For example, receive a new connect request while in shutdown process."
"Close channel " + ch + ", cause: The server " + ch.getLocalAddress() + " connections greater than max config " + accepts
"All clients has disconnected from " + ch.getLocalAddress() + ". You can graceful shutdown now."
t.getMessage(),t
t.getMessage(),t
t.getMessage(),t
"Decode decodeable message " + message.getClass().getName()
"Call Decodeable.decode failed: " + e.getMessage(),e
t.getMessage(),t
t.getMessage(),t
t.getMessage(),t
t.getMessage(),t
t.getMessage(),t
"ChannelEventRunnable handle " + state + " operation error, channel is " + channel + ", message is " + message,e
"ChannelEventRunnable handle " + state + " operation error, channel is " + channel,e
"ChannelEventRunnable handle " + state + " operation error, channel is " + channel,e
"ChannelEventRunnable handle " + state + " operation error, channel is " + channel + ", message is " + message,e
"ChannelEventRunnable handle " + state + " operation error, channel is " + channel + ", message is: " + message + ", exception is " + exception,e
"unknown state: " + state + ", message is " + message
"fail to destroy thread pool of server: " + t.getMessage(),t
NEWIllegalThreadStateException("connectionordered channel handler `queue size: " + connectionExecutor.getQueue().size() + " exceed the warning limit number :" + queuewarninglimit)
"The timeout response finally returned at " + (NEWSimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS").format(NEWDate())) + ", response " + response + (channelEQEQnullQUES"":", channel: " + channel.getLocalAddress() + " -> " + channel.getRemoteAddress())
"callback invoke error .result:" + res.getResult() + ",url:" + channel.getUrl(),e
"callback invoke error ,url:" + channel.getUrl(),e
"callback invoke error ,url:" + channel.getUrl(),e
"Send heartbeat to remote channel " + channel.getRemoteAddress() + ", cause: The channel has no data-transmission exceeds a heartbeat period: " + heartbeat + "ms"
"Exception when heartbeat to remote channel " + channel.getRemoteAddress(),t
e.getMessage(),e
"send cannot write message error.",e
t.getMessage(),t
e.getMessage(),e
e.getMessage(),e
"Close channel " + channel + ", because idleCheck timeout: " + idleTimeout + "ms"
"Exception when close remote channel " + channel.getRemoteAddress(),t
"Received heartbeat from remote channel " + channel.getRemoteAddress() + ", cause: The channel has no data-transmission exceeds a heartbeat period" + (heartbeatGT0QUES": " + heartbeat + "ms":"")
"Receive heartbeat response in thread " + Thread.currentThread().getName()
"Send result to consumer failed, channel is " + channel + ", msg is " + e
e.getMessage(),e
"Initial connection to " + channel
"Fail to connect to " + channel,e
"Reconnect to channel " + channel + ", because heartbeat read idle time out: " + idleTimeout + "ms"
channel + "reconnect failed during idle time.",e
"Exception when reconnect to remote channel " + channel.getRemoteAddress(),t
"Skip input stream " + is.available()
e.getMessage(),e
t.getMessage(),t
"Failed to send bad_response info back: " + t.getMessage() + ", cause: " + e.getMessage(),e
"Failed to send bad_response info back: " + res + ", cause: " + e.getMessage(),e
t.getMessage(),t
t.getMessage(),t
t.getMessage(),t
NEWException("Close channel " + channel + " on exit command: " + Arrays.toString((byteLBRACKETRBRACKET)command))
NEWException("Close channel " + channel + " on exit command " + command)
"find valid zookeeper client from the cache for address: " + url
"find valid zookeeper client from the cache for address: " + url
"No valid zookeeper client found from cache, therefore create a new client for url. " + url
t.getMessage(),t
"Timeout! zookeeper server can not be connected in : " + timeout + "ms!",t
"Zkclient has already been started!"
"Got an exception when trying to create zkclient instance, can not connect to zookeeper server, please check!",e
"zookeeper failed to create persistent node with " + path + ": ",e
"zookeeper failed to create ephemeral node with " + path + ": ",e
"zookeeper failed to create persistent node with " + path + " and " + data + " : ",e
"zookeeper failed to create ephemeral node with " + path + " and " + data + " : ",e
"zookeeper failed to delete node with " + path + ": ",e
"zookeeper failed to get children node with " + path + ": ",e
"zookeeper failed to check node existing with " + path + ": ",t
"zookeeper failed to get data with " + path + ": ",e
"IdleStateEvent triggered, close channel " + channel
"IdleStateEvent triggered, send heartbeat to channel " + channel
"Close old netty channel " + oldChannel + " on create new netty channel " + newChannel
"Close new netty channel " + newChannel + ", because the client closed."
t.getMessage()
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
"Close netty channel " + channel
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
t.getMessage(),t
this.getClass().getSimpleName() + message.toString()
msg
msg,cause
msg
msg,cause
msg
msg,cause
msg
msg,cause
"Close old netty channel " + oldChannel + " on create new netty channel " + newChannel
"Close new netty channel " + newChannel + ", because the client closed."
t.getMessage()
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
"Close netty channel " + channel
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
"error"
"warn"
"info"
"debug"
"info"
NEWException("error")
NEWException("warn")
NEWException("info")
NEWException("debug")
NEWException("trace")
"error",NEWException("error")
"warn",NEWException("warn")
"info",NEWException("info")
"debug",NEWException("debug")
"trace",NEWException("trace")
"error"
"warn"
"info"
"debug"
"info"
NEWException("error")
NEWException("warn")
NEWException("info")
NEWException("debug")
NEWException("trace")
"error",NEWException("error")
"warn",NEWException("warn")
"info",NEWException("info")
"debug",NEWException("debug")
"trace",NEWException("trace")
"load status level: " + status.getLevel()
"load status message: " + status.getMessage()
"memory status level: " + status.getLevel()
"memory status message: " + status.getMessage()
"No codeSource for class " + cls.getName() + " when getVersion, use default version " + defaultVersion
error
e.getMessage(),e
e.getMessage(),e
"using logger: " + loggerAdapter.getClass().getName()
msg
eEQEQnullQUESnull:e.getMessage(),e
msg,e
msg
eEQEQnullQUESnull:e.getMessage(),e
msg,e
msg
eEQEQnullQUESnull:e.getMessage(),e
msg,e
msg
eEQEQnullQUESnull:e.getMessage(),e
msg,e
msg
eEQEQnullQUESnull:e.getMessage(),e
msg,e
msg
e.getMessage(),e
msg,e
msg
e.getMessage(),e
msg,e
msg
e.getMessage(),e
msg,e
msg
e.getMessage(),e
msg,e
msg
e.getMessage(),e
msg,e
appendContextMessage(msg),e
e
appendContextMessage(msg)
appendContextMessage(msg),e
e
appendContextMessage(msg)
appendContextMessage(msg),e
appendContextMessage(msg)
appendContextMessage(msg),e
appendContextMessage(msg)
appendContextMessage(msg),e
appendContextMessage(msg)
e
e
e
msg
e
msg,e
msg
e
msg,e
msg
e
msg,e
msg
e
msg,e
msg
e
msg,e
"Error when trying to get value for key " + key + " from " + config + ", will continue to try the next one."
"You specified the config centre, but there's not even one single config item in it."
"You are creating too many " + resourceType + " instances. " + resourceType + " is a shared resource that must be reused across the JVM," + "so that only a few instances are created."
"An exception was thrown while process a cancellation task",t
"An exception was thrown by " + TimerTask.CLASS.getSimpleName() + .,t
msg
"dump jStack error",t
code.toString()
"Failed to inject via method " + method.getName() + " of interface " + type.getName() + ": " + e.getMessage(),e
e.getMessage(),e
"Exception occurred when loading extension class (interface: " + type + ", description file: " + fileName + ").",t
"Exception occurred when loading extension class (interface: " + type + ", class file: " + resourceURL + ") in " + resourceURL,t
e.getMessage(),e
msg
throwable
msg,e
msg
e
msg,e
msg
e
msg,e
msg,e
msg
e
e
msg
msg,e
"Failed to load " + fileName + " file from " + fileName + "(ignore this file): " + e.getMessage(),e
"Fail to load " + fileName + " file: " + t.getMessage(),t
"No " + fileName + " found on the class path."
errMsg
"Failed to load " + fileName + " file from " + fileName + "(ignore this file): " + e.getMessage(),e
"load " + fileName + " properties file from " + list
"Fail to load " + fileName + " file from " + url + "(ignore this file): " + e.getMessage(),e
exceptionDescription,e
e.getMessage(),e
e
e
e
e
"Use container type(" + Arrays.toString(args) + ") to run dubbo serivce."
"Dubbo " + container.getClass().getSimpleName() + " stopped!"
t.getMessage(),t
"Dubbo " + container.getClass().getSimpleName() + " started!"
e.getMessage(),e
"Dubbo service server stopped, interrupted by other thread!",e
e.getMessage(),e
"Test debug:" + this.getClass().getName()
"Test warn:" + this.getClass().getName()
"Test info:" + this.getClass().getName()
"Test error:" + this.getClass().getName()
"Failed to put " + metadataIdentifier + " to redis " + v + ", cause: " + e.getMessage(),e
"publishProvider interfaceName is empty . providerUrl: " + providerUrl.toFullString()
"Failed to save service store file, cause: " + e.getMessage(),e
"Load service store file " + file + ", data: " + properties
"Failed to load service store file " + file,e
t.getMessage(),t
"store provider metadata. Identifier : " + providerMetadataIdentifier + "; definition: " + serviceDefinition
"Failed to put provider metadata " + providerMetadataIdentifier + " in  " + serviceDefinition + ", cause: " + e.getMessage(),e
"store consumer metadata. Identifier : " + consumerMetadataIdentifier + "; definition: " + serviceParameterMap
"Failed to put consumer metadata " + consumerMetadataIdentifier + ";  " + serviceParameterMap + ", cause: " + e.getMessage(),e
"start to publish all metadata."
"start to retry task for metadata report. retry times:" + times
"@Reference annotation is not supported on static fields: " + field
"@Reference annotation is not supported on static methods: " + method
"@Reference  annotation should only be used on methods with parameters: " + method
referenceBean + " was destroying!"
getClass() + " was destroying!"
"packagesToScan is empty , ServiceBean registry will be ignored!"
beanDefinitionHolders.size() + " annotated Dubbo's @Service Components { " + beanDefinitionHolders + " } were scanned under package[" + packageToScan + "]"
"No Spring Bean annotating Dubbo's @Service was found under package[" + packageToScan + "]"
"BeanNameGenerator bean can't be found in BeanFactory with name [" + CONFIGURATION_BEAN_NAME_GENERATOR + "]"
"BeanNameGenerator will be a instance of " + AnnotationBeanNameGenerator.CLASS.getName() + " , it maybe a potential problem on bean name generation."
"The BeanDefinition[" + serviceBeanDefinition + "] of ServiceBean has been registered with name : " + beanName
"The Duplicated BeanDefinition[" + serviceBeanDefinition + "] of ServiceBean[ bean name : " + beanName + "] was be found , Did @CompatibleDubboComponentScan scan to same package in many times?"
"Hello " + name + ", request from consumer: " + RpcContext.getContext().getRemoteAddress()
"Hello " + name + ", request from consumer: " + RpcContext.getContext().getRemoteAddress()
"Hello " + name + ", request from consumer: " + RpcContext.getContext().getRemoteAddress()
"IllegalArgumentException",e
"IllegalAccessException",e
"InvocationTargetException",e
"SecurityException: " + e.getMessage()
"NoSuchMethodException: " + e.getMessage()
"Verifying service url for " + configName + "... "
"Consumer url string: " + paramStringFromDb
"Checking " + (String)rowLBRACKETKEYRBRACKET + "for" + targetString
(String)rowLBRACKETKEYRBRACKET + " --> " + targetString + " OK!"
"ReferenceConfig(" + url + ") is not DESTROYED when FINALIZE"
"Unexpected error occured when destroy invoker of ReferenceConfig(" + url + ").",t
"No method found in service interface " + interfaceClass.getName()
"Using injvm service " + interfaceClass.getName()
"Refer dubbo service " + interfaceClass.getName() + " from url " + invoker.getUrl()
"Using default dubbo resolve file " + resolveFile + " replace " + interfaceName + "" + resolve + " to p2p invoke remote service."
"Using -D" + interfaceName + "=" + resolve + " to p2p invoke remote service."
"Run shutdown hook now."
t.getMessage(),t
"Use random available port(" + port + ") for protocol " + protocol
"Unexpected error occured when unexport " + exporter,t
"No method found in service interface " + interfaceClass.getName()
"Export dubbo service " + interfaceClass.getName() + " to url " + url
"Register dubbo service " + interfaceClass.getName() + " url " + url + " to registry " + registryURL
"Export dubbo service " + interfaceClass.getName() + " to local registry"
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
"Failed to override the property " + method.getName() + " in " + this.getClass().getSimpleName() + ", please make sure every property has getter/setter method provided."
"Failed to override ",e
e.getMessage(),e
t.getMessage(),t
"There's no valid monitor config found, if you want to open monitor statistics for Dubbo, " + "please make sure your monitor is configured properly."
"There's no valid metadata config found, if you are using the simplified mode of registry url, " + "please make sure you have a metadata address configured properly."
"Duplicate ProviderConfig found, there already has one default ProviderConfig or more than two ProviderConfigs have the same id, " + "you can try to give each ProviderConfig a different id. " + providerConfig
"Duplicate ConsumerConfig found, there already has one default ConsumerConfig or more than two ConsumerConfigs have the same id, " + "you can try to give each ConsumerConfig a different id. " + consumerConfig
"Duplicate ProtocolConfig found, there already has one default ProtocolConfig or more than two ProtocolConfigs have the same id, " + "you can try to give each ProtocolConfig a different id. " + protocolConfig
"Duplicate RegistryConfig found, there already has one default RegistryConfig or more than two RegistryConfigs have the same id, " + "you can try to give each RegistryConfig a different id. " + registryConfig
"[subscribe] service: " + service + ",client:" + client
"Disconnected " + client
"Register service: " + url.getServiceKey() + ",url:" + url
"Unregister service: " + url.getServiceKey() + ",url:" + url
"Subscribe service: " + url.getServiceKey() + ",url:" + url
"Unsubscribe service: " + url.getServiceKey() + ",url:" + url
"Failed to notify registry event, service: " + service + ", urls: " + urls + ", cause: " + t.getMessage(),t
"The service ready on spring started. service: " + getInterface()
"@" + getAnnotationType().getName() + " is not supported on static fields: " + field
"@" + getAnnotationType().getSimpleName() + " annotation is not supported on static methods: " + method
"@" + getAnnotationType().getSimpleName() + " annotation should only be used on methods with parameters: " + method
object + " was destroying!"
getClass() + " was destroying!"
"packagesToScan is empty , ServiceBean registry will be ignored!"
beanDefinitionHolders.size() + " annotated Dubbo's @Service Components { " + beanDefinitionHolders + " } were scanned under package[" + packageToScan + "]"
"No Spring Bean annotating Dubbo's @Service was found under package[" + packageToScan + "]"
"BeanNameGenerator bean can't be found in BeanFactory with name [" + CONFIGURATION_BEAN_NAME_GENERATOR + "]"
"BeanNameGenerator will be a instance of " + AnnotationBeanNameGenerator.CLASS.getName() + " , it maybe a potential problem on bean name generation."
"The BeanDefinition[" + serviceBeanDefinition + "] of ServiceBean has been registered with name : " + beanName
"The Duplicated BeanDefinition[" + serviceBeanDefinition + "] of ServiceBean[ bean name : " + beanName + "] was be found , Did @DubboComponentScan scan to same package in many times?"
"The bean[type:" + bean.getClass().getSimpleName() + "] has been built."
"The properties of bean [name : " + beanName + "] have been binding by prefix of " + "configuration properties : " + prefix
"DubboConfigBinder Bean can't be found in ApplicationContext."
t.getMessage(),t
e.getMessage(),e
registry.getClass().getSimpleName() + " will register annotated classes : " + Arrays.asList(annotatedClasses) + " ."
"No spring extension (bean) named:" + name + ", try to find an extension (bean) of type " + type.getName()
"Find more than 1 spring extensions (beans) of type " + type.getName() + ", will stop auto injection. Please make sure you have specified the concrete parameter type and there's only one extension of that type."
"Error when get spring extension(bean) for type:" + type.getName(),noBeanExe
"No spring extension (bean) named:" + name + ", type:" + type.getName() + " found, stop get bean."
"There is no property for binding to dubbo config class [" + configClass.getName() + "] within prefix [" + prefix + "]"
"The dubbo config bean definition [name : " + beanName + ", class : " + configClass.getName() + "] has been registered."
"The BeanPostProcessor bean definition [" + processorClass.getName() + "] for dubbo config bean [name : " + beanName + "] has been registered."
"Failed to connect to config center, the config center is Apollo, " + "the address is: " + (StringUtils.isNotEmpty(configAddr)QUESconfigAddr:configEnv) + ", will use the local cache value instead before eventually the connection is established."
"an empty rule is received for " + key + ", the current working rule is " + change.getOldValue() + ", the empty rule will not take effect."
"The config center (zookeeper) is not fully initialized in " + 3STARconnectTimeout + "ms, address is: " + connectString
"Failed to build local cache for config center (zookeeper), address is ." + connectString
"Failed to write provider heartbeat to redis registry. registry: " + entry.getKey() + ", cause: " + t.getMessage(),t
"Delete expired key: " + key + " -> value: " + entry.getKey() + ", expire: " + NEWDate(expire) + ", now: " + NEWDate(now)
t.getMessage(),t
t.getMessage(),t
"Failed to destroy the redis registry client. registry: " + entry.getKey() + ", cause: " + t.getMessage(),t
exception.getMessage(),exception
exception.getMessage(),exception
exception.getMessage(),exception
"redis notify: " + key + " = " + urls
"redis event: " + key + " = " + msg
t.getMessage(),t
t.getMessage(),t
t.getMessage(),t
e.getMessage(),e
"Clean expired provider " + url
"Receive multicast message: " + msg + " from " + remoteAddress
"Send multicast message: " + msg + " to " + multicastAddress + ":" + multicastPort
"Send unicast message: " + msg + " to " + host + ":" + multicastPort
t.getMessage(),t
t.getMessage(),t
"Please set -Dserver=127.0.0.1:9090"
"unexpected error when unregister service " + serviceKey + "from registry" + registry.getUrl(),t
"unexpected error when unsubscribe service " + serviceKey + "from registry" + registry.getUrl(),t
"Failed to destroy service " + serviceKey,t
NEWIllegalStateException("urls to invokers error .invokerUrls.size :" + invokerUrls.size() + ", invoker.size :0. urls :" + invokerUrls.toString())
"destroyUnusedInvokers error. ",e
"convert router url to router error, url: " + url,t
NEWIllegalStateException("Unsupported protocol " + providerUrl.getProtocol() + " in notified url: " + providerUrl + " from registry " + getUrl().getAddress() + " to consumer " + NetUtils.getLocalHost() + ", supported protocol: " + ExtensionLoader.getExtensionLoader(Protocol.CLASS).getSupportedExtensions())
"Failed to refer invoker for interface:" + serviceType + ",url:(" + url + ")" + t.getMessage(),t
"Failed to destroy service " + serviceKey + " to provider " + invoker.getUrl(),t
"destroy invoker[" + invoker.getUrl() + "] success. "
"destroy invoker[" + invoker.getUrl() + "] failed. " + e.getMessage(),e
"Failed to execute router: " + getUrl() + ", cause: " + t.getMessage(),t
"Unsupported category " + category + " in notified url: " + url + " from registry " + getUrl().getAddress() + " to consumer " + NetUtils.getLocalHost()
"Notification of overriding rule, change type is: " + event.getChangeType() + ", raw config content is:\n " + event.getValue()
"Failed to parse raw dynamic config and it will not take effect, the raw config is: " + event.getValue(),e
NEWIllegalStateException("error state, exporter should not be null")
"original override urls: " + urls
"subscribe url: " + subscribeUrl + ", override urls: " + matchedUrls
NEWIllegalStateException("error state, exporter should not be null")
"exported provider url changed, origin url: " + originUrl + ", old export url: " + currentUrl + ", new export url: " + newUrl
t.getMessage(),t
t.getMessage(),t
"Waiting " + timeout + "ms for registry to notify all consumers before unexport. " + "Usually, this is called when you use dubbo API"
t.getMessage(),t
"Failed to register " + url + ", waiting for retry, cause: " + t.getMessage(),t
"Failed to unregister " + url + ", waiting for retry, cause: " + t.getMessage(),t
"Failed to subscribe " + url + ", Using cached list: " + urls + " from cache file: " + getUrl().getParameter(Constants.FILE_KEY,System.getProperty("user.home") + "/dubbo-registry-" + url.getHost() + ".cache") + ", cause: " + t.getMessage(),t
"Failed to subscribe " + url + ", waiting for retry, cause: " + t.getMessage(),t
"Failed to unsubscribe " + url + ", waiting for retry, cause: " + t.getMessage(),t
"Failed to notify for subscribe " + url + ", waiting for retry, cause: " + t.getMessage(),t
"Recover register url " + recoverRegistered
"Recover subscribe url " + recoverSubscribed.keySet()
"Failed to save registry cache file, cause: " + e.getMessage(),e
"Load registry cache file " + file + ", data: " + properties
"Failed to load registry cache file " + file,e
e.getMessage(),e
"Register: " + url
"Unregister: " + url
"Subscribe: " + url
"Unsubscribe: " + url
"Recover register url " + recoverRegistered
"Recover subscribe url " + recoverSubscribed.keySet()
"Failed to notify registry event, urls: " + urls + ", cause: " + t.getMessage(),t
"Ignore empty notify urls for subscribe url " + url
"Notify urls for subscribe url " + url + ", urls: " + urls
t.getMessage(),t
"Destroy registry:" + getUrl()
"Destroy unregister url " + url
"Failed to unregister url " + url + " to registry " + getUrl() + " on destroy, cause: " + t.getMessage(),t
"Destroy unsubscribe url " + url
"Failed to unsubscribe url " + url + " to registry " + getUrl() + " on destroy, cause: " + t.getMessage(),t
taskName + " : " + url
"[subscribe] service: " + service + ",client:" + client
"Disconnected " + client
"Register service: " + url.getServiceKey() + ",url:" + url
"Unregister service: " + url.getServiceKey() + ",url:" + url
"Subscribe service: " + url.getServiceKey() + ",url:" + url
"Unsubscribe service: " + url.getServiceKey() + ",url:" + url
"Failed to notify registry event, service: " + service + ", urls: " + urls + ", cause: " + t.getMessage(),t
"Begin to register: " + url
"Begin to ungister: " + url
"Begin to subscribe: " + url
"Begin to unSubscribe: " + url
"Reconnect to registry " + getUrl()
"Failed to connect to registry " + getUrl().getAddress() + " from provider/consumer " + NetUtils.getLocalHost() + " use dubbo " + Version.getVersion() + ", cause: " + t.getMessage(),t
"Failed to cancel reconnect timer",t
e.getMessage(),e
"Failed to close zookeeper client " + getUrl() + ", cause: " + e.getMessage(),e
"Failed to monitor count service " + invoker.getUrl() + ", cause: " + t.getMessage(),t
"Thread was interrupted unexpectedly, monitor will never be got."
"Create monitor failed, monitor data will not be collected until you fix this problem. ",e
"Unexpected error occur at send statistic, cause: " + t.getMessage(),t
"Send statistics to monitor " + getUrl()
"Unexpected error occur at cancel sender timer, cause: " + t.getMessage(),t
e.getMessage(),e
"Failed to validate service: " + clazz.getName() + ", method: " + methodName + ", cause: " + violations
t.getMessage(),t
t.getMessage(),t
t.getMessage(),t
t.getMessage(),t
"Destroy reference: " + invoker.getUrl()
t.getMessage(),t
"Unexport service: " + exporter.getInvoker().getUrl()
t.getMessage(),t
"Invoker for service " + this + " on consumer " + NetUtils.getLocalHost() + " is destroyed, " + ", dubbo version is " + Version.getVersion() + ", this invoker should not be used any longer"
t.getMessage(),t
e.getMessage(),e
t.getMessage(),t
"Got unchecked and undeclared exception which called by " + RpcContext.getContext().getRemoteHost() + ". service: " + invoker.getInterface().getName() + ", method: " + invocation.getMethodName() + ", exception: " + e.getClass().getName() + ": " + e.getMessage(),e
"Fail to ExceptionFilter when called by " + RpcContext.getContext().getRemoteHost() + ". service: " + invoker.getInterface().getName() + ", method: " + invocation.getMethodName() + ", exception: " + e.getClass().getName() + ": " + e.getMessage(),e
"invoke time out. method: " + invocation.getMethodName() + " arguments: " + Arrays.toString(invocation.getArguments()) + " , url is " + invoker.getUrl() + ", invoke elapsed " + elapsed + " ms."
"Exception in AcessLogFilter of service(" + invoker + " -> " + inv + ")",t
"Append log to " + accessLog
e.getMessage(),e
"Provider async started, but got an exception from the original method, cannot write the exception back to consumer because an async result may have returned the new thread.",e
t.getMessage(),t
t.getMessage(),t
e.getMessage(),e
"Closing the rest server at " + entry.getKey()
"Error closing rest server",t
"Closing rest clients"
"Error closing rest client",t
"The contents of request body is: \n" + NEWString(buffer,"UTF-8") + "\n"
"The contents of response body is: \n" + NEWString(wrapper.getBytes(),"UTF-8") + "\n"
msg.toString()
e.getMessage(),e
t.getMessage(),t
"Lazy connect to " + url
NEWIllegalStateException("safe guard client , should not be called ,must have a bug.")
"Decode rpc result failed: " + e.getMessage(),e
NEWIllegalStateException("The methodName " + inv.getMethodName() + " not found in callback service interface ,invoke will be ignored." + " please update the api interface. url is:" + invoker.getUrl()) + " ,invocation is :" + inv
"disconnected from " + channel.getRemoteAddress() + ",url:" + channel.getUrl()
"Failed to invoke event method " + invocation.getMethodName() + "(), cause: " + t.getMessage(),t
NEWIllegalStateException("consumer [" + url.getParameter(Constants.INTERFACE_KEY) + "], has set stubproxy support event ,but no stub methods founded.")
"Optimizing the serialization process for Kryo, FST, etc..."
"Close dubbo server: " + server.getLocalAddress()
t.getMessage(),t
"Close dubbo connect: " + client.getLocalAddress() + "-->" + client.getRemoteAddress()
t.getMessage(),t
"Decode rpc invocation failed: " + e.getMessage(),e
"Decode argument failed: " + e.getMessage(),e
"Export a callback service :" + exportUrl + ", on " + channel + ", url is: " + url
"method " + inv.getMethodName() + " include a callback service :" + invoker.getUrl() + ", a proxy :" + invoker + " has been created."
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
e.getMessage(),e
"Decode response failed: " + t.getMessage(),t
"Decode request failed: " + t.getMessage(),t
invocation.getMethodName() + ".call back method invoke error . callback method :" + onthrowMethod + ", url:" + invoker.getUrl(),e
invocation.getMethodName() + ".call back method invoke error . callback method :" + onthrowMethod + ", url:" + invoker.getUrl(),exception
e.getMessage(),e
"Close dubbo server: " + server.getLocalAddress()
t.getMessage(),t
t.getMessage(),t
"Unsupported magic " + magic
"Could not find processor for service " + serviceName
"Close hessian server " + server.getUrl()
t.getMessage(),t
"returnResource error: " + t.getMessage(),t
e.getMessage(),e
"Saving cookies failed for " + url.resolve("/..."),e
"Loading cookies failed for " + url.resolve("/..."),e
"Ignoring header " + field + " because its value was null.",null
"Ignoring header " + field + " because its value was null.",null
"Callback failure for " + toLoggableString(),e
"DiskLruCache " + directory + " is corrupt: " + journalIsCorrupt.getMessage() + ", removing",journalIsCorrupt
"ALPN callback dropped: HTTP/2 is disabled. " + "Is alpn-boot on the boot class path?",null
message,(Throwable)stackTrace
"Http2Connection.Listener failure for " + connectionName,e
message,null
"Failure serving Http2Stream: " + e.getMessage(),null
"served " + requestLine
MockWebServer.this + " starting to accept connections"
MockWebServer.this + " done accepting connections: " + e.getMessage()
MockWebServer.this + " connection from " + raw.getInetAddress() + " failed: " + e
MockWebServer.this + " received request: " + request + " and responded: " + response
MockWebServer.this + " received request: " + request + " and responded: " + response + " protocol is " + protocol.toString()
name + " done accepting connections: " + e.getMessage()
name + " done: " + e.getMessage()
MockHttp2Peer.this + " done: " + e.getMessage()
String.format("Sending request %s on %s%n%s",request.url(),chain.connection(),request.headers())
String.format("Received response for %s in %.1fms%n%s",request.url(),(t2SUBt1)SLASH1e6d,response.headers())
"{} Setting offsets for topic partitions {}",this,offsets
"{} Setting offset for topic partition {} to {}",this,tp,offset
"{} Setting timeout to {} ms",this,timeoutMs
"{} Connector is paused, so not pausing consumer's partitions {}",this,partitions
"{} Pausing partitions {}. Connector is not paused.",this,partitions
"{} Connector is paused, so not resuming consumer's partitions {}",this,partitions
"{} Resuming partitions: {}",this,partitions
"{} Requesting commit",this
"Kafka Connect instance created"
"Kafka Connect starting"
"Kafka Connect started"
"Kafka Connect stopping"
"Kafka Connect stopped"
"Interrupted waiting for Kafka Connect to shutdown"
"Interrupted in shutdown hook while waiting for Kafka Connect startup to finish"
b.toString()
"Worker starting"
"Worker started"
"Worker stopping"
"Shutting down connectors {} uncleanly; herder should have shut down connectors before the Worker is stopped",connectors.keySet()
"Shutting down tasks {} uncleanly; herder should have shut down tasks before the Worker is stopped",tasks.keySet()
"Worker stopped"
"Creating connector {} of type {}",connName,connClass
"Instantiated connector {} with version {} of type {}",connName,connector.version(),connector.getClass()
"Failed to start connector {}",connName,t
"Finished creating connector {}",connName
"Reconfiguring connector tasks for {}",connName
"Stopping connector {}",connName
"Ignoring stop request for unowned connector {}",connName
"Stopped connector {}",connName
"Creating task {}",id
"Instantiated task {} with version {} of type {}",id,task.version(),taskClass.getName()
"Set up the key converter {} for task {} using the worker config",keyConverter.getClass(),id
"Set up the key converter {} for task {} using the connector config",keyConverter.getClass(),id
"Set up the value converter {} for task {} using the worker config",valueConverter.getClass(),id
"Set up the value converter {} for task {} using the connector config",valueConverter.getClass(),id
"Set up the header converter {} for task {} using the worker config",headerConverter.getClass(),id
"Set up the header converter {} for task {} using the connector config",headerConverter.getClass(),id
"Failed to start task {}",id,t
"Initializing: {}",transformationChain
"Initializing: {}",transformationChain
"Tasks must be a subclass of either SourceTask or SinkTask",task
"Ignoring stop request for unowned task {}",taskId
"Stopping task {}",task.id()
"Ignoring await stop request for non-present task {}",taskId
"Graceful stop of task {} failed.",task.id()
"Graceful stop of task {} succeeded.",task.id()
"Setting connector {} state to {}",connName,state
"Registering Connect metrics with JMX for worker '{}'",workerId
"Unregistering Connect metrics with JMX for worker '{}'",workerId
"Scheduling a restart of connector {} in {} ms",connectorName,ttl
"Unexpected error during connector restart: ",error
"{} Initializing connector {} with config {}",this,connName,config
"{} Connector raised an error",WorkerConnector.this,e
"{} Error initializing connector",this,t
"{} Error while starting connector",this,t
"{} Error while shutting down connector",this,t
"{} Error while shutting down connector",this,t
"{} Cannot transition connector to {} since it has failed",this,targetState
"{} Transition connector to {}",this,targetState
"Graceful shutdown of offset commitOffsets thread timed out."
"{} Committing offsets",workerTask
"{} Failed to commit offsets",workerTask
"{} Task failed initialization and will not be started.",this,t
"Could not stop task",t
"Could not close consumer",t
"Could not close transformation chain",t
"{} Commit of offsets timed out",this
"{} Consumer woken up",this
"{} Received out of order commit callback for sequence number {}, but most recent sequence number is {}",this,seqno,commitSeqno
"{} Commit of offsets threw an unexpected exception for sequence number {}: {}",this,seqno,committedOffsets,error
"{} Finished offset commit successfully in {} ms for sequence number {}: {}",this,durationMillis,seqno,committedOffsets
"{} Setting last committed offsets to {}",this,committedOffsets
"{} Initializing and starting task for topics {}",this,topics
"{} Initializing and starting task for topics regex {}",this,topicsRegexStr
"{} Sink task finished initialization and start",this
"{} Polling consumer with timeout {} ms",this,timeoutMs
"{} Polling returned {} messages",this,msgs.count()
"{} Committing offsets synchronously using sequence number {}: {}",this,seqno,offsets
"{} Committing offsets asynchronously using sequence number {}: {}",this,seqno,offsets
"{} Calling task.preCommit with current offsets: {}",this,currentOffsets
"{} Offset commit failed during close",this
"{} Offset commit failed, rewinding to last committed offsets",this,t
"{} Rewinding topic partition {} to offset {}",this,entry.getKey(),entry.getValue().offset()
"{} Closing the task before committing the offsets: {}",this,currentOffsets
"{} Skipping offset commit, task opted-out by returning no offsets from preCommit",this
"{} Ignoring invalid task provided offset {}/{} -- not yet consumed, taskOffset={} currentOffset={}",this,partition,taskProvidedOffset,taskOffset,currentOffset
"{} Ignoring invalid task provided offset {}/{} -- partition not assigned, assignment={}",this,partition,taskProvidedOffset,consumer.assignment()
"{} Skipping offset commit, no change since last commit",this
"{} Consuming and converting message in topic '{}' partition {} at offset {} and timestamp {}",this,msg.topic(),msg.partition(),msg.offset(),msg.timestamp()
"{} Converters and transformations returned null, possibly because of too many retries, so " + "dropping record in topic '{}' partition {} at offset {}",this,msg.topic(),msg.partition(),msg.offset()
"{} Applying transformations to record in topic '{}' partition {} at offset {} and timestamp {} with key {} and value {}",this,msg.topic(),msg.partition(),msg.offset(),timestamp,keyAndSchema.value(),valueAndSchema.value()
"{} RetriableException from SinkTask:",this,e
"{} Task threw an uncaught and unrecoverable exception. Task is being killed and will not " + "recover until manually restarted.",this,t
"{} Rewind {} to offset {}",this,tp,offset
"{} Cannot rewind {} to null offset",this,tp
"{} Partitions assigned {}",WorkerSinkTask.this,partitions
"{} Assigned topic partition {} with offset {}",WorkerSinkTask.this,tp,pos
"{} Partitions revoked",WorkerSinkTask.this
"{} Task failed initialization and will not be started.",this,t
"Could not close producer",t
"Could not close transformation chain",t
"Could not stop task",t
"{} Source task finished initialization and start",this
"{} Nothing to send to Kafka. Polling source for additional records",this
"{} About to send {} records to Kafka",this,toSend.size()
"{} failed to poll records from SourceTask. Will retry operation.",this,e
"{} Appending record with key {}, value {}",this,record.key(),record.value()
"{} failed to send record to {}:",WorkerSourceTask.this,topic,e
"{} Failed record: {}",WorkerSourceTask.this,preTransformRecord
"{} Wrote record successfully: topic {} partition {} offset {}",WorkerSourceTask.this,recordMetadata.topic(),recordMetadata.partition(),recordMetadata.offset()
"{} Failed to send {}, backing off before retrying:",this,producerRecord,e
"{} Exception thrown while calling task.commitRecord()",this,t
"{} CRITICAL Saw callback for record that was not present in the outstanding message set: {}",this,record
"{} Committing offsets",this
"{} Failed to flush, timed out while waiting for producer to flush outstanding {} messages",this,outstandingMessages.size()
"{} Finished offset commitOffsets successfully in {} ms",this,durationMillis
"{} Failed to flush offsets to storage: ",WorkerSourceTask.this,error
"{} Finished flushing offsets to storage",WorkerSourceTask.this
"{} Flush of offsets interrupted, cancelling",this
"{} Flush of offsets threw an unexpected exception: ",this,e
"{} Timed out waiting to flush offsets to storage",this
"{} Finished commitOffsets successfully in {} ms",this,durationMillis
"{} Exception thrown while calling task.commit()",this,t
"Applying transformation {} to {}",transformation.getClass().getName(),record
"Worker configuration property '{}'{} is deprecated and may be removed in an upcoming release. " + "The specified value '{}' matches the default, so this property can be safely removed from the worker configuration.",propName,prefixNotice,propValue
"Worker configuration property '{}'{} is deprecated and may be removed in an upcoming release. " + "The specified value '{}' does NOT match the default and recommended value '{}'.",propName,prefixNotice,propValue,defaultValue
"Worker configuration property '{}'{} is deprecated and may be removed in an upcoming release.",propName,prefixNotice
"{} Task threw an uncaught and unrecoverable exception during shutdown",this,t
"{} Task threw an uncaught and unrecoverable exception",this,t
"{} Task is being killed and will not recover until manually restarted",this
"Getting plugin class loader for connector: '{}'",connectorClassOrAlias
"Plugin class loader for connector: '{}' was not found. Returning: {}",connectorClassOrAlias,this
"Invalid path in plugin path: {}. Ignoring.",path,e
"Could not get listing for plugin path: {}. Ignoring.",path,e
"Could not instantiate plugins in: {}. Ignoring: {}",path,e
"Loading plugin from: {}",pluginLocation
"Loading plugin urls: {}",Arrays.toString(urls)
"Registered loader: {}",loader
"Registered java.sql.Driver: {} to java.sql.DriverManager",driver
"Ignoring java.sql.Driver classes listed in resources but not" + " present in class loader's classpath: ",t
"Skipping {} as it is not concrete implementation",plugin
"Retrieving loaded class '{}' from '{}'",fullName,pluginLoader
"Added alias '{}' to plugin '{}'",simple,plugin.className()
"Added aliases '{}' and '{}' to plugin '{}'",simple,pruned,plugin.className()
"could not create Vfs.Dir from url. ignoring the exception and continuing",e
"Plugin path contains both java archives and class files. Returning only the" + " archives"
"Configuring the {} converter with configuration keys:{}{}",isKeyConverterQUES"key":"value",System.lineSeparator(),converterConfig.keySet()
"Configuring the header converter with configuration keys:{}{}",System.lineSeparator(),converterConfig.keySet()
"Topic {} doesn't exist. Will attempt to create topic.",topic
"Could not produce message to dead letter queue. topic=" + dlqTopicName,exception
"Could not serialize stacktrace.",e
message(context),context.error()
"ProcessingContext is already in failed state. Ignoring requested operation."
"Caught a retriable exception while executing {} operation with {}",context.stage(),context.executingClass()
"Thread was interrupted. Marking operation as failed."
"Can't retry. start={}, attempt={}, deadline={}",startTime,attempt,deadline
"Sleeping for {} millis",delay
"Herder starting"
"Herder started"
"Herder stopping"
"Herder stopped"
"Task that requested reconfiguration does not exist: {}",connName
"Skipping update of connector {} since it is not running",connName
"The resource {} is already registered",component
"The resource {} is already registered",componentClass
"Failed to start RestClient: ",e
"Sending {} with input {} to {}",method,serializedBody,url
"Request's response code: {}",responseCode
"IO error forwarding REST request: ",e
"Failed to stop HTTP client",e
"Added connector for " + listener
"Initializing REST server"
"REST server listening at " + jettyServer.getURI() + ", advertising URL " + advertisedUrl()
"Initializing REST resources"
"REST resources initialized; server is started and ready to handle requests"
"Stopping REST server"
"Error while invoking close on " + connectRestExtension.getClass(),e
"REST server stopped"
"Advertised URI: {}",builder.build()
"Uncaught exception in REST call to /{}",uriInfo.getPath(),exception
"Uncaught exception in REST call to /{}",uriInfo.getPath(),exception
"Ignoring unknown expanion type {}",expansion
"Forwarding request {} {} {}",forwardUrl,method,body
"Deserialized new assignment: {}",newAssignment
"After revocations snapshot of assignment: {}",assignmentSnapshot
"Augmented new assignment: {}",newAssignment
"Rebalance started"
"Revoking previous assignment {}",assignmentSnapshot
"Cooperative rebalance triggered. Keeping assignment {} until it's " + "explicitly revoked.",assignmentSnapshot
"Performing task assignment"
"Was selected to perform assignments, but do not have latest config found in sync request. " + "Returning an empty configuration to trigger re-sync."
"Assigning connector {} to {}",connectorId,connectorAssignedTo
"Assigning task {} to {}",taskId,taskAssignedTo
"Assignment: {} -> {}",member,assignment
"Finished assignment"
"Max config offset root: {}, local snapshot config offsets root: {}",maxOffset,coordinator.configSnapshot().offset()
"Connect group member created"
"Stopping the Connect group member."
"The Connect group member has stopped."
"Herder starting"
"Herder started"
"Herder stopped"
"Uncaught exception in herder work thread, exiting: ",t
"Scheduled rebalance at: {} (now: {} nextRequestTimeoutMs: {}) ",scheduledRebalance,now,nextRequestTimeoutMs
"Requesting rebalance due to reconfiguration of tasks (needsReconfigRebalance: {})",needsReconfigRebalance
"Handling connector-only config update by {} connector {}",remainsQUES"restarting":"stopping",connectorName
"Received target state change for unknown connector: {}",connector
"Handling task config update by restarting tasks {}",tasksToStop
"Herder stopping"
"Herder stopped"
"Submitting connector listing request"
"Submitting connector info request {}",connName
"Handling connector config request {}",connName
"Removing connector config {} {}",connName,configState.connectors()
"Submitting connector config write request {}",connName
"Handling connector config request {}",connName
"Submitting connector config {} {} {}",connName,allowReplace,configState.connectors()
"Submitting connector task reconfiguration request {}",connName
"Unexpected error during task reconfiguration: ",error
"Task reconfiguration for {} failed unexpectedly, this connector will not be properly reconfigured unless manually triggered.",connName
"Submitting get task configuration request {}",connName
"Submitting put task configuration request {}",connName
"Returning early because rebalance is marked as resolved (rebalanceResolved: true)"
"Join group completed, but assignment failed and we are the leader. Reading to end of config and retrying."
"Join group completed, but assignment failed and we lagging. Reading to end of config and retrying."
"Join group completed, but assignment failed. We were up to date, so just retrying."
"Catching up to assignment's config offset."
"Requesting rebalance because scheduled rebalance timeout has been reached " + "(now: {} scheduledRebalance: {}",scheduledRebalance,now
"Current config state offset {} does not match group assignment {}. Forcing rebalance.",configState.offset(),assignment.offset()
"Current config state offset {} is behind group assignment {}, reading to end of config log",configState.offset(),assignment.offset()
"Finished reading to end of log and updated config snapshot, new config log offset: {}",configState.offset()
"Finished starting connectors and tasks"
"Starting task {}",taskId
"Couldn't instantiate task {} because it has an invalid task configuration. This task will not execute until reconfigured.",taskId,t
"Starting connector {}",connectorName
"Couldn't instantiate connector " + connectorName + " because it has an invalid connector " + "configuration. This connector will not execute until reconfigured.",t
"Failed to shut down connector " + connectorName,t
"Failed to reconfigure connector's tasks, retrying after backoff:",error
"Unexpected error during connector task reconfiguration: ",error
"Task reconfiguration for {} failed unexpectedly, this connector will not be properly reconfigured unless manually triggered.",connName
"Skipping reconfiguration of connector {} since it is not running",connName
"Change in connector task count from {} to {}, writing updated task configurations",currentNumTasks,taskProps.size()
"Change in task configurations, writing updated task configurations"
"Request to leader to reconfigure connector tasks failed",e
"Connector {} config removed",connector
"Connector {} config updated",connector
"Tasks {} configs updated",tasks
"Connector {} target state change",connector
"Cleaning status information for connector {}",connector
"Finished stopping tasks in preparation for rebalance"
"Wasn't unable to resume work after last rebalance, can skip stopping connectors and tasks"
"Performing task assignment"
"Member configs: {}",memberConfigs
"Max config offset root: {}, local snapshot config offsets root: {}",maxOffset,coordinator.configSnapshot().offset()
"Was selected to perform assignments, but do not have latest config found in sync request. " + "Returning an empty configuration to trigger re-sync."
"Configured assignments: {}",configured
"Active assignments: {}",activeAssignments
"Deleted assignments: {}",deleted
"Remaining (excluding deleted) active assignments: {}",remainingActive
"Lost assignments: {}",lostAssignments
"New assignments: {}",newSubmissions
"Complete (ignoring deletions) worker assignments: {}",completeWorkerAssignment
"Complete (ignoring deletions) connector assignments: {}",connectorAssignments
"Complete (ignoring deletions) task assignments: {}",taskAssignments
"Connector and task to delete assignments: {}",toRevoke
"Connector and task to revoke assignments: {}",toRevoke
"Current complete assignments: {}",currentWorkerAssignment
"New complete assignments: {}",completeWorkerAssignment
"Incremental connector assignments: {}",incrementalConnectorAssignments
"Incremental task assignments: {}",incrementalTaskAssignments
"Actual assignments: {}",assignments
"Connectors and tasks to delete assignments: {}",toRevoke
"Found the following connectors and tasks missing from previous assignments: " + lostAssignments
"Per worker current load size; worker: {} connectors: {} tasks: {}",wl.worker(),wl.connectorsSize(),wl.tasksSize())
"No task revocation required; workers with existing load: {} workers with " + "no load {} total workers {}",existingWorkersNum,newWorkersNum,totalWorkersNum
"Task revocation is required; workers with existing load: {} workers with " + "no load {} total workers {}",existingWorkersNum,newWorkersNum,totalWorkersNum
"New rounded down (floor) average number of connectors per worker {}",floorConnectors
"Previous rounded down (floor) average number of tasks per worker {}",totalActiveTasksNumSLASHexistingWorkersNum
"New rounded down (floor) average number of tasks per worker {}",floorTasks
"Filling assignment: {} -> {}",member,assignment
"Finished assignment"
"Received assignments: {}",memberConfigs
"Assigning connector {} to {}",connector,worker.worker()
"Assigning task {} to {}",task,worker.worker()
"Usage: ConnectDistributed worker.properties"
"Stopping due to error",t
"Scanning for plugin classes. This might take a moment ..."
"Kafka cluster ID: {}",kafkaClusterId
"Kafka Connect distributed worker initialization took {}ms",time.hiResClockMs()SUBinitStart
"Failed to start Connect",e
"Usage: ConnectStandalone worker.properties connector1.properties [connector2.properties ...]"
"Kafka Connect standalone worker initializing ..."
"Scanning for plugin classes. This might take a moment ..."
"Kafka cluster ID: {}",kafkaClusterId
"Kafka Connect standalone worker initialization took {}ms",time.hiResClockMs()SUBinitStart
"Failed to create job for {}",connectorPropsFile
"Created connector {}",info.result().name()
"Stopping after connector error",t
"Stopping due to error",t
"Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden"
"Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden"
"Setting up Principal policy for ConnectorClientConfigOverride. This will allow `sasl` client configuration to be " + "overridden."
"Created topic {} on brokers at {}",topicsByName.get(topic),bootstrapServers
"Found existing topic '{}' on the brokers at {}",topic,bootstrapServers
"Unable to create topic(s) '{}' since the brokers at {} do not support the CreateTopics API."," Falling back to assume topic(s) exist or will be auto-created by the broker.",topicNameList,bootstrapServers
"Not authorized to create topic(s) '{}'." + " Falling back to assume topic(s) exist or will be auto-created by the broker.",topicNameList,bootstrapServers
"Not authorized to create topic(s) '{}'." + " Falling back to assume topic(s) exist or will be auto-created by the broker.",topicNameList,bootstrapServers
"Thread {} exiting with uncaught exception: ",getName(),e
"Starting graceful shutdown of thread {}",getName()
"Forcing shutdown of thread {}",getName()
"Creating Kafka admin client"
"Looking up Kafka cluster ID"
"Kafka cluster version is too old to return cluster ID"
"Fetching Kafka cluster ID"
"Kafka cluster ID: {}",kafkaClusterId
"Starting KafkaBasedLog with topic " + topic
"Finished reading KafkaBasedLog for topic " + topic
"Started KafkaBasedLog for topic " + topic
"Stopping KafkaBasedLog for topic " + topic
"Failed to stop KafkaBasedLog producer",e
"Failed to stop KafkaBasedLog consumer",e
"Stopped KafkaBasedLog for topic " + topic
"Starting read to end log for topic {}",topic
"Error polling: " + e
"Reading to end of offset log"
"Reading to end of log offsets {}",endOffsets
"{} started execution",this
"Finished read to end log for topic {}",topic
"Timeout while reading log to end for topic '{}'. Retrying automatically. " + "This may occur when brokers are unavailable or unreachable. Reason: {}",topic,e.getMessage()
"Unexpected exception in {}",this,t
"Starting FileOffsetBackingStore with file {}",file
"Creating admin client to manage Connect internal offset topic"
"Starting KafkaOffsetBackingStore"
"Finished reading offsets topic and starting KafkaOffsetBackingStore"
"Stopping KafkaOffsetBackingStore"
"Stopped KafkaOffsetBackingStore"
"Starting KafkaConfigBackingStore"
"Started KafkaConfigBackingStore"
"Closing KafkaConfigBackingStore"
"Closed KafkaConfigBackingStore"
"Writing connector configuration for connector '{}'",connector
"Removing connector configuration for connector '{}'",connector
"Failed to remove connector configuration from Kafka: ",e
"Failed to write connector configuration to Kafka: ",e
"Failed to write root configuration to Kafka: ",e
"Writing configuration for connector '{}' task {}",connector,index
"Writing commit for connector '{}' with {} tasks.",connector,taskCount
"Failed to write root configuration to Kafka: ",e
"Writing target state {} for connector {}",state,connector
"Creating admin client to manage Connect internal config topic"
"Unexpected in consumer callback for KafkaConfigBackingStore: ",error
"Failed to convert config data to Kafka Connect format: ",e
"Found target state ({}) in wrong format: {}",record.key(),value.value().getClass()
"Invalid data for target state for connector '{}': 'state' field should be a Map but is {}",connectorName,targetStateEQEQnullQUESnull:targetState.getClass()
"Setting target state for connector '{}' to {}",connectorName,targetState
"Invalid target state for connector '{}': {}",connectorName,targetState
"Found configuration for connector '{}' in wrong format: {}",record.key(),value.value().getClass()
"Invalid data for config for connector '{}': 'properties' field should be a Map but is {}",connectorName,newConnectorConfigEQEQnullQUESnull:newConnectorConfig.getClass()
"Updating configuration for connector '{}'",connectorName
"Ignoring task configuration because {} couldn't be parsed as a task config key",record.key()
"Ignoring task configuration for task {} because it is unexpectedly null",taskId
"Ignoring task configuration for task {} because the value is not a Map but is {}",taskId,value.value().getClass()
"Invalid data for config of task {} 'properties' field should be a Map but is {}",taskId,newTaskConfig.getClass()
"Storing new config for task {}; this will wait for a commit message before the new config will take effect.",taskId
"Discarding config update record with invalid key: {}",record.key()
"CRITICAL: Failed to serialize partition key when getting offsets for task with " + "namespace {}. No value for this data will be returned, which may break the " + "task or cause it to skip some data.",namespace,t
"Failed to fetch offsets from namespace {}: ",namespace,e
"Should be able to map {} back to a requested partition-offset key, backing " + "store may have returned invalid data",rawEntry.getKey()
"CRITICAL: Failed to deserialize offset data when getting offsets for task with" + " namespace {}. No value for this data will be returned, which may break the " + "task or cause it to skip some data. This could either be due to an error in " + "the connector implementation or incompatible schema.",namespace,t
"Creating admin client to manage Connect internal status topic"
"Failed to write status update",exception
"Invalid connector status type {}",schemaAndValue.value().getClass()
"Failed to deserialize connector status",e
"Invalid task status type {}",schemaAndValue.value().getClass()
"Failed to deserialize task status",e
"Invalid task status key {}",key
"Discarding record with invalid connector status key {}",key
"Removing status for connector {}",connector
"Received connector {} status update {}",connector,status
"Discarding record with invalid task status key {}",key
"Removing task status for {}",id
"Failed to parse task status with key {}",key
"Received task {} status update {}",id,status
"Discarding record with invalid key {}",key
"Invalid call to OffsetStorageWriter flush() while already flushing, the " + "framework should not allow this"
"Cause of serialization failure:",t
"Started MockConnector with failure delay of {} ms",delayMs
"Triggering connector failure"
"Creating single task for MockConnector"
"Started MockSourceTask at {} with failure scheduled in {} ms",startTimeMs,failureDelayMs
"Triggering source task failure"
"Started SchemaSourceTask {}-{} producing to topic {} resuming from seqno {}",name,id,topic,startingSeqno
"Started VerifiableSourceTask {}-{} producing to topic {} resuming from seqno {}",name,id,topic,startingSeqno
"Started MockSinkTask at {} with failure scheduled in {} ms",startTimeMs,failureDelayMs
"Triggering sink task failure"
"Unable to parse the value as a map; reverting to string",e
"Failed to deserialize value for header '{}' on topic '{}', so using byte array",headerKey,topic,t
"Applying SetSchemaMetadata SMT. Original schema: {}, updated schema: {}",schema,updatedSchema
"Cast field '{}' from '{}' to '{}'",field.name(),origFieldValue,newFieldValue
"Writing line to {}: {}",logFilename(),record.value()
"Flushing output stream for {}",logFilename()
"Found previous offset, trying to skip to file offset {}",lastRecordedOffset
"Error while trying to seek to previous offset in file {}: ",filename,e
"Skipped to offset {}",lastRecordedOffset
"Opened {} for reading",logFilename()
"Couldn't find file {} for FileStreamSourceTask, sleeping to wait for it to be created",logFilename()
"Error while trying to open file {}: ",filename,e
"Read {} bytes from {}",nread,logFilename()
"Read a line from {}",logFilename()
"Stopping"
"Closed input stream"
"Failed to close FileStreamSourceTask stream: ",e
"Error loading credentials file ",e
"Method #getTimestampedKeyValueStore() should be used to access a TimestampedKeyValueStore."
"Method #getTimestampedWindowStore() should be used to access a TimestampedWindowStore."
"Cannot transit to {} within {}ms",targetState,waitMs
"State transition from {} to {}",oldState,newState
"All stream threads have died. The instance will be in error state and should be closed."
"Global thread has died. The instance will be in error state and should be closed."
"Negative cache size passed in. Reverting to cache size of 0 bytes."
"Starting Streams client"
"Stopping Streams client with timeoutMillis = {} ms. You are using deprecated method. " + "Please, consider update your code.",timeoutMs
"Streams client stopped completely"
"Streams client cannot stop completely within the timeout"
"Stopping Streams client with timeoutMillis = {} ms.",timeoutMs
"Using {} default value of {} as exactly once is enabled.",COMMIT_INTERVAL_MS_CONFIG,EOS_DEFAULT_COMMIT_INTERVAL_MS
String.format(nonConfigurableConfigMessage,"consumer",config,"",clientProvidedProps.get(config),CONSUMER_DEFAULT_OVERRIDES.get(config))
String.format(nonConfigurableConfigMessage,"consumer",config,eosMessage,clientProvidedProps.get(config),CONSUMER_EOS_OVERRIDES.get(config))
String.format(nonConfigurableConfigMessage,"producer",config,eosMessage,clientProvidedProps.get(config),PRODUCER_EOS_OVERRIDES.get(config))
"Exception caught during Deserialization, " + "taskId: {}, topic: {}, partition: {}, offset: {}",context.taskId(),record.topic(),record.partition(),record.offset(),exception
"Exception caught during Deserialization, " + "taskId: {}, topic: {}, partition: {}, offset: {}",context.taskId(),record.topic(),record.partition(),record.offset(),exception
message
"Input record {} will be dropped because it has an invalid (negative) timestamp.",record
"Empty partitions for topic {}",topic
"Failed to unlock the global state directory",e
"Restoring state for global store {}",store.name()
"Failed to get end offsets for topic partitions of global store {} after {} retry attempts. " + "You can increase the number of retries via configuration parameter `retries`.",store.name(),retries,retryableException
"Failed to get end offsets for partitions {}, backing off for {} ms to retry (attempt {} of {})",topicPartitions,retryBackoffMs,attempts,retries,retryableException
"Failed to get partitions for topic {} after {} retry attempts due to timeout. " + "The broker may be transiently unavailable at the moment. " + "You can increase the number of retries via configuration parameter `retries`.",sourceTopic,retries,retryableException
"Failed to get partitions for topic {} due to timeout. The broker may be transiently unavailable at the moment. " + "Backing off for {} ms to retry (attempt {} of {})",sourceTopic,retryBackoffMs,attempts,retries,retryableException
"Restoring GlobalStore {} failed due to: {}. Deleting global store to recreate from scratch.",storeName,recoverableException.toString()
"Flushing all global globalStores registered in the state manager"
"Flushing global store={}",store.name()
"Closing global storage engine {}",entry.getKey()
"Failed to close global state store {}",entry.getKey(),e
"Skipping to close non-initialized store {}",entry.getKey()
"Failed to write offset checkpoint file to {} for global stores: {}",checkpointFile,e
"Initializing {}s {}",taskTypeName,created.keySet()
"Transitioning {} {} to restoring",taskTypeName,entry.getKey()
"Suspending running {} {}",taskTypeName,runningTaskIds()
"Close created {} {}",taskTypeName,created.keySet()
"Failed to close {}, {}",taskTypeName,task.id(),e
"Suspending {} {} failed due to the following error:",taskTypeName,task.id(),e
"After suspending failed, closing the same {} {} failed again due to the following error:",taskTypeName,task.id(),f
"Failed to close zombie {} {} due to {}; ignore and proceed.",taskTypeName,task.id(),e.toString()
"Found suspended {} {}",taskTypeName,taskId
"Resuming suspended {} {}",taskTypeName,task.id()
"Couldn't resume task {} assigned partitions {}, task partitions {}",taskId,partitions,task.partitions()
"Transitioning {} {} to running",taskTypeName,task.id()
"Failed to commit {} {} since it got migrated to another thread already. " + "Closing it as zombie before triggering a new rebalance.",taskTypeName,task.id()
"Failed to commit {} {} due to the following error:",taskTypeName,task.id(),t
"Closing suspended and not re-assigned {} {}",taskTypeName,suspendedTask.id()
"Failed to remove suspended {} {} due to the following error:",taskTypeName,suspendedTask.id(),e
"Failed to close {} {} since it got migrated to another thread already. " + "Closing it as zombie and move on.",taskTypeName,task.id()
"Failed while closing {} {} due to the following error:",task.getClass().getSimpleName(),task.id(),t
"Try to close {} {} unclean.",task.getClass().getSimpleName(),task.id()
"Failed while closing {} {} due to the following error:",task.getClass().getSimpleName(),task.id(),fatalException
"Checkpointable offsets read from checkpoint: {}",initialLoadedCheckpoints
"Created state store manager for task {}",taskId
"Registering state store {} to its state manager",storeName
"Preparing standby replica of persistent state store {} with changelog topic {}",storeName,topic
"Restoring state store {} from changelog topic {} at checkpoint {}",storeName,topic,restoreCheckpoint
"Updating store offset limit for partition {} to {}",partition,limit
"Flushing all stores registered in the state manager"
"Flushing store {}",store.name()
"Failed to flush state store {}: ",store.name(),e
"Closing its state manager and all the registered state stores"
"Closing storage engine {}",store.name()
"Failed to close state store {}: ",store.name(),e
"Skipping to close non-initialized store {}",entry.getKey()
"Checkpointable offsets updated with active acked offsets: {}",checkpointFileCache
"Writing checkpoint: {}",checkpointFileCache
"Failed to write offset checkpoint file to [{}]",checkpointFile,e
"Checkpointable offsets updated with restored offsets: {}",checkpointFileCache
"Register global stores {}",stateStores
errorLogMessage,topic,exception.getMessage(),exception
LOG_MESSAGE,topic,exception.getMessage(),exception
"Error sending records topic=[{}] and partition=[{}]; " + "The exception handler chose to CONTINUE processing in spite of this error. " + "Enable TRACE logging to view failed messages key and value.",topic,partition,exception
"Timeout exception caught when sending record to topic {}. " + "This might happen if the producer cannot send data to the Kafka cluster and thus, " + "its internal buffer fills up. " + "This can also happen if the broker is slow to respond, if the network connection to " + "the broker was interrupted, or if similar circumstances arise. " + "You can increase producer parameter `max.block.ms` to increase this timeout.",topic,e
"Flushing producer"
"Closing producer"
"Unexpected state transition from {} to {}",oldState,newState
"State transition from {} to {}",oldState,newState
"Updating global state failed. You can restart KafkaStreams to recover from this error.",recoverableException
"Error happened during initialization of the global state store; this thread has shutdown"
"Shutting down"
"Failed to close state maintainer due to the following error:",e
"Shutdown complete"
errorMsg,fatalException
"nodeToSourceTopics {}",nodeToSourceTopics
"{}updating builder with {} topic(s) with possible matching regex subscription(s)",logPrefix,subscriptionUpdates
"{}found {} topics possibly matching regex",logPrefix,topics
"Updating store offset limits {} for changelog {}",offset,partition
"Initializing state stores"
"Initializing store {}",store.name()
"Closing state manager"
"Failed to resume an active task {} due to the following error:",taskId,e
"Task {} owned partitions {} are not contained in the assignment {}",taskId,partitions,assignment
"Adding assigned standby tasks {}",assignedStandbyTasks
"Suspending all active tasks {} and standby tasks {}",active.runningTaskIds(),standby.runningTaskIds()
"Shutting down all active tasks {}, standby tasks {}, suspended tasks {}, and suspended standby tasks {}",active.runningTaskIds(),standby.runningTaskIds(),active.previousTaskIds(),standby.previousTaskIds()
"Resuming partitions {}",assignment
"Previous delete-records request has failed: {}. Try sending the new request now",deleteRecordsResult.lowWatermarks()
"Sent delete-records request: {}",recordsToDelete
"{} Found cached state dir lock for task {}",logPrefix(),taskId
"{} Acquired state dir lock for task {}",logPrefix(),taskId
"{} Found cached state dir lock for the global task",logPrefix()
"{} Acquired global state dir lock",logPrefix()
"{} Released global state dir lock",logPrefix()
"{} Released state dir lock for task {}",logPrefix(),taskId
"{} Failed to delete global state directory due to an unexpected exception",logPrefix(),e
"{} Deleting obsolete state directory {} for task {} as {}ms has elapsed (cleanup delay is {}ms).",logPrefix(),dirName,id,nowSUBlastModifiedMs,cleanupDelayMs
"{} Deleting state directory {} for task {} as user calling cleanup.",logPrefix(),dirName,id
"{} Failed to get the state directory lock.",logPrefix(),e
"{} Failed to delete the state directory.",logPrefix(),e
"{} Failed to release the state directory lock.",logPrefix()
"Deserialization error callback failed after deserialization error for record {}",rawRecord,deserializationException
"Skipping record due to deserialization error. topic=[{}] partition=[{}] offset=[{}]",rawRecord.topic(),rawRecord.partition(),rawRecord.offset(),deserializationException
"Configs:" + Utils.NL,"\t{} = {}" + Utils.NL,"\t{} = {}" + Utils.NL,"\t{} = {}",AdminClientConfig.RETRIES_CONFIG,retries,StreamsConfig.REPLICATION_FACTOR_CONFIG,replicationFactor,StreamsConfig.WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG,windowChangeLogAdditionalRetention
"Going to create topic {} with {} partitions and config {}.",internalTopicConfig.name(),internalTopicConfig.numberOfPartitions(),topicConfig
INTERRUPTED_ERROR_MESSAGE,fatalException
"Unexpected error during topic creation for {}.\n" + "Error message was: {}",topicName,cause.toString()
"Topics {} can not be made ready with {} retries left",topicsNotReady,retries
timeoutAndRetryError
"Trying to check if topics {} have been created with expected number of partitions.",topics
INTERRUPTED_ERROR_MESSAGE,fatalException
"Unexpected error during topic description for {}.\n" + "Error message was: {}",topicName,cause.toString()
errorMsg
"Source node {} extracted timestamp {} for record {}",source.name(),timestamp,deserialized
"Skipping record due to negative extracted timestamp. topic=[{}] partition=[{}] offset=[{}] extractedTimestamp=[{}] extractor=[{}]",deserialized.topic(),deserialized.partition(),deserialized.offset(),timestamp,timestampExtractor.getClass().getCanonicalName()
"Downgrading metadata version from {} to 1 for upgrade from 0.10.0.x.",SubscriptionInfo.LATEST_SUPPORTED_VERSION
"Downgrading metadata version from {} to 2 for upgrade from {}.x.",SubscriptionInfo.LATEST_SUPPORTED_VERSION,upgradeFrom
fatalException.getMessage(),fatalException
fatalException.getMessage(),fatalException
fatalException.getMessage(),fatalException
fatalException.getMessage(),fatalException
"{} is unknown yet during rebalance," + " please make sure they have been pre-created before starting the Streams application.",topic
"Received a future (version probing) subscription (version: {}). Sending empty assignment back (with supported version {}).",futureMetadataVersion,SubscriptionInfo.LATEST_SUPPORTED_VERSION
"Downgrading metadata to version {}. Latest supported version is {}.",minReceivedMetadataVersion,SubscriptionInfo.LATEST_SUPPORTED_VERSION
"Constructed client metadata {} from the member subscriptions.",clientMetadataMap
"Missing source topic {} during assignment. Returning error {}.",topic,Error.INCOMPLETE_SOURCE_TOPIC_METADATA.name()
"Created repartition topics {} from the parsed topology.",allRepartitionTopicPartitions.values()
"Partition {} is assigned to more than one tasks: {}",partition,partitionsForTask
"Partition {} is not assigned to any tasks: {}" + " Possible causes of a partition not getting assigned" + " is that another topic defined in the topology has not been" + " created when starting your streams application," + " resulting in no tasks created for this topology at all.",partition,partitionsForTask
"No partitions found for topic {}",topic
"No tasks found for topic group {}",topicGroupId
"Created state changelog topics {} from the parsed topology.",changelogTopicMetadata.values()
"Assigning tasks {} to clients {} with number of replicas {}",partitionsForTask.keySet(),states,numStandbyReplicas
"Assigned tasks to clients as {}.",states
"Sent a version {} subscription and group leader's latest supported version is {}. " + "Upgrading subscription metadata version to {} for next rebalance.",usedSubscriptionMetadataVersion,leaderSupportedVersion,leaderSupportedVersion
"Sent a version {} subscription and got version {} assignment back (successful version probing). " + "Downgrading subscription metadata to received version and trigger new rebalance.",usedSubscriptionMetadataVersion,receivedAssignmentMetadataVersion
"Sent a version {} subscription and got version {} assignment back (successful version probing). " + "Setting subscription metadata to leaders supported version {} and trigger new rebalance.",usedSubscriptionMetadataVersion,receivedAssignmentMetadataVersion,leaderSupportedVersion
"Starting to validate internal topics {} in partition assignor.",topicPartitions
"Completed validating internal topics {} in partition assignor.",topicPartitions
str
"Ignoring request to transit from PENDING_SHUTDOWN to {}: " + "only DEAD state is a valid next state",newState
"Ignoring request to transit from DEAD to {}: " + "no valid next state after DEAD",newState
"Ignoring request to transit from PARTITIONS_REVOKED to PARTITIONS_REVOKED: " + "self transition is not allowed"
"Unexpected state transition from {} to {}",oldState,newState
"State transition from {} to {}",oldState,newState
"at state {}: partitions {} assigned at the end of consumer rebalance.\n" + "\tcurrent suspended active tasks: {}\n" + "\tcurrent suspended standby tasks: {}\n",streamThread.state,assignment,taskManager.suspendedActiveTaskIds(),taskManager.suspendedStandbyTaskIds()
"Received error code {} - shutdown",streamThread.assignmentErrorCode.get()
"Skipping task creation in rebalance because we are already in {} state.",streamThread.state()
"Encountered assignment error during partition assignment: {}. Skipping task initialization",streamThread.assignmentErrorCode
"Creating tasks based on assignment."
"Error caught during partition assignment, " + "will abort the current process and re-throw at the end of rebalance",t
"partition assignment took {} ms.\n" + "\tcurrent active tasks: {}\n" + "\tcurrent standby tasks: {}\n" + "\tprevious active tasks: {}\n",time.milliseconds()SUBstart,taskManager.activeTaskIds(),taskManager.standbyTaskIds(),taskManager.prevActiveTaskIds()
"at state {}: partitions {} revoked at the beginning of consumer rebalance.\n" + "\tcurrent assigned active tasks: {}\n" + "\tcurrent assigned standby tasks: {}\n",streamThread.state,assignment,taskManager.activeTaskIds(),taskManager.standbyTaskIds()
"Error caught during partition revocation, " + "will abort the current process and re-throw at the end of rebalance: {}",t
"partition revocation took {} ms.\n" + "\tsuspended active tasks: {}\n" + "\tsuspended standby tasks: {}",time.milliseconds()SUBstart,taskManager.suspendedActiveTaskIds(),taskManager.suspendedStandbyTaskIds()
"Created task {} with assigned partitions {}",taskId,partitions
"Creating producer client for task {}",id
"Failed to close producer due to the following error:",e
"Skipped standby task {} with assigned partitions {} " + "since it does not have any state stores to materialize",taskId,partitions
"Creating restore consumer client"
"Creating shared producer client"
"Creating consumer client"
"Starting"
"StreamThread already shutdown. Not running"
"Encountered the following unexpected Kafka exception during processing, " + "this usually indicate Streams internal errors:",e
"Version probing detected. Triggering new rebalance."
"Detected task {} that got migrated to another thread. " + "This implies that this thread missed a rebalance and dropped out of the consumer group. " + "Will try to rejoin the consumer group. Below is the detailed description of the task:\n{}",ignoreAndRejoinGroup.migratedTask().id(),ignoreAndRejoinGroup.migratedTask().toString(">")
"State already transits to {}, skipping the run once call after poll request",state
logMessage,topic,resetPolicy
"Unable to locate active task for received-record partition {}. Current tasks: {}",partition,taskManager.toString(">")
"Stream task {} is already closed, probably because it got unexpectedly migrated to another thread already. " + "Notifying the thread to trigger a new rebalance immediately.",task.id()
"Committing all active tasks {} and standby tasks {} since {}ms has elapsed (commit interval is {}ms)",taskManager.activeTaskIds(),taskManager.standbyTaskIds(),nowSUBlastCommitMs,commitTimeMs
"Committed all active tasks {} and standby tasks {} in {}ms",taskManager.activeTaskIds(),taskManager.standbyTaskIds(),intervalCommitLatency
"Standby task {} is already closed, probably because it got unexpectedly migrated to another thread already. " + "Notifying the thread to trigger a new rebalance immediately.",task.id()
"Updated standby tasks {} in {}ms",taskManager.standbyTaskIds(),time.milliseconds()SUBnow
"Standby task {} is already closed, probably because it got unexpectedly migrated to another thread already. " + "Notifying the thread to trigger a new rebalance immediately.",task.id()
"Updating StandbyTasks failed. Deleting StandbyTasks stores to recreate from scratch.",recoverableException
"Standby task {} is already closed, probably because it got unexpectedly migrated to another thread already. " + "Notifying the thread to trigger a new rebalance immediately.",task.id()
"Reinitializing StandbyTask {} from changelogs {}",task,recoverableException.partitions()
"Informed to shut down"
"Shutting down"
"Failed to close task manager due to the following error:",e
"Failed to close consumer due to the following error:",e
"Failed to close restore consumer due to the following error:",e
"Shutdown complete"
"Initializing state stores"
"Resuming"
"Start processing one record [{}]",record
"Completed processing one record [{}]",record
"Encountered error extracting stacktrace from this exception",ioe
"Punctuating processor {} with timestamp {} and punctuation type {}",node.name(),timestamp,type
"Committing"
"Flushing state and producer"
"Suspending"
"Failed to close producer due to the following error:",e
"Closing processor topology"
"Could not close state manager due to the following error:",e
"Closing"
"Could not close task due to the following error:",e
"Added records into the buffered queue of partition {}, new queue size is {}",partition,newQueueSize
"Timeout exception caught when initializing transactions for task {}. " + "This might happen if the broker is slow to respond, if the network connection to " + "the broker was interrupted, or if similar circumstances arise. " + "You can increase producer parameter `max.block.ms` to increase this timeout.",id,retriable
"Failed to write offset checkpoint file to {} while re-initializing {}: {}",checkpointFile,stateStores,fatalException
"Failed to reinitialize store {}.",storeName,fatalException
"Failed to reinitialize store {}.",storeName,fatalException
"Closing all restoring stream tasks {}",restoring.keySet()
"Closing restoring task {}",task.id()
"Failed to remove restoring task {} due to the following error:",task.id(),e
"Stream task changelog partitions that have completed restoring so far: {}",restored
"Stream task {} completed restoration as all its changelog partitions {} have been applied to restore state",task.id(),task.changelogPartitions()
"Stream task {} cannot resume processing yet since some of its changelog partitions have not completed restoring: {}",task.id(),outstandingPartitions
"Committed active task {} per user request in",task.id()
"Failed to commit {} since it got migrated to another thread already. " + "Closing it as zombie before triggering a new rebalance.",task.id()
"Failed to commit StreamTask {} due to the following error:",task.id(),t
"Failed to process stream task {} since it got migrated to another thread already. " + "Closing it as zombie before triggering a new rebalance.",task.id()
"Failed to process stream task {} due to the following error:",task.id(),e
"Failed to punctuate stream task {} since it got migrated to another thread already. " + "Closing it as zombie before triggering a new rebalance.",task.id()
"Failed to punctuate stream task {} due to the following error:",task.id(),e
"Added restorer for changelog {}",restorer.partition()
"Restoring StreamTasks failed. Deleting StreamTasks stores to recreate from scratch.",recoverableException
"Reinitializing StreamTask {} for changelog {}",task,partition
"End offset cannot be found form the returned metadata; removing this partition from the current run loop"
"Start restoring state stores from changelog topics {}",initialized
"Found checkpoint {} from changelog {} for store {}.",restorer.checkpoint(),partition,restorer.storeName()
"Did not find checkpoint from changelog {} for store {}, rewinding to beginning.",partition,restorer.storeName()
"No checkpoint found for task {} state store {} changelog {} with EOS turned on. " + "Reinitializing the task and restore its state from the beginning.",task.id,restorer.storeName(),partition
"Restoring task {}'s state store {} from beginning of the changelog {} ",task.id,restorer.storeName(),partition
"Restoring partition {} from offset {} to endOffset {}",partition,startingOffset,endOffset
"Could not fetch topic metadata within the timeout, will retry in the next run loop"
"Restored from {} to {} with {} records, ending offset is {}, next starting position is {}",restorer.partition(),restorer.storeName(),records.size(),lastRestoredOffset,nextPosition
"Initializing state stores"
"Resuming"
"Committing"
"Suspending"
"Closing"
"Updating standby replicas of its state store for partition [{}]",partition
"Unable to assign {} of {} standby tasks for task [{}]. " + "There is not enough available capacity. You should " + "increase the number of threads and/or application instances " + "to maintain the requested number of standby replicas.",numStandbyReplicasSUBi,numStandbyReplicas,taskId
fatalException.getMessage(),fatalException
"Unable to decode subscription data: used version: {}; latest supported version: {}",usedVersion,LATEST_SUPPORTED_VERSION
"Skipping record due to null key. change=[{}] topic=[{}] partition=[{}] offset=[{}]",change,context().topic(),context().partition(),context().offset()
"Using grace period of [{}] as the suppress duration for node [{}].",Duration.ofMillis(grace),name
"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]",value,context().topic(),context().partition(),context().offset()
"Skipping record for expired window. " + "key=[{}] " + "topic=[{}] " + "partition=[{}] " + "offset=[{}] " + "timestamp=[{}] " + "window=[{},{}] " + "expiration=[{}] " + "streamTime=[{}]",key,context().topic(),context().partition(),context().offset(),timestamp,mergedWindow.start(),mergedWindow.end(),closeTime,observedStreamTime
"Skipping record due to null key or value. key=[{}] value=[{}] topic=[{}] partition=[{}] offset=[{}]",key,value,context().topic(),context().partition(),context().offset()
"Adding nodes to topology {} child nodes {}",streamGraphNode,streamGraphNode.children()
"Optimizing the Kafka Streams graph for repartition nodes"
"Marking KTable source nodes to optimize using source topic for changelogs "
"Found the child node of the key changer {} from the repartition {}.",keyChangingNodeChild,repartitionNodeToBeReplaced
"Removing {} from {}  children {}",keyChangingNodeChild,keyChangingNode,keyChangingNode.children()
"Updated node {} children {}",optimizedSingleRepartition,optimizedSingleRepartition.children()
"Skipping record due to null key or value. key=[{}] value=[{}] topic=[{}] partition=[{}] offset=[{}]",key,value,context().topic(),context().partition(),context().offset()
"Skipping record due to null key. change=[{}] topic=[{}] partition=[{}] offset=[{}]",change,context().topic(),context().partition(),context().offset()
"Skipping record due to null key. change=[{}] topic=[{}] partition=[{}] offset=[{}]",change,context().topic(),context().partition(),context().offset()
"Skipping record due to null key or value. key=[{}] value=[{}] topic=[{}] partition=[{}] offset=[{}]",key,value,context().topic(),context().partition(),context().offset()
"Skipping record due to null key. topic=[{}] partition=[{}] offset=[{}]",context().topic(),context().partition(),context().offset()
"Detected out-of-order KTable update for {} at offset {}, partition {}.",store.name(),context().offset(),context().partition()
"Skipping record due to null key. change=[{}] topic=[{}] partition=[{}] offset=[{}]",change,context().topic(),context().partition(),context().offset()
"Skipping record due to null key. value=[{}] topic=[{}] partition=[{}] offset=[{}]",value,context().topic(),context().partition(),context().offset()
"Skipping record for expired window. " + "key=[{}] " + "topic=[{}] " + "partition=[{}] " + "offset=[{}] " + "timestamp=[{}] " + "window=[{},{}) " + "expiration=[{}] " + "streamTime=[{}]",key,context().topic(),context().partition(),context().offset(),context().timestamp(),windowStart,windowEnd,closeTime,observedStreamTime
"Skipping record due to null key or value. key=[{}] value=[{}] topic=[{}] partition=[{}] offset=[{}]",key,value,context().topic(),context().partition(),context().offset()
"The default close will be removed in 3.0.0 -- you should overwrite it if you have implemented RocksDBConfigSetter"
"Returning empty iterator for fetch with invalid key range: from > to. " + "This may be due to serdes that don't preserve ordering when lexicographically comparing the serialized bytes. " + "Note that the built-in numerical serdes do not follow this for negative numbers"
"Warning: window end time was truncated to Long.MAX"
"Opening store {} in upgrade mode",name
"Opening store {} in regular mode",name
"Writing tmp checkpoint file {}",temp.getAbsolutePath()
"Swapping tmp checkpoint file {} {}",temp.toPath(),file.toPath()
"Skipping record for expired segment."
"Returning empty iterator for fetch with invalid key range: from > to. " + "This may be due to serdes that don't preserve ordering when lexicographically comparing the serialized bytes. " + "Note that the built-in numerical serdes do not follow this for negative numbers"
"Closing {} open iterators for store {}",openIterators.size(),name
"Warning: window end time was truncated to Long.MAX"
"Named cache {} stats on flush: #hits={}, #misses={}, #overwrites={}, #flushes={}",name,hits(),misses(),overwrites(),flushes()
"Returning empty iterator for fetch with invalid key range: from > to. " + "This may be due to serdes that don't preserve ordering when lexicographically comparing the serialized bytes. " + "Note that the built-in numerical serdes do not follow this for negative numbers"
"Closing {} open iterators for store {}",iterators.size(),name
"Cache stats on flush: #puts={}, #gets={}, #evicts={}, #flushes={}",puts(),gets(),evicts(),flushes()
"Evicted {} entries from cache {}",numEvicted,namespace
"option walTtlSeconds will be ignored: Streams does not expose RocksDB ttl functionality"
"Returning empty iterator for fetch with invalid key range: from > to. " + "This may be due to serdes that don't preserve ordering when lexicographically comparing the serialized bytes. " + "Note that the built-in numerical serdes do not follow this for negative numbers"
"Skipping record for expired segment."
"Returning empty iterator for fetch with invalid key range: from > to. " + "This may be due to serdes that don't preserve ordering when lexicographically comparing the serialized bytes. " + "Note that the built-in numerical serdes do not follow this for negative numbers"
"Closing {} open iterators for store {}",openIterators.size(),name
"Returning empty iterator for fetch with invalid key range: from > to. " + "This may be due to serdes that don't preserve ordering when lexicographically comparing the serialized bytes. " + "Note that the built-in numerical serdes do not follow this for negative numbers"
"Skipping record for expired segment."
"Returning empty iterator for fetch with invalid key range: from > to. " + "This may be due to serdes that don't preserve ordering when lexicographically comparing the serialized bytes. " + "Note that the built-in numerical serdes do not follow this for negative numbers"
"Returning empty iterator for fetch with invalid key range: from > to. " + "This may be due to serdes that don't preserve ordering when lexicographically comparing the serialized bytes. " + "Note that the built-in numerical serdes do not follow this for negative numbers"
"Returning empty iterator for fetch with invalid key range: from > to. " + "This may be due to serdes that don't preserve ordering when lexicographically comparing the serialized bytes. " + "Note that the built-in numerical serdes do not follow this for negative numbers"
"Error destroying {}",segment,e
"Unable to parse segmentName {} to a date. This segment will be skipped",segmentName
"Invoking {} at {}",callable,now
"Scheduling {} for {} ms from now.",callable,delayMs
"Metric {} failed with exception",opName,t
"Error deleting {}",file.getAbsolutePath(),e
"Verified the error result of AdminClient#listTopics"
"Injecting timeout for {}.",call
"callHasExpired({}) = {}",call,ret
"Failed to send SSL Close message",ie
"SSLHandshake NEED_TASK channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}",channelId,appReadBuffer.position(),netReadBuffer.position(),netWriteBuffer.position()
"SSLHandshake NEED_WRAP channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}",channelId,appReadBuffer.position(),netReadBuffer.position(),netWriteBuffer.position()
"SSLHandshake NEED_WRAP channelId {}, handshakeResult {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}",channelId,handshakeResult,appReadBuffer.position(),netReadBuffer.position(),netWriteBuffer.position()
"SSLHandshake NEED_UNWRAP channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}",channelId,appReadBuffer.position(),netReadBuffer.position(),netWriteBuffer.position()
"SSLHandshake NEED_UNWRAP channelId {}, handshakeResult {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}",channelId,handshakeResult,appReadBuffer.position(),netReadBuffer.position(),netWriteBuffer.position()
"SSL handshake completed successfully with peerHost '{}' peerPort {} peerPrincipal '{}' cipherSuite '{}'",session.getPeerHost(),session.getPeerPort(),peerPrincipal(),session.getCipherSuite()
"SSLHandshake FINISHED channelId {}, appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {} ",channelId,appReadBuffer.position(),netReadBuffer.position(),netWriteBuffer.position()
"SSLHandshake handshakeWrap {}",channelId
"SSLHandshake handshakeUnwrap {}",channelId
"SSLHandshake handshakeUnwrap: handshakeStatus {} status {}",handshakeStatus,result.getStatus()
"Renegotiation requested, but it is not supported, channelId {}, " + "appReadBuffer pos {}, netReadBuffer pos {}, netWriteBuffer pos {}",channelId,appReadBuffer.position(),netReadBuffer.position(),netWriteBuffer.position()
"SSL peer is not authenticated, returning ANONYMOUS instead"
"SSLEngine.closeInBound() raised an exception.",e
"Failed to flush all bytes before closing channel",e
"SSLException while unwrapping data after IOException, original IOException will be propagated",sslException
"Failed to create channel due to ",e
"Broker low on memory - could not allocate buffer of size {} for source {}",requestedBufferSize,source
"Failed to create channel due to ",e
"Unexpected exception during send, closing connection {} and rethrowing exception {}",connectionId,e
"Created socket with SO_RCVBUF = {}, SO_SNDBUF = {}, SO_TIMEOUT = {} to node {}",socketChannel.socket().getReceiveBufferSize(),socketChannel.socket().getSendBufferSize(),socketChannel.socket().getSoTimeout(),channel.id()
"Should never happen: re-authentication latency for a re-authenticated channel was null; continuing..."
"Successfully {}authenticated with {}",isReauthenticationQUES"re-":"",channel.socketDescription()
"Connection with {} disconnected",desc,e
"Failed {}authentication with {} ({})",isReauthenticationQUES"re-":"",desc,exceptionMessage
"Unexpected error from {}; closing connection",desc,e
"About to close the idle connection from {} due to being idle for {} millis",connectionId,(currentTimeNanosSUBexpiredConnection.getValue())SLASH1000SLASH1000
"Exception handling close on authentication failure node {}",channel.id(),e
"Tracking closing connection {} to process outstanding requests",channel.id()
"Exception closing connection to node {}:",channel.id(),e
"Failed to create channel due to ",e
b.toString()
"The configuration '{}' was supplied but isn't a known config.",key
"ClassNotFoundException exception occurred: " + entry.getValue()
"Checking login config for Zookeeper JAAS context {}",zkSecuritySysConfigString()
"JAAS configuration is present, but system property " + ZK_SASL_CLIENT + " is set to false, which disables " + "SASL in the ZooKeeper client"
"Server config {} should be prefixed with SASL mechanism name, ignoring config",SaslConfigs.SASL_JAAS_CONFIG
"System property '" + JaasUtils.JAVA_LOGIN_CONFIG_PARAM + "' and Kafka SASL property '" + SaslConfigs.SASL_JAAS_CONFIG + "' are not set, using default JAAS configuration."
"System property '" + JaasUtils.JAVA_LOGIN_CONFIG_PARAM + "' is not set, using default JAAS " + "configuration."
"Successfully logged in."
"Creating SaslClient: client={};service={};serviceHostname={};mechs={}",clientPrincipalName,servicePrincipal,host,Arrays.toString(mechs)
"Set SASL client state to {}",saslState
"Invalid SASL mechanism response, server may be expecting only GSSAPI tokens"
"Finished {} with session expiration in {} ms and session re-authentication on or after {} ms",authenticationOrReauthenticationText(),positiveSessionLifetimeMs,sessionLifetimeMsToUse
"Finished {} with no session expiration and no session re-authentication",authenticationOrReauthenticationText()
"{} for mechanism={}: {}",BrokerSecurityConfigs.CONNECTIONS_MAX_REAUTH_MS,mechanism,connectionsMaxReauthMsByMechanism.get(mechanism)
"Creating SaslServer for {} with mechanism {}",kerberosName,saslMechanism
"Cannot add private credential to subject; clients authentication may fail",ex
"Failed during {}: {}",reauthInfo.authenticationOrReauthenticationText(),e.getMessage()
"Beginning re-authentication: {}",this
"Set SASL server state to {} during {}",saslState,reauthInfo.authenticationOrReauthenticationText()
"Handling Kafka request {} during {}",apiKey,reauthInfo.authenticationOrReauthenticationText()
"Received client packet of length {} starting with bytes 0x{}, process as GSSAPI packet",requestBytes.length,tokenBuilder
"First client packet is not a SASL mechanism request, using default mechanism GSSAPI"
"Using SASL mechanism '{}' provided by client",clientMechanism
"SASL mechanism '{}' requested by client is not supported",clientMechanism
badMechanismErrorMessage
"Authentication complete; session max lifetime from broker config={} ms, credential expiration={} ({} ms); session expiration = {} ({} ms), sending {} ms to client",connectionsMaxReauthMs,NEWDate(credentialExpirationMs),Long.valueOf(credentialExpirationMs.longValue()SUBauthenticationEndMs),NEWDate(authenticationEndMs + retvalSessionLifetimeMs),retvalSessionLifetimeMs,retvalSessionLifetimeMs
"Authentication complete; session max lifetime from broker config={} ms, credential expiration={} ({} ms); no session expiration, sending 0 ms to client",connectionsMaxReauthMs,NEWDate(credentialExpirationMs),Long.valueOf(credentialExpirationMs.longValue()SUBauthenticationEndMs)
"Authentication complete; session max lifetime from broker config={} ms, no credential expiration; session expiration = {} ({} ms), sending {} ms to client",connectionsMaxReauthMs,NEWDate(authenticationEndMs + retvalSessionLifetimeMs),retvalSessionLifetimeMs,retvalSessionLifetimeMs
"Authentication complete; session max lifetime from broker config={} ms, no credential expiration; no session expiration, sending 0 ms to client",connectionsMaxReauthMs
"Client supplied realm: {} ",rc.getDefaultText()
"Successfully authenticated client: authenticationID={}; authorizationID={}.",authenticationID,authorizationID
"Extensions callback is not supported by client callback handler {}, no extensions will be added",callbackHandler
"Setting SASL/{} client state to {}",mechanism,state
"Unsupported extensions will be ignored, supported {}, provided {}",SUPPORTED_EXTENSIONS,scramExtensions.map().keySet()
"Setting SASL/{} server state to {}",mechanism,state
"Created new {} SSL engine builder with keystore {} truststore {}",mode,newSslEngineBuilder.keystore(),newSslEngineBuilder.truststore()
"Validation of dynamic config update of SSLFactory failed.",e
"Unrecognized client authentication configuration {}.  Falling " + "back to NONE.  Recognized client authentication configurations are {}.",key,String.join(", ",SslClientAuth.VALUES.stream().map(aARROWa.name()).collect(Collectors.toList()))
"Created SSL context with keystore {}, truststore {}",keystore,truststore
"Modification time of key store could not be obtained: " + path,e
"Logged in without a token, this login cannot be used to establish client connections"
"Login succeeded; invoke commit() to commit it; current committed token count={}",committedTokenCount()
e.getMessage(),e
"Login failed: {} : {} (URI={})",tokenCallback.errorCode(),tokenCallback.errorDescription(),tokenCallback.errorUri()
e.getMessage(),e
"CallbackHandler {} does not support SASL extensions. No extensions will be added",callbackHandler.getClass().getName()
"SASL Extensions cannot be null. Check whether your callback handler is explicitly setting them as null."
"Nothing here to log out"
"Logging out my token; current committed token count = {}",committedTokenCount()
"Done logging out my token; committed token count is now {}",committedTokenCount()
"No tokens to logout for this login"
"Logging out my extensions"
"Done logging out my extensions"
"No extensions to logout for this login"
"Nothing here to commit"
"Committing my token; current committed token count = {}",committedTokenCount()
"Done committing my token; committed token count is now {}",committedTokenCount()
"No tokens to commit, this login cannot be used to establish client connections"
"Login aborted"
"Nothing here to abort"
"Sending %%x01 response to server after receiving an error: {}",jsonErrorResponse
"Successfully authenticated as {}",callback.token().principalName()
"Setting SASL/{} client state to {}",OAuthBearerLoginModule.OAUTHBEARER_MECHANISM,state
"Extensions callback is not supported by client callback handler {}, no extensions will be added",callbackHandler()
"Received %x01 response from client after it received our error"
e.getMessage()
errorMessage
"Successfully authenticate User={}",token.principalName()
errorMessage
msg,e
"Found {} OAuth Bearer tokens in Subject's private credentials; the oldest expires at {}, will use the newest, which expires at {}",sortedByLifetime.size(),NEWDate(sortedByLifetime.first().lifetimeMs()),NEWDate(sortedByLifetime.last().lifetimeMs())
"Found expiring credential with principal '{}'.",token.principalName()
"[Principal={}]: Expiring credential re-login thread started.",principalLogText()
"[Principal={}]: Expiring credential re-login sleep time was calculated to be in the past! Will explicitly adjust. ({})",principalLogText(),NEWDate(nextRefreshMs)
"[Principal={}]: Expiring credential re-login sleeping until: {}",principalLogText(),NEWDate(nextRefreshMs)
"[Principal={}]: Expiring credential re-login thread has been interrupted and will exit.",principalLogText()
e.getMessage(),e
String.format("[Principal=%s]: LoginException during login retry; will sleep %d seconds before trying again.",principalLogText(),DELAY_SECONDS_BEFORE_NEXT_RETRY_WHEN_RELOGIN_FAILS),loginException
"[Principal={}]: Interrupted while trying to perform a subsequent expiring credential re-login after one or more initial re-login failures: re-login thread exiting now: {}",principalLogText(),String.valueOf(loginException.getMessage())
"Successfully logged in."
"[Principal={}]: Current clock: {} is later than expiry {}. This may indicate a clock skew problem." + " Check that this host's and remote host's clocks are in sync. Not starting refresh thread." + " This process is likely unable to authenticate SASL connections (for example, it is unlikely" + " to be able to authenticate a connection with a Kafka Broker).",principalLogText(),NEWDate(nowMs),NEWDate(expireTimeMs)
"[Principal={}]: It is an expiring credential",principalLogText()
"[Principal={}]: Interrupted while waiting for re-login thread to shutdown.",principalLogText(),e
"[Principal={}]: No Expiring credential found: will try again at {}",principalLogText(),NEWDate(retvalNextRefreshMs)
"[Principal={}]: Current clock: {} is later than expiry {}. This may indicate a clock skew problem." + " Check that this host's and remote host's clocks are in sync. Exiting refresh thread.",principalLogText(),NEWDate(relativeToMs),NEWDate(expireTimeMs)
"[Principal={}]: Expiring credential already expired at {}: will try to refresh again at {}",principalLogText(),NEWDate(expireTimeMs),NEWDate(retvalNextRefreshMs)
"[Principal={}]: Expiring credential refresh thread exiting because the" + " expiring credential's current expiration time ({}) exceeds the latest possible refresh time ({})." + " This process will not be able to authenticate new SASL connections after that" + " time (for example, it will not be able to authenticate a new connection with a Kafka Broker).",principalLogText(),NEWDate(expireTimeMs),NEWDate(absoluteLastRefreshTimeMs.longValue())
"[Principal={}]: Expiring credential valid from {} to {}",expiringCredential.principalName(),NEWjava.util.Date(startMs),NEWjava.util.Date(expireTimeMs)
"[Principal={}]: Expiring credential expires at {}, so buffer times of {} and {} seconds" + " at the front and back, respectively, cannot be accommodated.  We will refresh at {}.",principalLogText(),NEWDate(expireTimeMs),refreshMinPeriodSeconds,clientRefreshBufferSeconds,NEWDate(retvalRefreshMs)
"[Principal={}]: Proposed refresh time of {} extends into the desired buffer time of {} seconds before expiration, so refresh it at the desired buffer begin point, at {}",expiringCredential.principalName(),NEWDate(proposedRefreshMs),clientRefreshBufferSeconds,NEWDate(beginningOfEndBufferTimeMs)
"[Principal={}]: Expiring credential re-login thread time adjusted from {} to {} since the former is sooner " + "than the minimum refresh interval ({} seconds from now).",principalLogText(),NEWDate(proposedRefreshMs),NEWDate(endOfMinRefreshBufferTime),refreshMinPeriodSeconds
"Initiating logout for {}",principalLogTextPriorToLogout
"Initiating re-login for {}, logout() still needs to be called on a previous login = {}",principalName,optionalCredentialToLogoutBANGEQnull
"[Principal={}]: It is an expiring credential after re-login as expected",principalLogText()
"Successfully validated token with principal {}: {}",unsecuredJwt.principalName(),unsecuredJwt.claims().toString()
"Token not provided, this login cannot be used to establish client connections"
"Retrieved token with principal {}",jws.principalName()
"Kerberos return code could not be determined from {} due to {}",exception,e
"[Principal={}]: It is not a Kerberos ticket",principal
"[Principal={}]: It is a Kerberos ticket",principal
"[Principal={}]: TGT refresh thread started.",principal
"[Principal={}]: No TGT found: will try again at {}",principal,nextRefreshDate
"The TGT cannot be renewed beyond the next expiry date: {}." + "This process will not be able to authenticate new SASL connections after that " + "time (for example, it will not be able to authenticate a new connection with a Kafka " + "Broker).  Ask your system administrator to either increase the " + "'renew until' time by doing : 'modprinc -maxrenewlife {} ' within " + "kadmin, or instead, to generate a keytab for {}. Because the TGT's " + "expiry cannot be further extended by refreshing, exiting refresh thread now.",expiryDate,principal,principal
"[Principal={}]: TGT refresh thread time adjusted from {} to {} since the former is sooner " + "than the minimum refresh interval ({} seconds) from now.",principal,until,newUntil,minTimeBeforeReloginSLASH1000
"[Principal={}]: Next refresh: {} is later than expiry {}. This may indicate a clock skew problem." + "Check that this host and the KDC hosts' clocks are in sync. Exiting refresh thread.",principal,nextRefreshDate,expiryDate
"[Principal={}]: TGT refresh sleeping until: {}",principal,until
"[Principal={}]: TGT renewal thread has been interrupted and will exit.",principal
"[Principal={}]: NextRefresh: {} is in the past: exiting refresh thread. Check" + " clock sync between this host and KDC - (KDC's clock is likely ahead of this host)." + " Manual intervention will be required for this client to successfully authenticate." + " Exiting refresh thread.",principal,nextRefreshDate
"[Principal={}]: Running ticket cache refresh command: {} {}",principal,kinitCmd,kinitArgs
"[Principal={}]: Interrupted while renewing TGT, exiting Login thread",principal
"[Principal={}]: Could not renew TGT due to problem running shell command: '{} {}'. " + "Exiting refresh thread.",principal,kinitCmd,kinitArgs,e
"[Principal={}]: Interrupted during login retry after LoginException:",principal,le
"[Principal={}]: Could not refresh TGT.",principal,le
"[Principal={}]: Failed to refresh TGT: refresh thread exiting now.",principal,le
"[Principal={}]: Error while waiting for Login thread to shutdown.",principal,e
"[Principal={}]: TGT valid starting at: {}",principal,tgt.getStartTime()
"[Principal={}]: TGT expires: {}",principal,tgt.getEndTime()
"Found TGT with client principal '{}' and server principal '{}'.",ticket.getClient().getName(),ticket.getServer().getName()
"[Principal={}]: Not attempting to re-login since the last re-login was attempted less than {} seconds before.",principal,minTimeBeforeReloginSLASH1000
"Initiating logout for {}",principal
"Initiating re-login for {}",principal
"Unexpected error code: {}.",code
"refused to allocate buffer of size {}",sizeBytes
"allocated buffer of size {} ",justAllocated.capacity()
"released buffer of size {}",justReleased.capacity()
"allocated buffer of size {} and identity {}",sizeBytes,ref.hashCode
"released buffer of size {} and identity {}",metadata.sizeBytes,ref.hashCode
"Reclaimed buffer of size {} and identity {} that was not properly release()ed. This is a bug.",metadata.sizeBytes,ref.hashCode
"interrupted",e
"GC listener shutting down"
"Received unknown control record key version {}. Parsing as version {}",version,CURRENT_CONTROL_RECORD_KEY_VERSION
"Received end transaction marker value version {}. Parsing as version {}",version,CURRENT_END_TXN_MARKER_VERSION
"mismatch in sending bytes over socket; expected: {} actual: {}",size,totalWritten
"Bytes written as part of multi-send call: {}, total bytes written so far: {}, expected bytes to write: {}",totalWrittenPerCall,totalWritten,size
"Record batch from {} with last offset {} exceeded max record batch size {} after cleaning " + "(new size is {}). Consumers with version earlier than 0.10.1.0 may need to " + "increase their fetch sizes.",partition,batch.lastOffset(),maxRecordBatchSize,filteredBatchSize
"Constructed overflow message batch for partition {} with length={}",topicPartition(),remaining
"Down-converted records for partition {} with length={}",topicPartition(),convertedRecords.sizeInBytes()
"Non-atomic move of {} to {} succeeded after atomic move failed due to {}",source,target,outer.getMessage()
"Failed to close {} with type {}",name,closeable.getClass().getName(),t
"Failed to close {} with type {}",name,closeable.getClass().getName(),t
"Registered signal handlers for " + String.join(", ",SIGNALS)
"Terminating process due to signal {}",signal
addPrefix(message)
addPrefix(message),arg
addPrefix(message),arg1,arg2
addPrefix(message),args
addPrefix(msg),t
marker,addPrefix(msg)
marker,addPrefix(format),arg
marker,addPrefix(format),arg1,arg2
marker,addPrefix(format),argArray
marker,addPrefix(msg),t
addPrefix(message)
addPrefix(message),arg
addPrefix(message),arg1,arg2
addPrefix(message),args
addPrefix(msg),t
marker,addPrefix(msg)
marker,addPrefix(format),arg
marker,addPrefix(format),arg1,arg2
marker,addPrefix(format),arguments
marker,addPrefix(msg),t
addPrefix(message)
addPrefix(message),arg
addPrefix(message),arg1,arg2
addPrefix(message),args
addPrefix(msg),t
marker,addPrefix(msg)
marker,addPrefix(format),arg
marker,addPrefix(format),arg1,arg2
marker,addPrefix(format),arguments
marker,addPrefix(msg),t
addPrefix(message)
addPrefix(message),arg
addPrefix(message),arg1,arg2
addPrefix(message),args
addPrefix(msg),t
marker,addPrefix(msg)
marker,addPrefix(format),arg
marker,addPrefix(format),arg1,arg2
marker,addPrefix(format),arguments
marker,addPrefix(msg),t
addPrefix(message)
addPrefix(message),arg
addPrefix(message),arg1,arg2
addPrefix(message),args
addPrefix(msg),t
marker,addPrefix(msg)
marker,addPrefix(format),arg
marker,addPrefix(format),arg1,arg2
marker,addPrefix(format),arguments
marker,addPrefix(msg),t
"Uncaught exception in thread '{}':",name,e
"Error reading the error stream",ioe
"Interrupted while reading the error stream",ie
"Error while closing the input stream",ioe
"Error while closing the error stream",ioe
"Error while loading kafka-version.properties: {}",e.getMessage()
"Error registering AppInfo mbean",e
"Error unregistering AppInfo mbean",e
"Kafka version: {}",AppInfoParser.getVersion()
"Kafka commitId: {}",AppInfoParser.getCommitId()
"Kafka startTimeMs: {}",startTimeMs
"Error getting JMX attribute '{}'",name,e
"Added sensor with name {}",name
"Removed sensor with name {}",name
"Error when removing metric from " + reporter.getClass().getName(),e
"Removed metric named {}",metricName
"Error when registering metric on " + reporter.getClass().getName(),e
"Registered metric named {}",metricName
"Removing expired sensor {}",sensorEntry.getKey()
"Error when closing " + reporter.getClass().getName(),e
"Hostname for node {} changed from {} to {}.",id,connectionState.host(),host
"Disabling exponential reconnect backoff because {} is set, but {} is not.",RECONNECT_BACKOFF_MS_CONFIG,RECONNECT_BACKOFF_MAX_MS_CONFIG
"Couldn't resolve server {} from {} as DNS resolution of the canonical hostname {} failed for {}",url,CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG,resolvedCanonicalName,host
"Couldn't resolve server {} from {} as DNS resolution failed for {}",url,CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG,host
"Determining if we should replace existing epoch {} with new epoch {}",oldEpoch,epoch
"Updating last seen epoch from {} to {} for partition {}",oldEpoch,epoch,topicPartition
"Not replacing existing epoch {} with new epoch {} for partition {}",oldEpoch,epoch,topicPartition
"Cluster ID: {}",newClusterId
"Updated cluster metadata updateVersion {} to {}",this.updateVersion,this.cache
"Metadata response reported invalid topics {}",cluster.invalidTopics()
"Topic authorization failed for topics {}",cluster.unauthorizedTopics()
"Requesting metadata update for partition {} due to error {}",NEWTopicPartition(metadata.topic(),partitionMetadata.partition()),partitionMetadata.error()
"Requesting metadata update for topic {} due to error {}",metadata.topic(),metadata.error()
"Manually disconnected from {}. Removed requests: {}.",nodeId,Utils.join(requestTypes,", ")
"No version information found when sending {} with correlation id {} to node {}. " + "Assuming version {}.",clientRequest.apiKey(),clientRequest.correlationId(),nodeId,version
"Sending {} {} with correlation id {} to node {}",clientRequest.apiKey(),request,clientRequest.correlationId(),destination
"Using older server API v{} to send {} {} with correlation id {} to node {}",header.apiVersion(),clientRequest.apiKey(),request,clientRequest.correlationId(),destination
"Unexpected error during I/O",e
"Uncaught error in request completion:",e
"Attempting to close NetworkClient that has already been closed."
"Removing node {} from least loaded node selection since it is neither ready " + "for sending or connecting",node
"Found least loaded node {} with {} inflight requests",foundReady,inflight
"Found least loaded connecting node {}",foundConnecting
"Found least loaded node {} with no active connection",foundCanConnect
"Least loaded node selection failed to find an available node"
"Connection to node {} ({}) failed authentication due to: {}",nodeId,disconnectState.remoteAddress(),exception.getMessage()
"Connection to node {} ({}) terminated during authentication. This may happen " + "due to any of the following reasons: (1) Authentication failed due to invalid " + "credentials with brokers older than 1.0.0, (2) Firewall blocking Kafka TLS " + "traffic (eg it may only allow HTTPS traffic), (3) Transient network issue.",nodeId,disconnectState.remoteAddress()
"Connection to node {} ({}) could not be established. Broker may not be available.",nodeId,disconnectState.remoteAddress()
"Cancelled request {} {} with correlation id {} due to node {} being disconnected",request.header.apiKey(),request.request,request.header.correlationId(),nodeId
"Disconnecting from node {} due to request timeout.",nodeId
"Connection to node {} is throttled for {} ms until timestamp {}",nodeId,throttleTimeMs,now + throttleTimeMs
"Completed receive from node {} for {} with correlation id {}, received {}",req.destination,req.header.apiKey(),req.header.correlationId(),responseStruct
"Received error {} from node {} when making an ApiVersionsRequest with correlation id {}. Disconnecting.",apiVersionsResponse.error(),node,req.header.correlationId()
"Recorded API versions for node {}: {}",node,nodeVersionInfo
"Node {} disconnected.",node
"Completed connection to node {}. Fetching API versions.",node
"Completed connection to node {}. Ready.",node
"Initiating API versions fetch from node {}.",node
"Initiating connection to node {} using address {}",node,address
"Error connecting to node {}",node,e
"Give up sending metadata request since no node is available"
"Bootstrap broker {} disconnected",node
"{} partitions have leader brokers without a matching listener, including {}",count,missingListenerPartitions.subList(0,Math.min(10,count))
"Error while fetching metadata with correlation id {} : {}",requestHeader.correlationId(),errors
"Ignoring empty metadata response with correlation id {}.",requestHeader.correlationId()
"Sending metadata request {} to node {}",metadataRequest,node
"Built full fetch {} for node {} with {}.",nextMetadata,node,partitionsToLogString(next.keySet())
"Built incremental fetch {} for node {}. Added {}, altered {}, removed {} " + "out of {}",nextMetadata,node,partitionsToLogString(added),partitionsToLogString(altered),partitionsToLogString(removed),partitionsToLogString(sessionPartitions.keySet())
"Node {} was unable to process the fetch request with {}: {}.",node,nextMetadata,response.error()
"Node {} sent an invalid full fetch response with {}",node,problem
"Node {} sent a full fetch response{}",node,responseDataToLogString(response)
"Node {} sent an invalid incremental fetch response with {}",node,problem
"Error sending fetch request {} to node {}: {}.",nextMetadata,node,t.toString()
"Starting the Kafka producer"
"Kafka producer started"
"{} should be equal to or larger than {} + {}. Setting it to {}.",ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG,ProducerConfig.LINGER_MS_CONFIG,ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG,deliveryTimeoutMs
"Instantiated a transactional producer."
"Instantiated an idempotent producer."
"Overriding the default {} to all since idempotence is enabled.",ProducerConfig.ACKS_CONFIG
"Attempting to append record {} with callback {} to topic {} partition {}",record,callback,record.topic(),partition
"Retrying append due to new batch creation for topic {} partition {}. The old partition was {}",record.topic(),partition,prevPartition
"Waking up the sender since topic {} partition {} is either full or getting a new batch",record.topic(),partition
"Exception occurred during message send:",e
"Requesting metadata update for partition {} of topic {}.",partition,topic
"Requesting metadata update for topic {}.",topic
"Flushing accumulated records in producer."
"Closing the Kafka producer with timeoutMillis = {} ms.",timeoutMs
"Overriding close timeout {} ms to 0 ms in order to prevent useless blocking due to self-join. " + "This means you have incorrectly invoked close with a non-zero timeout from the producer call-back.",timeoutMs
"Interrupted while joining ioThread",t
"Proceeding to force close the producer since pending requests could not be completed " + "within timeout {} ms.",timeoutMs
"Kafka producer has been closed"
"Error when sending message to topic {} with key: {}, value: {} with error:",topic,keyString,valueString,e
"Aborting batch for partition {}",topicPartition,exception
"Successfully produced messages to {} with base offset {}.",topicPartition,baseOffset
"Failed to produce messages to {} with base offset {}.",topicPartition,baseOffset,exception
"Error executing user-provided callback on message for topic-partition '{}'",topicPartition,e
"Begin adding offsets {} for consumer group {} to transaction",offsets,consumerGroupId
"Begin adding new partition {} to transaction",topicPartition
"Skipping transition to abortable error state since the transaction is already being " + "aborted. Underlying exception: ",exception
"ProducerId set to {} with epoch {}",producerIdAndEpoch.producerId,producerIdAndEpoch.epoch
"Partition {} keeps lastOffset at {}",batch.topicPartition,lastOffset
"Ignoring completed batch {} with producer id {}, epoch {}, and sequence number {} " + "since the producerId has been reset internally",batch,batch.producerId(),batch.producerEpoch(),batch.baseSequence()
"ProducerId: {}; Set last ack'd sequence number for topic-partition {} to {}",batch.producerId(),batch.topicPartition,lastAckedSequence(batch.topicPartition).orElse(SUB1)
"Ignoring failed batch {} with producer id {}, epoch {}, and sequence number {} " + "since the producerId has been reset internally",batch,batch.producerId(),batch.producerEpoch(),batch.baseSequence(),exception
"The broker returned {} for topic-partition {} with producerId {}, epoch {}, and sequence number {}",exception,batch.topicPartition,batch.producerId(),batch.producerEpoch(),batch.baseSequence()
"producerId: {}, send to partition {} failed fatally. Reducing future sequence numbers by {}",batch.producerId(),batch.topicPartition,batch.recordCount
"Resetting sequence number of batch with current sequence {} for partition {} to {}",inFlightBatch.baseSequence(),batch.topicPartition,newSequence
"Resetting sequence number of batch with current sequence {} for partition {} to {}",inFlightBatch.baseSequence(),inFlightBatch.topicPartition,sequence.value
"Marking partition {} unresolved",topicPartition
"Not sending transactional request {} because we are in an error state",nextRequestHandler.requestBuilder()
"Not sending EndTxn for completed transaction since no partitions " + "or offsets were successfully added"
"Request {} dequeued for sending",nextRequestHandler.requestBuilder()
"Transition from state {} to error state {}",currentState,target,lastError
"Transition from state {} to {}",currentState,target
"Enqueuing transactional request {}",requestHandler.requestBuilder()
"Disconnected from {}. Will retry.",response.destination()
"Received transactional response {} for request {}",response.responseBody(),requestBuilder()
"Did not attempt to add partition {} to transaction because other partitions in the " + "batch had errors.",topicPartition
"Could not add partition {} due to unexpected error {}",topicPartition,error
"Successfully added partitions {} to transaction",partitions
"Successfully added partition for consumer group {} to transaction",builder.consumerGroupId()
"Received TxnOffsetCommit response for consumer group {}: {}",builder.data.groupId(),errors
"Error executing interceptor onSend callback for topic: {}, partition: {}",record.topic(),record.partition(),e
"Error executing interceptor onSend callback",e
"Failed to close producer interceptor ",e
"Allocating a new {} byte message buffer for topic {} partition {}",size,tp.topic(),tp.partition()
"Skipping next batch expiry time update due to addition overflow: " + "batch.createMs={}, deliveryTimeoutMs={}",batch.createdMs,deliveryTimeoutMs
"Reordered incoming batch with sequence {} for partition {}. It was placed in the queue at " + "position {}",batch.baseSequence(),batch.topicPartition,orderedBatches.size()
"Assigned producerId {} and producerEpoch {} to batch with base sequence " + "{} being sent to partition {}",producerIdAndEpoch.producerId,producerIdAndEpoch.epoch,batch.baseSequence(),tp
"Removing unused topic {} from the metadata list, expiryMs {} now {}",topic,expireMs,nowMs
"Starting Kafka producer I/O thread."
"Uncaught error in kafka producer I/O thread: ",e
"Beginning shutdown of Kafka producer I/O thread, sending remaining records."
"Uncaught error in kafka producer I/O thread: ",e
"Aborting incomplete transaction due to shutdown"
"Uncaught error in kafka producer I/O thread: ",e
"Aborting incomplete transactional requests due to forced shutdown"
"Aborting incomplete batches due to forced shutdown"
"Failed to close network client",e
"Shutdown of Kafka producer I/O thread has completed."
"Requesting metadata update due to unknown leader topics from the batched records: {}",result.unknownLeaderTopics
"Expired {} batches in accumulator",expiredBatches.size()
"Nodes with data ready to send: {}",result.readyNodes
"Sending transactional request {} to node {}",requestBuilder,targetNode
"Disconnect from {} while trying to send request {}. Going " + "to back off and retry.",targetNode,requestBuilder,e
"Aborting producer batches due to fatal error",exception
"Retriable error from InitProducerId response",error.message()
"Could not find an available broker to send InitProducerIdRequest to. Will back off and retry."
"Broker {} disconnected while awaiting InitProducerId response",node,e
"Retry InitProducerIdRequest in {}ms.",retryBackoffMs
"Cancelled request with header {} due to node {} being disconnected",requestHeader,response.destination()
"Cancelled request {} due to a version mismatch with node {}",response,response.destination(),response.versionMismatch()
"Received produce response from node {} with correlation id {}",response.destination(),correlationId
"Got error produce response with correlation id {} on topic-partition {}, retrying ({} attempts left). Error: {}",correlationId,batch.topicPartition,this.retriesSUBbatch.attempts()SUB1,error
"Received unknown topic or partition error in produce request on partition {}. The " + "topic-partition may not exist or the user may not have Describe access to it",batch.topicPartition
"Received invalid metadata error in produce request on partition {} due to {}. Going " + "to request metadata update now",batch.topicPartition,error.exception().toString()
"Sent produce request to {}: {}",nodeId,requestBuilder
"Kafka admin client initialized"
"Initiating close operation."
"Moving hard shutdown time forward."
"Hard shutdown time is already earlier than requested."
"Waiting for the I/O thread to exit. Hard shutdown in {} ms.",deltaMs
"Kafka admin client closed."
"Interrupted while joining I/O thread",e
"{} aborted at {} after {} attempt(s)",this,now,tries,NEWException(prettyPrintException(throwable))
"{} attempting protocol downgrade and then retry.",this
"{} timed out at {} after {} attempt(s)",this,now,tries,NEWException(prettyPrintException(throwable))
"{} failed with non-retriable exception after {} attempt(s)",this,tries,NEWException(prettyPrintException(throwable))
"{} failed after {} attempt(s)",this,tries,NEWException(prettyPrintException(throwable))
"{} failed: {}. Beginning retry #{}",this,prettyPrintException(throwable),tries
"Timed out {} pending calls.",numTimedOut
"Timed out {} call(s) with assigned nodes.",numTimedOut
"Trying to choose nodes for {} at {}",pendingCalls,now
"Assigned {} to node {}",call,node
"Unable to assign {} to a node.",call
"Client is not ready to send to {}. Must delay {} ms",node,nodeTimeout
"Sending {} to {}. correlationId={}",requestBuilder,node,clientRequest.correlationId()
"Aborted call {} is still in callsInFlight.",call
"Closing connection to {} to time out {}",nodeId,call
"Timed out {} call(s) in flight.",numTimedOut
"Internal server error on {}: ignoring call {} in correlationIdToCall " + "that did not exist in callsInFlight",response.destination(),call
"{} got response {}",call,response.responseBody().toString(response.requestHeader().apiVersion())
"{} handleResponse failed with {}",call,prettyPrintException(t)
"All work has been completed, and the I/O thread is now exiting."
"Forcing a hard I/O thread shutdown. Requests in progress will be aborted."
"Hard shutdown in {} ms.",curHardShutdownTimeMsSUBnow
"Thread starting"
"KafkaClient#poll retrieved {} response(s)",responses.size()
"Timed out {} remaining operation(s).",numTimedOut
"Exiting AdminClientRunnable thread."
"Queueing {} with a timeout {} ms from now.",call,call.deadlineMsSUBnow
"The AdminClient thread has exited. Timing out {}.",call
"The AdminClient is not accepting new calls. Timing out {}.",call
"Server response mentioned unknown topic {}",result.name()
"Server response mentioned unknown topic {}",result.name()
"Node {} is no longer the Coordinator. Retrying with new coordinator.",context.getNode().orElse(null)
"Skipping return offset for {} due to error {}.",topicPartition,error
"Metadata is not usable: failed to get metadata.",authException
"Metadata is not ready: bootstrap nodes have not been " + "initialized yet."
"Metadata is not ready: we have not fetched metadata from " + "the bootstrap nodes yet."
"Metadata is ready to use."
"Requesting metadata update."
"Clearing cached controller node {}.",cluster.controller()
"Metadata update failed due to authentication error",exception
"Metadata update failed",exception
"Setting bootstrap cluster metadata {}.",cluster
"Updating cluster metadata to {}",cluster
"Support for using the empty group id by consumers is deprecated and will be removed in the next major release."
"Initializing the Kafka consumer"
"Kafka consumer initialized"
"Subscribed to topic(s): {}",Utils.join(topics,", ")
"Subscribed to pattern: '{}'",pattern
"Unsubscribed all topics or patterns and assigned partitions"
"Subscribed to partition(s): {}",Utils.join(partitions,", ")
"Still waiting for metadata"
"Committing offsets: {}",offsets
"Seeking to offset {} for partition {}",offset,partition
"Seeking to offset {} for partition {} with epoch {}",offset,partition,offsetAndMetadata.leaderEpoch().get()
"Seeking to offset {} for partition {}",offset,partition
"Pausing partitions {}",partitions
"Resuming partitions {}",partitions
"Closing the Kafka consumer"
"Failed to close coordinator",t
"Kafka consumer has been closed"
"Skipping assignment for topic {} since no metadata is available",topic
"Failed to close consumer interceptor ",e
"Joining group with current subscription: {}",subscriptions.subscription()
"Adding newly assigned partitions: {}",Utils.join(assignedPartitions,", ")
"User provided listener {} failed on invocation of onPartitionsAssigned for partitions {}",listener.getClass().getName(),assignedPartitions,e
"Revoke previously assigned partitions {}",Utils.join(revokedPartitions,", ")
"User provided listener {} failed on invocation of onPartitionsRevoked for partitions {}",listener.getClass().getName(),revokedPartitions,e
"Lost previously assigned partitions {}",Utils.join(lostPartitions,", ")
"User provided listener {} failed on invocation of onPartitionsLost for partitions {}",listener.getClass().getName(),lostPartitions,e
"Executing onJoinComplete with generation {} and memberId {}",generation,memberId
"We received an assignment {} that doesn't match our current subscription {}; it is likely " + "that the subscription has changed since we joined the group. Will try re-join the group with current subscription",assignment.partitions(),subscriptions.prettyString()
"Updating with newly assigned partitions: {}, compare with already owned partitions: {}, " + "newly added partitions: {}, revoking partitions: {}",Utils.join(assignedPartitions,", "),Utils.join(ownedPartitions,", "),Utils.join(addedPartitions,", "),Utils.join(revokedPartitions,", ")
"Performing assignment using strategy {} with subscriptions {}",assignor.name(),subscriptions
"The following subscribed topics are not assigned to any members: {} ",notAssignedTopics
"The following not-subscribed topics are assigned, and their metadata will be " + "fetched from the brokers: {}",newlyAddedTopics
"Finished assignment for group: {}",assignments
"With the COOPERATIVE protocol, owned partitions cannot be " + "reassigned to other members; however the assignor has reassigned partitions {} which are still owned " + "by some members; return the error code to all members to let them stop",totalAddedPartitions
"Executing onJoinPrepare with generation {} and memberId {}",generation,memberId
"Giving away all assigned partitions as lost since generation has been reset," + "indicating that consumer is no longer part of the group"
"Setting offset for partition {} to the committed offset {}",tp,position
"Sending asynchronous auto-commit of offsets {}",allConsumedOffsets
"Asynchronous auto-commit of offsets {} failed due to retriable error: {}",offsets,exception
"Asynchronous auto-commit of offsets {} failed: {}",offsets,exception.getMessage()
"Completed asynchronous auto-commit of offsets {}",offsets
"Sending synchronous auto-commit of offsets {}",allConsumedOffsets
"Auto-commit of offsets {} timed out before completion",allConsumedOffsets
"Auto-commit of offsets {} was interrupted before completion",allConsumedOffsets
"Offset commit with offsets {} failed",offsets,exception
"Failing OffsetCommit request since the consumer is not part of an active group"
"Sending OffsetCommit request with {} to coordinator {}",offsets,coordinator
"Committed offset {} for partition {}",offset,tp
"Offset commit failed on partition {} at offset {}: {}",tp,offset,error.message()
"Offset commit failed on partition {} at offset {}: {}",tp,offset,error.message()
"Received fatal exception: group.instance.id gets fenced"
"Not authorized to commit to topics {}",unauthorizedTopics
"Fetching committed offsets for partitions: {}",partitions
"Offset fetch failed: {}",error.message()
"Failed to fetch offset for partition {}: {}",tp,error.message()
"Found no committed offset for partition {}",tp
"Coordinator discovery failed, refreshing metadata"
"No broker available to send FindCoordinator request"
"still waiting to ensure active group"
"Interrupted while waiting for consumer heartbeat thread to close"
"Successfully joined group with generation {}",generation.generationId
"Sending JoinGroup ({}) to coordinator {}",requestBuilder,this.coordinator
"Received successful JoinGroup response: {}",joinResponse
"Attempt to join group rejected since coordinator {} is loading the group.",coordinator()
"Attempt to join group failed due to unknown member id."
"Attempt to join group failed due to obsolete coordinator information: {}",error.message()
"Received fatal exception: group.instance.id gets fenced"
"Attempt to join group failed due to unsupported version error. Please unset field group.instance.id and retry" + "to see if the problem resolves"
"Sending follower SyncGroup to coordinator {}: {}",this.coordinator,requestBuilder
"Sending leader SyncGroup to coordinator {}: {}",this.coordinator,requestBuilder
"SyncGroup failed because the group began another rebalance"
"Received fatal exception: group.instance.id gets fenced"
"SyncGroup failed: {}",error.message()
"SyncGroup failed: {}",error.message()
"Received FindCoordinator response {}",resp
"Discovered group coordinator {}",coordinator
"Group coordinator lookup failed: {}",findCoordinatorResponse.data().errorMessage()
"Group coordinator {} is unavailable or invalid, will attempt rediscovery",this.coordinator
"Resetting generation after encountering " + error + " from " + api + " response"
"Resetting generation due to consumer pro-actively leaving the group"
"Close timed out with {} pending requests to coordinator, terminating client connections",client.pendingRequestCount(coordinator)
"LeaveGroup request returned successfully"
"LeaveGroup request failed with error: {}",error.message()
"Sending Heartbeat request to coordinator {}",coordinator
"Received successful Heartbeat response"
"Attempt to heartbeat failed since coordinator {} is either not started or not valid.",coordinator()
"Attempt to heartbeat failed since group is rebalancing"
"Attempt to heartbeat failed since generation {} is not current",generation.generationId
"Received fatal exception: group.instance.id gets fenced"
"Attempt to heartbeat failed for since member id {} is not valid.",generation.memberId
"Enabling heartbeat thread"
"Disabling heartbeat thread"
"Heartbeat thread started"
"Caught fenced group.instance.id {} error in heartbeat thread",rebalanceConfig.groupInstanceId
"An authentication error occurred in the heartbeat thread",e
"A group authorization error occurred in the heartbeat thread",e
"Unexpected interrupt received in heartbeat thread",e
"Heartbeat thread failed due to unexpected error",e
"Heartbeat thread has closed"
"Assigned partition {} for non-subscribed topic regex pattern; subscription pattern is {}",topicPartition,this.subscribedPattern
"Assigned partition {} for non-subscribed topic; subscription is {}",topicPartition,this.subscription
"Skipping reset of partition {} since it is no longer assigned",tp
"Skipping reset of partition {} since reset is no longer needed",tp
"Skipping reset of partition {} since an alternative reset has been requested",tp
"Resetting offset for partition {} to offset {}.",tp,offset
"Skipping completed validation for partition {} which is not currently assigned.",tp
"Skipping completed validation for partition {} which is no longer expecting validation.",tp
"Skipping completed validation for partition {} since the current position {} " + "no longer matches the position {} when the request was sent",tp,currentPosition,requestPosition
"Truncation detected for partition {} at offset {}, resetting offset to " + "the first offset known to diverge {}",tp,currentPosition,newPosition
"Truncation detected for partition {} at offset {} (the end offset from the " + "broker is {}), but no reset policy is set",tp,currentPosition,epochEndOffset
"Seeking to {} offset of partition {}",offsetResetStrategy,tp
"The PartitionAssignor interface has been deprecated, " + "please implement the ConsumerPartitionAssignor interface instead."
"Could not cast response body",cce
"Received {} {} from broker {}",resp.getClass().getSimpleName(),resp,node
"{} is assigned to more than one consumer.",topicPartition
"{} can be moved from consumer {} to consumer {} for a more balanced assignment.",topicPartition,otherConsumer,consumer
"The consumer {} is assigned more partitions than the maximum possible.",consumer
"Expected more than one potential consumer for partition '{}'",partition
"Expected partition '{}' to be assigned to a consumer",partition
"A cycle of length {} was found: {}",path.size()SUB1,path.toString()
"Stickiness is violated for topic {}" + "\nPartition movements for this topic occurred among the following consumer pairs:" + "\n{}",topicMovements.getKey(),topicMovements.getValue().toString()
"Raising WakeupException in response to user wakeup"
"Cancelled request with header {} due to node {} being disconnected",response.requestHeader(),response.destination()
"Sending {} {} to broker {}",isolationLevel,data.toString(),fetchTarget
"Unable to find FetchSessionHandler for node {}. Ignoring fetch response.",fetchTarget.id()
"Fetch {} at offset {} for partition {} returned fetch data {}",isolationLevel,fetchOffset,partition,partitionData
"Returning fetched records at offset {} for assigned partition {} and update " + "position to {}",position,completedFetch.partition,nextPosition
"Draining fetched records for partition {}",completedFetch.partition
"Discarding error in ListOffsetResponse because another error is pending",e
"Skipping validation of fetch offsets for partitions {} since the broker does not " + "support the required protocol version (introduced in Kafka 2.3)",fetchPostitions.keySet()
"Discarding error in OffsetsForLeaderEpoch because another error is pending",e
"Leader for partition {} is unknown for fetching offset {}",tp,offset
"Leader for partition {} is unavailable for fetching offset {}",tp,offset
"Sending ListOffsetRequest {} to broker {}",builder,node
"Received ListOffsetResponse {} from broker {}",lor,node
"Handling v0 ListOffsetResponse response for {}. Fetched offset {}",topicPartition,offset
"Attempt to fetch offsets for partition {} failed due to {}, retrying.",topicPartition,error
"Attempt to fetch offsets for partition {} failed due to {}, retrying.",topicPartition,error
"Received unknown topic or partition error in ListOffset request for partition {}",topicPartition
"Attempt to fetch offsets for partition {} failed due to: {}, retrying.",topicPartition,error.message()
"Not fetching from {} for partition {} since it is marked offline or is missing from our metadata," + " using the leader instead.",nodeId,partition
"Skipping fetch for partition {} because previous request to {} has not been processed",partition,node
"Added {} fetch request for partition {} at position {} to node {}",isolationLevel,partition,position,node
"Discarding stale fetch response for partition {} since its offset {} does not match " + "the expected offset {}",tp,fetchOffset,position
"Preparing to read {} bytes of data for partition {} with offset {}",partition.records.sizeInBytes(),tp,position
"Updating high watermark for partition {} to {}",tp,partition.highWatermark
"Updating log start offset for partition {} to {}",tp,partition.logStartOffset
"Updating last stable offset for partition {} to {}",tp,partition.lastStableOffset
"Updating preferred read replica for partition {} to {}, set to expire at {}",tp,partition.preferredReadReplica.get(),expireTimeMs
"Error in fetch for partition {}: {}",tp,error.exceptionName()
"Received unknown topic or partition error in fetch for partition {}",tp
"Discarding stale fetch response for partition {} since the fetched offset {} " + "does not match the current offset {}",tp,fetchOffset,subscriptions.position(tp)
"Fetch offset {} is out of range for partition {}, resetting offset",fetchOffset,tp
"Unset the preferred read replica {} for partition {} since we got {} when fetching {}",clearedReplicaId.get(),tp,error,fetchOffset
"Received unknown leader epoch error in fetch for partition {}",tp
"Unknown error fetching data for topic-partition {}",tp
"Skipping aborted record batch from partition {} with producerId {} and " + "offsets {} to {}",partition,producerId,currentBatch.baseOffset(),currentBatch.lastOffset()
"{}: Activating ConsumeBenchWorker with {}",id,spec
"Will consume from topics {} via dynamic group assignment.",topics
"Will consume from topic partitions {} via manual assignment.",partitions
"{} Consumed total number of messages={}, bytes={} in {} ms.  status: {}",clientId,messagesConsumed,bytesConsumed,curTimeMsSUBstartTimeMs,statusData
"{} was interrupted. Closing...",this.getClass().getName()
"Status={}",JsonUtil.toJsonString(statusData)
"{}: Deactivating ConsumeBenchWorker.",id
"{}: Activating ExternalCommandWorker with {}",id,spec
"{}: Unable to start process",id,t
"{}: starting stdout monitor.",id
"{}: can't read any more from stdout: {}",id,e.getMessage()
"{}: read line from stdin: {}",id,line
"{}: New status: {}",id,resp.get("status").toString()
"{}: (stdout): {}",id,resp.get("log").asText()
"{}: error: {}",id,error
"{}: error reading from stdout.",id,e
"{}: starting stderr monitor.",id
"{}: can't read any more from stderr: {}",id,e.getMessage()
"{}: (stderr):{}",id,line
"{}: error reading from stderr.",id,e
"{}: stdin writer ready.",id
"{}: StdinWriter terminating.",id
"{}: writing to stdin: {}",id,inputString
"{}: can't write any more to stdin: {}",id,e.getMessage()
"{}: error writing to stdin.",id,e
"{}: error closing stdinWriter: {}",id,e.getMessage()
"{}: process exited with return code {}",id,exitStatus
"{}: ExitMonitor error",id,e
"{}: destroying process",id
"{}: forcibly destroying process",id
"{}: closing Terminator thread.",id
"{}: Terminator error",id,e
"{}: Deactivating ExternalCommandWorker.",id
"{}: Activating ConnectionStressWorker with {}",id,spec
"{}: Deactivating ConnectionStressWorker.",id
"{}: Activating RoundTripWorker.",id
"{}: Starting RoundTripWorker#ProducerRunnable.",id
"{}: Got exception when sending message {}: {}",id,messageIndex,exception.getMessage()
"{}: ProducerRunnable is exiting.  messagesSent={}; uniqueMessagesSent={}; " + "ackedSends={}/{}.",id,messagesSent,uniqueMessagesSent,spec.maxMessages()SUBunackedSends,spec.maxMessages()
"{}: consumer waiting for {} message(s), starting with: {}",id,numToReceive,Utils.join(list,", ")
"{}: Starting RoundTripWorker#ConsumerRunnable.",id
"{}: Consumer received the full count of {} unique messages.  " + "Waiting for all {} sends to be acked...",id,spec.maxMessages(),unackedSends
"{}: all sends have been acked.",id
"{}: Consumer got WakeupException",id,e
"{}: Consumer got TimeoutException",id,e
"{}: ConsumerRunnable is exiting.  Invoked poll {} time(s).  " + "messagesReceived = {}; uniqueMessagesReceived = {}.",id,pollInvoked,messagesReceived,uniqueMessagesReceived
"{}: Deactivating RoundTripWorker.",id
"{}: Deactivated RoundTripWorker.",id
"{}: Activating ProduceBenchWorker with {}",id,spec
"SendRecordsCallback: error",exception
"Exception on final future",e
"Sent {} total record(s) in {} ms.  status: {}",histogram.summarize().numSamples(),curTimeMsSUBstartTimeMs,statusData
"Beginning transaction."
"Committing transaction."
"Aborting transaction."
"{}: Deactivating ProduceBenchWorker.",id
"RUN: {}. RESULT: [{}]",Utils.join(command," "),result
"RUN: {}. ERROR: [{}]",Utils.join(command," "),e.getMessage()
"{} caught an exception",what,exception
"Failed to create or verify topics {}",topics,e
"Request to create topics has an empty topic list."
"Topic(s) {} already exist.",topicsExists
"Attempting to create {} topics (try {})...",topicsToCreate.size(),PLUSPLUStries
"Successfully created {}.",topicName
"Attempt to create topic `{}` failed: {}",topicName,e.getCause().getMessage()
"Topic {} already exists.",topicName
"Failed to create {}",topicName,e.getCause()
str
str
"{}: Activating NoOpTask.",id
"{}: Deactivating NoOpTask.",id
"Starting REST server"
"Registered resource {}",resource
"REST server listening at " + jettyServer.getURI()
"Stopping REST server"
"REST server stopped"
"Unable to stop REST server",e
"Sending {} with input {} to {}",method,serializedBody,url
"{} {}: error: {}",method,url,e.getMessage()
"Uncaught exception in REST call: ",e
"Uncaught exception in REST call: {}",e.getMessage()
"Activating DegradedNetworkFaultWorker {}.",id
"Deactivating DegradedNetworkFaultWorker {}.",id
"Activating ProcessStopFault {}.",id
"Deactivating ProcessStopFault {}.",id
"Failed to parse process ID from line {}",e
"{}: no processes containing {} found to send {} to.",id,javaProcessName,signalName
"{}: sending {} to {} pid(s) {}",id,signalName,javaProcessName,Utils.join(pids,", ")
"Activating NetworkPartitionFault {}.",id
"Deactivating NetworkPartitionFault {}.",id
"Activating {} {}: {}.",spec.getClass().getSimpleName(),id,spec
"Deactivating {} {}: {}.",spec.getClass().getSimpleName(),id,spec
"Starting coordinator process."
"Running coordinator shutdown hook."
"Got exception while running coordinator shutdown hook.",e
"Created TaskManager for agent(s) on: {}",Utils.join(nodeManagers.keySet(),", ")
"createTask(id={}, spec={}) error",id,spec,e
"Task {} already exists with spec {}",id,originalSpec
"Failed to create a new task {} with spec {}: {}",id,spec,failure
"Created a new task {} with spec {}, scheduled to start {} ms from now.",id,spec,delayMs
"Can't start task {}, because it is already in state {}.",task.id,task.state
"Unable to find nodes for task {}",task.id,e
"Running task {} on node(s): {}",task.id,Utils.join(nodeNames,", ")
"stopTask(id={}) error",id,e
"Can't cancel non-existent task {}.",id
"Stopped pending task {}.",id
"Task {} is now complete with no errors.",id
"Task {} is now complete with error: {}",id,task.error
"Cancelling task {} with worker(s) {}",id,Utils.mkString(activeWorkerIds,"",""," = ",", ")
"Can't cancel task {} because it is already stopping.",id
"Can't cancel task {} because it is already done.",id
"destroyTask(id={}) error",id,e
"Can't destroy task {}: no such task found.",id
"Destroying task {}.",id
"Task {}: Updating worker state for {} on {} from {} to {}.",task.id,workerId,nodeName,prevState,nextState
"Error updating worker state for {} on {}.  Stopping worker.",workerId,nodeName,e
"{}: Worker {} finished with status '{}'",nodeName,task.id,JsonUtil.toJsonString(state.status())
"{}: Worker {} finished with error '{}' and status '{}'",nodeName,task.id,state.error(),JsonUtil.toJsonString(state.status())
"{}: Task {} is now complete on {} with error: {}",nodeName,task.id,Utils.join(task.workerIds.keySet(),", "),task.error.isEmpty()QUES"(none)":task.error
"{}: task {} stopped with error {}.  Stopping worker(s): {}",nodeName,task.id,task.error,Utils.mkString(activeWorkerIds,"{","}",": ",", ")
"Shutting down TaskManager{}.",stopAgentsQUES" and agents":""
"{}: error creating worker {}.",node.name(),this,e
"{}: error stopping worker {}.",node.name(),this,e
"{}: failed to get agent status: ConnectException {}",node.name(),e.getMessage()
"{}: failed to get agent status",node.name(),e
"{}: got heartbeat status {}",node.name(),agentStatus
"{}: Unhandled exception in NodeHeartbeatRunnable",node.name(),e
"{}: worker state is still {}",node.name(),worker.state
"{}: worker state changed from {} to {}",node.name(),worker.state,state
"{}: there is already a worker {} with ID {}.",node.name(),worker,workerId
"{}: scheduling worker {} to start.",node.name(),worker
"{}: unable to locate worker to stop with ID {}.",node.name(),workerId
"{}: Worker {} is already scheduled to stop.",node.name(),worker
"{}: scheduling worker {} to stop.",node.name(),worker
"{}: unable to locate worker to destroy with ID {}.",node.name(),workerId
"{}: Failed to send shutdown request",node.name(),e
"{}: Ignoring request to create worker {}, because there is already " + "a worker with that id.",nodeName,workerId
"{}: Will not run worker {} as it has expired.",nodeName,worker
"{}: Worker {} is halting.",nodeName,worker
"{}: Worker {} is halting with error {}",nodeName,worker,errorString
"{}: Worker {} start() exception",nodeName,worker,e
"{}: request conflict while creating worker {} for task {} with spec {}.",nodeName,workerId,taskId,spec
"{}: Error creating worker {} for task {} with spec {}",nodeName,workerId,taskId,spec,e
"{}: Created worker {} with spec {}",nodeName,worker,spec
"{}: unable to create worker {} for task {}, with spec {}",nodeName,workerId,taskId,spec,e
"{}: Worker {} was cancelled while it was starting up.  " + "Transitioning to STOPPING.",nodeName,worker
"{}: Worker {} is now RUNNING.  Scheduled to stop in {} ms.",nodeName,worker,worker.spec.durationMs()
"{}: Worker {} {} during startup.  Transitioning to DONE.",nodeName,worker,verb
"{}: Worker {} {} during startup.  Transitioning to CANCELLING.",nodeName,worker,verb
"{}: Cancelling worker {} {}.  ",nodeName,worker,verb
"{}: Running worker {} {}.  Transitioning to STOPPING.",nodeName,worker,verb
"{}: Stopping worker {} {}.",nodeName,worker,verb
"{}: Can't halt worker {} because it is already DONE.",nodeName,worker
"{}: destroying worker {} with error {}",nodeName,worker,worker.error
"{}: completed worker {} with error {}",nodeName,worker,worker.error
"{}: Can't stop worker {} because there is no worker with that ID.",nodeName,workerId
"{}: Cancelling worker {} during its startup process.",nodeName,worker
"{}: Can't stop worker {}, because it is already being " + "cancelled.",nodeName,worker
"{}: Stopping running worker {}.",nodeName,worker
"{}: Can't stop worker {}, because it is already " + "stopping.",nodeName,worker
"{}: destroying worker {} with error {}",nodeName,worker,worker.error
"{}: Can't stop worker {}, because it is already done.",nodeName,worker
"{}: worker.stop() exception",nodeName,exception
"{}: Shutting down WorkerManager.",nodeName
"{}: Waiting for shutdownManager quiescence...",nodeName
"{}: Waiting for workerCleanupExecutor to terminate...",nodeName
"{}: Waiting for stateChangeExecutor to terminate...",nodeName
"{}: Shutting down shutdownExecutor.",nodeName
"{}: Caught exception while shutting down WorkerManager",nodeName,e
"{}: Destroying all workers.",nodeName
"Failed to stop worker {}",workerId,e
"Starting agent process."
"Running agent shutdown hook."
"Got exception while running agent shutdown hook.",e
msg
"Saw only {} cluster nodes.  Waiting to see {}.",nodes.size(),testConfig.numClusterNodes
"Did not see newtopic.  Retrying listTopics..."
"offsetsForTime = {}",offsetsForTime.result)
"Found first message..."
"Got RecordTooLargeException",e
"Closing consumer."
"Closed consumer."
"Successfully used feature {}",featureName
"Got UnsupportedVersionException when attempting to use feature {}",featureName
"Configured PushHttpMetricsReporter for {} to report every {} seconds",url,period
"Adding metric {}",metric.metricName()
"Updating metric {}",metric.metricName()
"Removing metric {}",metric.metricName()
"Reporting {} metrics to {}",samples.size(),url
"Error reporting metrics, {}: {}",responseCode,msg
"PushHttpMetricsReporter does not currently support redirects, saw {}",responseCode
"Finished reporting metrics with response code {}",responseCode
"Error reporting metrics",t
"Couldn't use reflection for resolving characterEntityReferences in HtmlUtils class",e
"Couldn't get direct performance optimized instantiator for GrailsPrintWriterAdapter. Using default instantiation.",e
"Couldn't get direct performance optimized instantiator for FastStringPrintWriter. Using default instantiation.",e
"I/O exception in GrailsPrintWriter: " + e.getMessage(),e
"A plugin or your application called the flatten() method which can degrade startup performance"
"[RuntimeConfiguration] Unable to load beans from resources.groovy",ex
"Bean couldn't be autowired using grails optimization: " + e.getMessage()
"Retrying using spring autowire"
"Added autowiring by name from bean name '" + beanName + "' via property '" + propertyName + "' to bean named '" + propertyName + "'"
"Loading properties [" + resource.getFilename() + "]"
"Loading properties [" + resource.getFilename() + "] with encoding '" + encoding + "'"
"Loading properties [" + resource.getFilename() + "]"
"Clearing entire resource bundle cache"
resource + " could not be resolved in the file system - assuming that is hasn't changed",ex
"Could not parse properties file [" + resource.getFilename() + "]",ex
"No properties file found for [" + filename + "] - neither plain properties nor XML"
"Scanning only this classloader:" + originalResourceLoader.getClassLoader()
"Scanning URL " + url.toExternalForm() + " while searching for '" + location + "'"
"Attempting to load [" + resources.length + "] core plugins"
"[GrailsPluginManager] Core plugin [" + pluginClassName + "] not found, resuming load without.."
e.getMessage(),e
"Error loading views for binary plugin [" + this + "]: " + e.getMessage(),e
"Configuring plugin " + this + " to watch resources with pattern: " + referencedResources
"Cannot load plug-in resource watch list from [" + GrailsArrayUtils.toString(watchedResourcePatternReferences) + "]. This means that the plugin " + this + ", will not be able to auto-reload changes effectively. Try running grails upgrade.: " + e.getMessage()
"Plugin " + this + " is participating in Spring configuration..."
"Error refreshing plugin: " + e.getMessage(),e
"Cannot instantiate an Artefact Handler:" + e.getMessage(),e
"The constructor of the Artefact Handler is not accessible:" + e.getMessage(),e
"This class is not an ArtefactHandler:" + artefactClass.getName()
"This object is not an ArtefactHandler:" + artefact + "[" + artefact.getClass().getName() + "]"
"Error loading " + filename + " for plugin: " + pluginClass.getName() + ": " + e.getMessage(),e
"Plugin " + plugin.getName() + " has an artefact " + shortName + " without a package name " + "This could lead to artefacts being excluded from the application"
"Plugin " + plugin.getName() + " has an artefact " + shortName + " that is being excluded from " + "the application because another artefact exists with the same name without a package defined."
"Plugin " + grailsPlugin + " could not reload changes to file [" + file + "]: " + e.getMessage(),e
"Applying rules to determine whether transaction should rollback on $ex"
"Winning rollback rule is: $winner"
"No relevant rollback rule found: applying default rules"
"The GrailsDomainClass API should no longer be used to retrieve data about domain classes. Use the mapping context API instead"
"Error occurred running shutdown operation: " + e.getMessage(),e
"Loading resource for path {} from base resource {}",location,baseResource
"Found resource for path {} from base resource {}",location,baseResource
"Error loading resource for path: " + location,e
stopWatch.prettyPrint()
"Error running deferred data binding: " + e.getMessage(),e
"Locking cache for update"
"Locking cache for update"
"Updating cache for value [{}]",value
"Unlocking cache for update"
"Obtained adapted MetaClass [" + emc + "] from AdapterMetaClass instance [" + adapter + "]"
"No adapter MetaClass found, using original [" + emc + "]"
"Adding MetaClass for class [" + toClass + "] MetaClass [" + replacement + "]"
"Adding MetaClass for class [" + toClass + "] MetaClass [" + replacement + "] with adapter [" + adapter + "]"
"Exception thrown constructing new MetaClass adapter when reloading: " + e.getMessage(),e
String.format("Got exception while checking property descriptors for class %s",clazz.getName()),e
String.format("Got exception while checking PropertyDescriptor.propertyType for field %s.%s",clazz.getName(),descriptor.getName()),e
String.format("Got exception while checking property descriptors for class %s",clazz.getName()),e
String.format("Got exception while checking PropertyDescriptor.propertyType for field %s.%s",clazz.getName(),descriptor.getName()),e
"[DEPRECATED] " + message
"[WARNING] " + message
"Unable to load plugins for resource path " + resourcePath,ioe
"Unable to load plugins for resource path " + resourcePath,ioe
"Plugin [" + plugin.getName() + ":" + plugin.getVersion() + "] may not be compatible with this application as the application Grails version is less" + " than the plugin requires. Plugin is compatible with Grails version " + pluginGrailsVersion + " but app is " + appGrailsVersion
"Plugin [" + plugin.getName() + ":" + plugin.getVersion() + "] may not be compatible with this application as the application Grails version is less" + " than the plugin requires. Plugin is compatible with Grails version " + pluginGrailsVersion + " but app is " + appGrailsVersion
"Plugin [" + plugin.getName() + ":" + plugin.getVersion() + "] may not be compatible with this application as the application Grails version is greater" + " than the plugins max specified. Plugin is compatible with Grails versions " + pluginGrailsVersion + " but app is " + appGrailsVersion
"grailsVersion not formatted as expected, unable to determine compatibility."
e.getMessage()
"Attempting to load [" + pluginResources.length + "] user defined plugins"
"Class [" + pluginClass + "] not loaded as plug-in. Grails plug-ins must end with the convention 'GrailsPlugin'!"
"Class [" + pluginClass + "] not loaded as plug-in. Grails plug-ins must end with the convention 'GrailsPlugin'!"
"ERROR: Plugin [" + plugin.getName() + "] cannot be loaded because its dependencies [" + DefaultGroovyMethods.inspect(plugin.getDependencyNames()) + "] cannot be resolved"
"Parsing & compiling " + r.getFilename()
"Grails plugin " + plugin + " is disabled and was not loaded"
"Grails plug-in [" + plugin.getName() + "] with version [" + plugin.getVersion() + "] loaded successfully"
"Grails plug-in " + pluginToEvict + " was evicted by " + evictor
"Inspecting [" + theClass.getName() + "]"
"Adding artefact " + theClass + " of kind " + artefactHandler.getType()
"loaded classes: [" + loadedClasses + "]"
"Could not find urlMapping which matches: " + request.getRequestURI()
"Unable to render errors view: " + e.getMessage(),e
"Matched URI [" + uri + "] to URL mapping [" + info + "], forwarding to [" + forwardUrl + "] with response [" + response.getClass() + "]"
getRequestLogMessage(e,request),e
"Problem instantiating StackTracePrinter class, using default: " + t.getMessage()
"Bound Grails request context to thread: " + request
"Cleared Grails thread-bound request context: " + request
"paramListenerBeans isn't initialized."
"[UrlMappingFactoryBean] Creating URL mappings as..."
"[UrlMappingFactoryBean] " + key + "=" + mappings.get(key)
"Created ApplicationContext in " + (System.currentTimeMillis()SUBnow) + "ms"
"[RuntimeConfiguration] Registering bean [" + beanName + "]"
"[RuntimeConfiguration] With property [" + pv.getName() + "] set to [" + pv.getValue() + "]"
"[RuntimeConfiguration] Registering bean [" + key + "]"
"[RuntimeConfiguration] With property [" + pv.getName() + "] set to [" + pv.getValue() + "]"
"Error loading beans for resource pattern: " + resourcePattern,e
"Attempting to display code snippet found in url " + url
"[GrailsWrappedRuntimeException] I/O error reading line diagnostics: " + e.getMessage(),e
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("append(%s)",s)
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
"comma()"
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("array()")
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("end(%s, %s)",m,c)
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("endArray()")
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("endObject()")
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("key(%s)",s)
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("object()")
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("pop(%s)",c)
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("push(%s)",c)
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("Next index: " + x)
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("value(boolean %b)",b)
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("value(double %s)",d)
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("value(long %s)",l)
delegate.mode.name() + " > " + String.format(">> " + getCurrentStrackReference())
delegate.mode.name() + " > " + String.format("value(Object %s)",o)
"Bean named '" + name + "' is missing."
"No bean named [" + ResourceAwareTemplateEngine.BEAN_ID + "] defined in Spring application context!"
"Error processing form encoded " + request.getMethod() + " request",e
e.getMessage()
"Exception while closing watchService",e
"Error Initializing Native OS X File Event Watcher. Add JNA to classpath for Faster File Watching performance."
"Exception while trying to load WatchServiceDirectoryWatcher (this is probably Java 6 and WatchService isn't available). Falling back to PollingDirectoryWatcher.",e
e.toString()
"Exception while closing watchService",e
"Reverse mapping: " + key + " -> " + mapping
"Reverse mapping: " + key + " -> " + mapping
"Attempting to match URI [" + uri + "] with pattern [" + mapping.getUrlData().getUrlPattern() + "]"
"Attempting to match URI [" + uri + "] with pattern [" + mapping.getUrlData().getUrlPattern() + "]"
"Matched URI [" + uri + "] with pattern [" + mapping.getUrlData().getUrlPattern() + "], adding to posibilities"
"Attempting to match URI [" + uri + "] with pattern [" + mapping.getUrlData().getUrlPattern() + "]"
"Matched URI [" + uri + "] with pattern [" + mapping.getUrlData().getUrlPattern() + "], adding to posibilities"
"Mapping: '" + mappedURI + "' does not start with " + SLASH + " or is response code."
"Both [action] and [view] specified in URL mapping [" + mapping + "]. The action takes precendence!"
"URL mapping argument [exception] with value [" + exceptionArg + "] must be a subclass of java.lang.Throwable"
"URL mapping argument [exception] with value [" + exceptionArg + "] must be a valid class"
"Created reverse URL mapping [" + uri.toString() + "] for parameters [" + paramValues + "]"
"Mapping [{}] has a higher precedence than [{}] because it is the root",this.toString(),other.toString()
"Mapping [{}] has a lower precedence than [{}] because the latter is the root",this.toString(),other.toString()
"Mapping [{}] has a higher precedence than [{}] because it has more path tokens",this.toString(),other.toString()
"Mapping [{}] has a lower precedence than [{}] because it has fewer path tokens",this.toString(),other.toString()
"Mapping [{}] has a higher precedence than [{}] because it has more path tokens [{} vs {}]",this.toString(),other.toString(),thisStaticAndWildcardTokenCount,otherStaticAndWildcardTokenCount
"Mapping [{}] has a higher precedence than [{}] because the latter has more path tokens [{} vs {}]",this.toString(),other.toString(),thisStaticAndWildcardTokenCount,otherStaticAndWildcardTokenCount
"Mapping [{}] has a lower precedence than [{}] because the latter has more concrete path tokens [{} vs {}]",this.toString(),other.toString(),thisStaticTokenCount,otherStaticTokenCount
"Mapping [{}] has a higher precedence than [{}] because it has more concrete path tokens [{} vs {}]",this.toString(),other.toString(),thisStaticTokenCount,otherStaticTokenCount
"Mapping [{}] has a lower precedence than [{}] because the latter contains more concrete tokens",this.toString(),other.toString()
"Mapping [{}] has a higher precedence than [{}] because it contains more concrete tokens",this.toString(),other.toString()
"Mapping [{}] has a higher precedence than [{}] due containing more double wild cards [{} vs. {}]",this.toString(),other.toString(),thisDoubleWildcardCount,otherDoubleWildcardCount
"Mapping [{}] has a lower precedence than [{}] due to the latter containing more double wild cards [{} vs. {}]",this.toString(),other.toString(),thisDoubleWildcardCount,otherDoubleWildcardCount
"Mapping [{}] has a higher precedence than [{}] because it contains more single wild card matches [{} vs. {}]",this.toString(),other.toString(),thisSingleWildcardCount,otherSingleWildcardCount
"Mapping [{}] has a lower precedence than [{}] due to the latter containing more single wild card matches[{} vs. {}]",this.toString(),other.toString(),thisSingleWildcardCount,otherSingleWildcardCount
"Mapping [{}] has a higher precedence than [{}] since it defines more constraints [{} vs. {}]",this.toString(),other.toString(),thisConstraintCount,thatConstraintCount
"Mapping [{}] has a lower precedence than [{}] since the latter defines more constraints [{} vs. {}]",this.toString(),other.toString(),thisConstraintCount,thatConstraintCount
"Mapping [{}] has a higher precedence than [{}] due to the overall diff",this.toString(),other.toString()
"Mapping [{}] has a lower precedence than [{}] due to the overall diff",this.toString(),other.toString()
"Mapping [{}] has a lower precedence than [{}] due to version precedence [{} vs {}]",this.toString(),other.toString(),thisVersion,thatVersion
"Mapping [{}] has a higher precedence than [{}] due to version precedence [{} vs {}]",this.toString(),other.toString(),thisVersion,thatVersion
"Mapping [{}] has a higher precedence than [{}] due to version precedence [{} vs. {}]",this.toString(),other.toString(),thisVersion,thatVersion
"Mapping [{}] has a lower precedence than [{}] due to version precedence [{} vs. {}]",this.toString(),other.toString(),thisVersion,thatVersion
"Mapping [{}] has lower precedence than [{}] because the latter has priority over plugins",this.toString(),other.toString()
"Mapping [{}] has higher precedence than [{}] because it has priority over plugins",this.toString(),other.toString()
"Mapping [{}] has higher precedence than [{}] because it was loaded after",this.toString(),other.toString()
"Mapping [{}] has lower precedence than [{}] because it was loaded before",this.toString(),other.toString()
"Mapping [{}] has equal precedence with mapping [{}]",this.toString(),other.toString()
"Mapping [{}] has equal precedence with mapping [{}]",this.toString(),other.toString()
"Fetching resumed JDBC Connection from DataSource"
"Setting JDBC Connection [" + con + "] read-only"
"Changing isolation level of JDBC Connection [" + con + "] to " + definition.getIsolationLevel()
"Resetting isolation level of JDBC Connection [" + con + "] to " + previousIsolationLevel
"Resetting read-only flag of JDBC Connection [" + con + "]"
"Could not reset JDBC Connection after transaction",ex
"Could not close JDBC Connection",ex
"Unexpected exception on closing JDBC Connection",ex
"Acquired Connection [" + newCon + "] for JDBC transaction"
"Switching JDBC Connection [" + con + "] to manual commit"
"Committing JDBC transaction on Connection [" + con + "]"
"Rolling back JDBC transaction on Connection [" + con + "]"
"Setting JDBC transaction [" + txObject.getConnectionHolder().getConnection() + "] rollback-only"
"Could not reset JDBC Connection after transaction",ex
"Releasing JDBC Connection [" + con + "] after transaction"
"Could not explicitly release JDBC savepoint",ex
"Obtaining JDBC Connection from WebSphere DataSource [" + getTargetDataSource() + "], using ConnectionSpec [" + connSpec + "]"
"Established shared JDBC Connection: " + this.target
"Could not close shared JDBC Connection",ex
"Creating new JDBC Driver Connection to [" + url + "]"
"Loaded JDBC driver: " + driverClassNameToUse
"Creating new JDBC DriverManager Connection to [" + url + "]"
"Could not retrieve default auto-commit and transaction isolation settings",ex
"Connecting to database for operation '" + operation.getName() + "'"
"Using existing database connection for operation '" + operation.getName() + "'"
String.format("Starting embedded database: url='%s', username='%s'",simpleDriverDataSource.getUrl(),simpleDriverDataSource.getUsername())
String.format("Starting embedded database '%s'",this.databaseName)
String.format("Shutting down embedded database: url='%s'",((SimpleDriverDataSource)this.dataSource).getUrl())
String.format("Shutting down embedded database '%s'",this.databaseName)
"Could not shut down embedded database",ex
"Could not close JDBC Connection on shutdown",ex
"Executing SQL script from " + resource
rowsAffected + " returned as update count for SQL: " + statement
"SQLWarning ignored: SQL state '" + warningToLog.getSQLState() + "', error code '" + warningToLog.getErrorCode() + "', message [" + warningToLog.getMessage() + "]"
ScriptStatementFailedException.buildErrorMessage(statement,stmtNumber,resource),ex
"Could not close JDBC Statement",ex
"Executed SQL script from " + resource + " in " + elapsedTime + " ms."
"Triggering auto-flush because queue reached batch size of " + this.batchSize
"Compiled stored procedure. Call string is [" + this.callString + "]"
"RdbmsOperation with SQL [" + getSql() + "] compiled"
"SQL operation not compiled before execution - invoking compile"
"Could not close JDBC Connection",ex
"Could not close JDBC Statement",ex
"Could not close JDBC ResultSet",ex
"JDBC driver does not implement JDBC 4.1 'getObject(int, Class)' method",err
"JDBC driver does not support JDBC 4.1 'getObject(int, Class)' method",ex
"JDBC driver has limited support for JDBC 4.1 'getObject(int, Class)' method",ex
"JDBC driver supports batch updates"
"JDBC driver does not support batch updates"
"JDBC driver 'supportsBatchUpdates' method threw exception",ex
"Extracted SQL state class '" + classCode + "' from value '" + sqlState + "'"
"Validation query [" + this.validationQuery + "] threw exception",ex
"Database has not started up yet - retrying in " + this.interval + " seconds (timeout in " + rest + " seconds)"
"Database startup detected after " + duration + " seconds"
"Replacing custom translator [" + replaced + "] for database '" + dbName + "' with [" + translator + "]"
"Adding custom translator of type [" + translator.getClass().getName() + "] for database '" + dbName + "'"
"Using nested SQLException from the BatchUpdateException"
"Unable to translate SQLException with " + codes + ", will now try the fallback translator"
"Unable to find appropriate constructor of custom exception class [" + exceptionClass.getName() + "]"
"Unable to instantiate custom exception class [" + exceptionClass.getName() + "]",ex
intro + " SQLException with SQL state '" + sqlEx.getSQLState() + "', error code '" + sqlEx.getErrorCode() + "', message [" + sqlEx.getMessage() + "]" + (sqlBANGEQnullQUES"; SQL was [" + sql + "]":"") + " for task [" + task + "]"
"Default sql-error-codes.xml not found (should be included in spring-jdbc jar)"
"Found custom sql-error-codes.xml file at the root of the classpath"
"SQLErrorCodes loaded: " + errorCodes.keySet()
"Error loading SQL error codes from config file",ex
"SQL error codes for '" + databaseName + "' found"
"SQL error codes for '" + databaseName + "' not found"
"Looking up default SQLErrorCodes for DataSource [" + identify(dataSource) + "]"
"Error while extracting database name - falling back to empty error codes",ex
"SQLErrorCodes found in cache for DataSource [" + identify(dataSource) + "]"
"Caching SQL error codes for DataSource [" + identify(dataSource) + "]: database product name is '" + databaseName + "'"
"Overriding already defined custom translator '" + errorCodes.getCustomSqlExceptionTranslator().getClass().getSimpleName() + " with '" + customTranslator.getClass().getSimpleName() + "' found in the CustomSQLExceptionTranslatorRegistry for database '" + databaseName + "'"
"Using custom translator '" + customTranslator.getClass().getSimpleName() + "' found in the CustomSQLExceptionTranslatorRegistry for database '" + databaseName + "'"
contentBANGEQnullQUES"Copied bytes into temporary BLOB with length " + content.length:"Set BLOB to null"
binaryStreamBANGEQnullQUES"Copied binary stream into temporary BLOB with length " + contentLength:"Set BLOB to null"
contentBANGEQnullQUES"Copied string into temporary CLOB with length " + content.length():"Set CLOB to null"
asciiStreamBANGEQnullQUES"Copied ASCII stream into temporary CLOB with length " + contentLength:"Set CLOB to null"
characterStreamBANGEQnullQUES"Copied character stream into temporary CLOB with length " + contentLength:"Set CLOB to null"
"Could not free BLOB",ex
"Could not free CLOB",ex
"Returning BLOB as bytes"
"Returning BLOB as binary stream"
"Returning CLOB as string"
"Returning CLOB as ASCII stream"
"Returning CLOB as character stream"
contentBANGEQnullQUES"Set bytes for BLOB with length " + content.length:"Set BLOB to null"
binaryStreamBANGEQnullQUES"Set binary stream for BLOB with length " + contentLength:"Set BLOB to null"
contentBANGEQnullQUES"Set string for CLOB with length " + content.length():"Set CLOB to null"
asciiStreamBANGEQnullQUES"Set ASCII stream for CLOB with length " + contentLength:"Set CLOB to null"
characterStreamBANGEQnullQUES"Set character stream for CLOB with length " + contentLength:"Set CLOB to null"
"Overriding type info with runtime info from SqlParameterValue: column index " + paramIndex + ", SQL type " + parameterValue.getSqlType() + ", type name " + parameterValue.getTypeName()
"Setting SQL statement parameter value: column index " + paramIndex + ", parameter value [" + inValueToUse + "], value class [" + (inValueToUseBANGEQnullQUESinValueToUse.getClass().getName():"null") + "], SQL type " + (sqlTypeToUseEQEQSqlTypeValue.TYPE_UNKNOWNQUES"unknown":Integer.toString(sqlTypeToUse))
"JDBC getParameterType call failed - using fallback method instead: " + ex
"Mapping column '" + column + "' to property '" + pd.getName() + "' of type '" + ClassUtils.getQualifiedName(pd.getPropertyType()) + "'"
"Intercepted TypeMismatchException for row " + rowNumber + " and column '" + column + "' with null value when setting property '" + pd.getName() + "' of type '" + ClassUtils.getQualifiedName(pd.getPropertyType()) + "' on object: " + mappedObject,ex
"No property found for column '" + column + "' mapped to field '" + field + "'"
"Executing SQL statement [" + sql + "]"
"Executing SQL query [" + sql + "]"
"Executing SQL update [" + sql + "]"
"SQL update affected " + rows + " rows"
"Executing SQL batch update of " + sql.length + " statements"
"Executing prepared SQL statement" + (sqlBANGEQnullQUES" [" + sql + "]":"")
"Executing prepared SQL query"
"Executing prepared SQL update"
"SQL update affected " + rows + " rows"
"Executing SQL update and returning generated keys"
"SQL update affected " + rows + " rows and returned " + generatedKeys.size() + " keys"
"Executing SQL batch update [" + sql + "]"
"Executing SQL batch update [" + sql + "] with a batch size of " + batchSize
"Sending SQL batch update #" + batchIdx + " with " + items + " items"
"Calling stored procedure" + (sqlBANGEQnullQUES" [" + sql + "]":"")
"CallableStatement.execute() returned '" + retVal + "'"
"CallableStatement.getUpdateCount() returned " + updateCount
"Added default SqlReturnResultSet parameter named '" + rsName + "'"
"Added default SqlReturnUpdateCount parameter named '" + undeclaredName + "'"
"CallableStatement.getUpdateCount() returned " + updateCount
"Added default SqlReturnResultSet parameter named '" + rsName + "'"
"SQLWarning ignored: SQL state '" + warningToLog.getSQLState() + "', error code '" + warningToLog.getErrorCode() + "', message [" + warningToLog.getMessage() + "]"
"Overriding supportsGetGeneratedKeys from DatabaseMetaData to 'true'; it was reported as " + "'false' by " + databaseMetaData.getDriverName() + " " + databaseMetaData.getDriverVersion()
"Using " + provider.getClass().getSimpleName()
"Error retrieving 'DatabaseMetaData.supportsCatalogsInProcedureCalls' - " + ex.getMessage()
"Error retrieving 'DatabaseMetaData.supportsSchemasInProcedureCalls' - " + ex.getMessage()
"Error retrieving 'DatabaseMetaData.storesUpperCaseIdentifiers' - " + ex.getMessage()
"Error retrieving 'DatabaseMetaData.storesLowerCaseIdentifiers' - " + ex.getMessage()
"Cannot check default schema - no Connection from DatabaseMetaData"
"Exception encountered during default schema lookup",ex
"Defaulting to no synonyms in table meta-data lookup"
"Unable to include synonyms in table meta-data lookup - no Connection from DatabaseMetaData"
"Unable to include synonyms in table meta-data lookup - no Oracle Connection: " + ex
"Including synonyms in table meta-data lookup"
"Error retrieving 'DatabaseMetaData.supportsCatalogsInProcedureCalls': " + ex.getMessage()
"Error retrieving 'DatabaseMetaData.supportsSchemasInProcedureCalls': " + ex.getMessage()
"Error retrieving 'DatabaseMetaData.storesUpperCaseIdentifiers': " + ex.getMessage()
"Error retrieving 'DatabaseMetaData.storesLowerCaseIdentifiers': " + ex.getMessage()
"Retrieving meta-data for " + metaDataCatalogName + / + metaDataSchemaName + / + metaDataProcedureName
"Oracle JDBC driver did not return procedure/function/signature for '" + metaDataProcedureName + "' - assuming a non-exposed synonym"
"Skipping meta-data for: " + columnType + " " + procs.getInt("DATA_TYPE") + " " + procs.getString("TYPE_NAME") + " " + procs.getInt("NULLABLE") + " (probably a member of a collection)"
"Retrieved meta-data: " + meta.getParameterName() + " " + meta.getParameterType() + " " + meta.getSqlType() + " " + meta.getTypeName() + " " + meta.isNullable()
"Error while retrieving meta-data for procedure columns: " + ex
"Problem closing ResultSet for procedure column meta-data: " + ex
"Accessing single output value when procedure has more than one output parameter"
"Using declared out parameter '" + paramName + "' for function return value"
"Using declared parameter for '" + (paramNameToUseBANGEQnullQUESparamNameToUse:getFunctionReturnName()) + "'"
"Bypassing meta-data return parameter for '" + paramName + "'"
"Added meta-data return parameter for '" + returnNameToUse + "'"
"Added meta-data out parameter for '" + paramNameToUse + "'"
"Added meta-data in-out parameter for '" + paramNameToUse + "'"
"Added meta-data in parameter for '" + paramNameToUse + "'"
"Limited set of parameters " + limitedInParamNamesMap.keySet() + " skipped parameter for '" + paramNameToUse + "'"
"Unable to locate the corresponding parameter value for '" + parameterName + "' within the parameter values provided: " + caseInsensitiveParameterNames.values()
"Matching " + caseInsensitiveParameterNames.values() + " with " + callParameterNames.values()
"Found match for " + matchedParameters.keySet()
"Unable to locate the corresponding IN or IN-OUT parameter for \"" + parameterName + "\" in the parameters used: " + callParameterNames.keySet()
"Unable to locate the corresponding parameter value for '" + parameterName + "' within the parameter values provided: " + inParameters.keySet()
"Matching " + inParameters.keySet() + " with " + callParameterNames.values()
"Found match for " + matchedParameters.keySet()
"GetGeneratedKeys is supported"
"GetGeneratedKeys is not supported"
"Error retrieving 'DatabaseMetaData.getGeneratedKeys': " + ex.getMessage()
"GeneratedKeysColumnNameArray is not supported for " + databaseProductName
"GeneratedKeysColumnNameArray is supported for " + databaseProductName
"Error retrieving 'DatabaseMetaData.getDatabaseProductName': " + ex.getMessage()
"Error retrieving 'DatabaseMetaData.getDatabaseProductVersion': " + ex.getMessage()
"Error retrieving 'DatabaseMetaData.storesUpperCaseIdentifiers': " + ex.getMessage()
"Error retrieving 'DatabaseMetaData.storesLowerCaseIdentifiers': " + ex.getMessage()
"Error while accessing table meta-data results: " + ex.getMessage()
"Unable to locate table meta-data for '" + tableName + "': column names must be provided"
"Retrieving meta-data for " + metaDataCatalogName + / + metaDataSchemaName + / + metaDataTableName
"Overriding meta-data: " + columnName + " now NUMERIC instead of DECIMAL"
"Retrieved meta-data: " + meta.getParameterName() + " " + meta.getSqlType() + " " + meta.isNullable()
"Error while retrieving meta-data for table columns: " + ex.getMessage()
"Unable to locate non-key columns for table '" + getTableName() + "' so an empty insert statement is generated"
databaseProductName + " is not one of the databases fully supported for function calls " + "-- supported are: " + supportedDatabaseProductsForFunctions
"Metadata processing disabled - you must specify all parameters explicitly"
databaseProductName + " is not one of the databases fully supported for procedure calls " + "-- supported are: " + supportedDatabaseProductsForProcedures
"Metadata processing disabled - you must specify all parameters explicitly"
"Using " + provider.getClass().getName()
"Added declared parameter for [" + getProcedureName() + "]: " + parameter.getName()
"Added row mapper for [" + getProcedureName() + "]: " + parameterName
"SqlCall for " + (isFunction()QUES"function":"procedure") + " [" + getProcedureName() + "] compiled"
"Compiled stored procedure. Call string is [" + this.callString + "]"
"JdbcCall call not compiled before execution - invoking compile"
"The following parameters are used for call " + getCallString() + " with " + args
i + ": " + param.getName() + ", SQL type " + param.getSqlType() + ", type name " + param.getTypeName() + ", parameter class [" + param.getClass().getName() + "]"
"JdbcInsert for table [" + getTableName() + "] compiled"
"Compiled insert object: insert string is [" + this.insertString + "]"
"JdbcInsert not compiled before execution - invoking compile"
"The following parameters are used for insert " + getInsertString() + " with: " + values
"The following parameters are used for call " + getInsertString() + " with: " + values
"Using generated keys support with array of column names."
"Using generated keys support with Statement.RETURN_GENERATED_KEYS."
"Executing statement " + getInsertString() + " with batch of size: " + batchValues.size()
"Ignoring non-serializable message headers: " + keysToIgnore
"Uncaught error in session attribute destruction callback",ex
"Sending " + accessor.getDetailedLogMessage(message.getPayload())
"Connection established in session id=" + this.sessionId
"Failed to connect session id=" + this.sessionId,ex
"Received " + accessor.getDetailedLogMessage(message.getPayload())
"No handler for: " + accessor.getDetailedLogMessage(message.getPayload()) + ". Perhaps just unsubscribed?"
"No matching receipt: " + accessor.getDetailedLogMessage(message.getPayload())
"Message not handled."
"Uncaught failure while handling transport failure",ex2
"Connection closed in session id=" + this.sessionId
error
"Decoded " + headerAccessor.getDetailedLogMessage(payload)
"Incomplete frame, resetting input buffer..."
"Decoded " + headerAccessor.getDetailedLogMessage(null)
"Ignoring invalid content-length: '" + headerAccessor
"Starting \"system\" session, " + toString()
"Forwarding " + accessor.getShortLogMessage(EMPTY_PAYLOAD)
"Error in shutdown of TCP client",ex
"Only STOMP SEND supported from within the server side. Ignoring " + message
stompAccessor.getShortLogMessage(EMPTY_PAYLOAD)
"Ignoring DISCONNECT in session " + sessionId + ". Connection already cleaned up."
"No TCP connection for session " + sessionId + " in " + message
"TCP connection opened in session=" + getSessionId()
"TCP connection failure in session " + this.sessionId + ": " + error,ex
"Failure while clearing TCP connection state in session " + this.sessionId,ex2
"Received " + accessor.getShortLogMessage(EMPTY_PAYLOAD)
"Received " + accessor.getShortLogMessage(message.getPayload())
"Received " + accessor.getDetailedLogMessage(message.getPayload())
"Transport failure: " + ex
"TCP connection to broker closed in session " + this.sessionId
"TCP connection closed already, ignoring " + accessor.getShortLogMessage(message.getPayload())
"Forwarding " + accessor.getShortLogMessage(message.getPayload())
"Forwarding " + accessor.getDetailedLogMessage(message.getPayload())
"Failed to forward " + accessor.getShortLogMessage(message.getPayload())
"Failure while clearing TCP connection state in session " + this.sessionId,ex
"Cleaning up connection state for session " + this.sessionId
"Closing TCP connection in session " + this.sessionId
"\"System\" session connected."
"Subscribing to " + destination + " on \"system\" connection."
"Got message on \"system\" connection, with no destination: " + accessor.getDetailedLogMessage(message.getPayload())
"Got message on \"system\" connection with no handler: " + accessor.getDetailedLogMessage(message.getPayload())
"Error while handling message on \"system\" connection.",ex
"Encoding heartbeat"
"Encoding STOMP " + command + ", headers=" + nativeHeaders
"Starting..."
"Started."
"Stopping..."
"Stopped."
this + " not running yet. Ignoring " + message
this.availableEvent
this.notAvailableEvent
"Detected unsent DISCONNECT message. Processing anyway."
"Subscription selector: [" + selector + "]"
"Failed to parse selector: " + selector,ex
"Failed to evaluate selector: " + ex.getMessage()
"Failed to evaluate selector",ex
"No sessionId in  " + message
"No subscriptionId in " + message
"No destination in " + message
"No sessionId in " + message
"No subscriptionId " + message
"No destination in " + message
"Failed to send " + message,ex
"Processing " + accessor.getShortLogMessage(message.getPayload())
"Broadcasting to " + subscriptions.size() + " sessions."
"Failed to send " + message,ex
"Reply to @SubscribeMapping: " + returnValue
"No active sessions for user destination: " + result.getSourceDestination()
"Translated " + result.getSourceDestination() + " -> " + result.getTargetDestinations()
"Checking unresolved user destination: " + destination
"Translated " + destination + " -> " + getBroadcastDestination()
"No session id. Ignoring " + message
msg,cause
msg,cause
msg + ": " + cause
name + " returned null from beforeHandle, i.e. precluding the send."
"Exception from afterMessageHandled in " + interceptor,ex2
"Skipping header '" + name + "': expected type [" + type + "], but got [" + value.getClass() + "]"
getBeanName() + " added " + handler
getBeanName() + " removed " + handler
name + " returned null from preSend, i.e. precluding the send."
"Exception from afterSendCompletion in " + interceptor,ex2
"Exception from afterReceiveCompletion in " + interceptor,ex2
"Connected to " + conn.address()
"Message headers contain two values for the same header '" + name + "', " + "one in the top level header map and a second in the nested map with native headers. " + "Using the value from top level map. " + "Use 'nativeHeader.myHeader' to resolve to the value from the nested native header map."
"Arguments: " + Arrays.toString(args)
formatArgumentError(parameter,error)
"Processing return value with " + handler
"Could not resolve target class for bean with name '" + beanName + "'",ex
methods.size() + " message handler methods found on " + userType + ": " + methods
"Mapped \"" + mapping + "\" onto " + newHandlerMethod
"Searching methods to handle " + headerAccessor.getShortLogMessage(message.getPayload()) + ", lookupDestination='" + lookupDestination + "'"
"Found " + matches.size() + " handler methods: " + matches
"No matching message handler methods."
"Invoking " + handlerMethod.getShortLogMessage()
"Unhandled exception from message handler method",exception
"Invoking " + invocable.getShortLogMessage()
"Error while processing handler method exception",ex2
"Searching methods to handle " + exception.getClass().getSimpleName()
"Failed to receive message from channel '" + channel + "' within timeout: " + timeout
errorDescription + ": " + message
"Falling back to Hibernate's default producer after bean creation failure for " + beanType + ": " + ex
"Falling back to Hibernate's default producer after bean creation failure for " + beanType + ": " + ex
"Using DataSource [" + sfds + "] of Hibernate SessionFactory for HibernateTransactionManager"
"Found thread-bound Session [" + sessionHolder.getSession() + "] for Hibernate transaction"
"Found Hibernate-managed Session [" + session + "] for Spring-managed transaction"
"Opened new Session [" + newSession + "] for Hibernate transaction"
"Preparing JDBC Connection of Hibernate Session [" + session + "]"
"Not preparing JDBC Connection of Hibernate Session [" + session + "]"
"Exposing Hibernate transaction as JDBC [" + conHolder.getConnectionHandle() + "]"
"Could not rollback Session after failed transaction begin",ex
"Committing Hibernate transaction on Session [" + txObject.getSessionHolder().getSession() + "]"
"Rolling back Hibernate transaction on Session [" + txObject.getSessionHolder().getSession() + "]"
"Setting Hibernate transaction on Session [" + txObject.getSessionHolder().getSession() + "] rollback-only"
"Could not access JDBC Connection of Hibernate Session",ex
"Could not reset JDBC Connection after transaction",ex
"Closing Hibernate Session [" + session + "] after transaction"
"Not closing pre-bound Hibernate Session [" + session + "] after transaction"
"Could not retrieve pre-bound Hibernate session",ex
"Flushing Hibernate Session on transaction synchronization"
"Flushing Hibernate Session on explicit request"
"Could not close Hibernate Session",ex
"Unexpected exception on closing Hibernate Session",ex
"No ConnectionProvider found - cannot determine DataSource for SessionFactory: " + ex
"Opening Hibernate Session in OpenSessionInViewInterceptor"
"Closing Hibernate Session in OpenSessionInViewInterceptor"
"Opening Hibernate Session in OpenSessionInViewFilter"
"Closing Hibernate Session in OpenSessionInViewFilter"
"Using SessionFactory '" + getSessionFactoryBeanName() + "' for OpenSessionInViewFilter"
"Closing Hibernate Session after async request timeout/error"
"Cannot access EntityTransaction handle - assuming we're in a JTA environment"
"Joined JTA transaction"
"No JTA transaction to join: " + ex
"Joined local transaction"
"No local transaction to join"
"Starting resource-local transaction on application-managed " + "EntityManager [" + this.target + "]"
"Found thread-bound EntityManager [" + emHolder.getEntityManager() + "] for JPA transaction"
"Opened new EntityManager [" + newEm + "] for JPA transaction"
"Exposing JPA transaction as JDBC [" + conHandle + "]"
"Not exposing JPA transaction [" + em + "] as JDBC transaction because " + "JpaDialect [" + getJpaDialect() + "] does not support JDBC Connection retrieval"
"Could not rollback EntityManager after failed transaction begin",ex
"Committing JPA transaction on EntityManager [" + txObject.getEntityManagerHolder().getEntityManager() + "]"
"Rolling back JPA transaction on EntityManager [" + txObject.getEntityManagerHolder().getEntityManager() + "]"
"Setting JPA transaction on EntityManager [" + txObject.getEntityManagerHolder().getEntityManager() + "] rollback-only"
"Closing JPA EntityManager [" + em + "] after transaction"
"Not closing pre-bound JPA EntityManager after transaction"
"Initialized JPA EntityManagerFactory for persistence unit '" + getPersistenceUnitName() + "'"
"Closing JPA EntityManagerFactory for persistence unit '" + getPersistenceUnitName() + "'"
"Creating new EntityManager for shared EntityManager invocation"
"Building JPA EntityManagerFactory for persistence unit '" + getPersistenceUnitName() + "'"
"Building JPA container EntityManagerFactory for persistence unit '" + this.persistenceUnitInfo.getPersistenceUnitName() + "'"
"Could not join transaction because none was actually active",ex
"Could not close JPA EntityManager",ex
"Unexpected exception on closing JPA EntityManager",ex
"Found explicit default unit with name '" + defaultName + "' in persistence.xml - " + "overriding local default unit settings ('packagesToScan'/'mappingResources')"
"Failed to determine persistence unit root URL from orm.xml location",ex
"Cannot resolve jar-file entry [" + value + "] in persistence unit '" + unitInfo.getPersistenceUnitName() + "' without root URL"
resource.getFilename() + " should be located inside META-INF directory; cannot determine persistence unit root URL for " + resource
resource.getFilename() + " is not located in the root of META-INF directory; cannot determine persistence unit root URL for " + resource
"Transformer of class [" + this.classTransformer.getClass().getName() + "] transformed class [" + className + "]; bytes in=" + classfileBuffer.length + "; bytes out=" + transformed.length
"Circularity error while weaving class [" + className + "] with " + "transformer of class [" + this.classTransformer.getClass().getName() + "]",ex
"Error weaving class [" + className + "] with transformer of class [" + this.classTransformer.getClass().getName() + "]",ex
"Opening JPA EntityManager in OpenEntityManagerInViewFilter"
"Closing JPA EntityManager in OpenEntityManagerInViewFilter"
"Closing JPA EntityManager after async request timeout/error"
"Opening JPA EntityManager in OpenEntityManagerInViewInterceptor"
"Closing JPA EntityManager in OpenEntityManagerInViewInterceptor"
message,ex
"Remote service [" + serviceName + "] threw exception",ex
"Binding service '" + this.serviceName + "' to RMI registry: " + this.registry
"Looking for RMI registry at port '" + registryPort + "' of host [" + registryHost + "]"
"Creating new RMI registry"
"Looking for RMI registry at port '" + registryPort + "', using custom socket factory"
"RMI registry access threw exception",ex
"Could not detect RMI registry - creating new one"
"Creating new RMI registry"
"Looking for RMI registry at port '" + registryPort + "'"
"RMI registry access threw exception",ex
"Could not detect RMI registry - creating new one"
"Unbinding RMI service '" + this.serviceName + "' from registry" + (this.createdRegistryQUES(" at port '" + this.registryPort + "'"):"")
"RMI service '" + this.serviceName + "' is not bound to registry" + (this.createdRegistryQUES(" at port '" + this.registryPort + "' anymore"):""),ex
"RMI object for service '" + this.serviceName + "' is not exported anymore",ex
"Looking for RMI registry at port '" + registryPort + "' of host [" + registryHost + "]"
"Creating new RMI registry"
"Looking for RMI registry at port '" + registryPort + "', using custom socket factory"
"RMI registry access threw exception",ex
"Could not detect RMI registry - creating new one"
"Creating new RMI registry"
"Looking for RMI registry at port '" + registryPort + "'"
"RMI registry access threw exception",ex
"Could not detect RMI registry - creating new one"
"Unexporting RMI registry"
"RMI stub [" + getServiceUrl() + "] is an RMI invoker"
"Using service interface [" + getServiceInterface().getName() + "] for RMI stub [" + getServiceUrl() + "] - " + (BANGisImplQUES"not ":"") + "directly implemented"
"Located RMI stub with URL [" + getServiceUrl() + "]"
msg,ex
msg
"RMI service [" + getService() + "] is an RMI invoker"
"JNDI RMI object [" + getJndiName() + "] is an RMI invoker"
"Using service interface [" + getServiceInterface().getName() + "] for JNDI RMI object [" + getJndiName() + "] - " + (BANGisImplQUES"not ":"") + "directly implemented"
"Could not connect to RMI service [" + getJndiName() + "] - retrying",ex
"Could not connect to RMI service [" + getJndiName() + "] - retrying"
"Binding RMI service to JNDI location [" + this.jndiName + "]"
"Unbinding RMI service from JNDI location [" + this.jndiName + "]"
"Executing " + invocation
"Could not find target method for " + invocation,ex
"Could not access target method for " + invocation,ex
"Target method failed for " + invocation,ex.getTargetException()
"Starting HttpServer at address " + address
"Stopping HttpServer"
"Incoming " + this.exporterNameClause + "remote call: " + ClassUtils.getQualifiedMethodName(method)
"Finished processing of " + this.exporterNameClause + "remote call: " + ClassUtils.getQualifiedMethodName(method)
"Processing of " + this.exporterNameClause + "remote call resulted in fatal exception: " + ClassUtils.getQualifiedMethodName(method),ex
"Processing of " + this.exporterNameClause + "remote call resulted in exception: " + ClassUtils.getQualifiedMethodName(method),ex
"Using ThemeSource [" + themeSource + "]"
"Unable to locate ThemeSource with name '" + THEME_SOURCE_BEAN_NAME + "': using default [" + themeSource + "]"
"Theme created: name '" + themeName + "', basename [" + basename + "]"
"DataBinder requires binding of required fields [" + StringUtils.arrayToCommaDelimitedString(requiredFields) + "]"
"Field [" + field + "] has been removed from PropertyValues " + "and will not be bound, because it has not been found in the list of allowed fields"
"Invoking validator [" + validator + "]"
"Validator found " + errors.getErrorCount() + " errors"
"Validator found no errors"
"Invalidating " + (keyBANGEQnullQUES"cache key [" + key + "]":"entire cache") + " for operation " + operation + " on method " + context.metadata.method
"No cache entry for key '" + key + "' in cache(s) " + context.getCacheNames()
"Cache entry for key '" + key + "' found in cache '" + cache.getName() + "'"
"Cache condition failed on method " + context.metadata.method + " for operation " + context.metadata.operation
"Computed cache key '" + key + "' for operation " + context.metadata.operation
"Adding method [" + methodName + "] with cache operations [" + ops + "]"
"Adding cacheable method '" + method.getName() + "' with attribute: " + cacheOps
"Converted JNDI name [" + convertedName + "] not found - trying original name [" + jndiName + "]. " + ex
"Located object with JNDI name [" + convertedName + "]"
"JNDI lookup for name [" + name + "] returned: [" + value + "]"
"JNDI lookup for name [" + name + "] threw NamingException " + "with message: " + ex.getMessage() + ". Returning null."
"Could not close JNDI InitialContext",ex
"Looking up JNDI object with name [" + name + "]"
"Binding JNDI object with name [" + name + "]"
"Rebinding JNDI object with name [" + name + "]"
"Unbinding JNDI object with name [" + name + "]"
"JNDI lookup failed - returning specified default object instead",ex
"JNDI lookup failed - returning specified default object instead: " + ex
"The ClassLoader [" + classLoader.getClass().getName() + "] does NOT provide a " + "'getThrowawayClassLoader()' method; SimpleThrowawayClassLoader will be used instead."
"Could not apply remove-on-cancel policy - not a ScheduledThreadPoolExecutor"
"Initializing ExecutorService" + (this.beanNameBANGEQnullQUES" '" + this.beanName + "'":"")
"Shutting down ExecutorService" + (this.beanNameBANGEQnullQUES" '" + this.beanName + "'":"")
"Timed out while waiting for executor" + (this.beanNameBANGEQnullQUES" '" + this.beanName + "'":"") + " to terminate"
"Interrupted while waiting for executor" + (this.beanNameBANGEQnullQUES" '" + this.beanName + "'":"") + " to terminate"
"Could not apply remove-on-cancel policy - not a ScheduledThreadPoolExecutor"
"Could not apply remove-on-cancel policy - not a ScheduledThreadPoolExecutor"
"Failed to retrieve [" + this.jndiName + "] from JNDI",ex
"Could not find default managed thread factory in JNDI - " + "proceeding with default local thread factory"
"Unexpected error occurred in scheduled task.",t
getInvocationFailureMessage(),ex.getTargetException()
getInvocationFailureMessage(),ex
"Could not find unique TaskScheduler bean",ex
"More than one TaskScheduler bean exists within the context, and " + "none is named 'taskScheduler'. Mark one of them as primary or name it 'taskScheduler' " + "(possibly as an alias); or implement the SchedulingConfigurer interface and call " + "ScheduledTaskRegistrar#setScheduler explicitly within the configureTasks() callback: " + ex.getBeanNamesFound()
"Could not find default TaskScheduler bean",ex
"Could not find unique ScheduledExecutorService bean",ex2
"More than one ScheduledExecutorService bean exists within the context, and " + "none is named 'taskScheduler'. Mark one of them as primary or name it 'taskScheduler' " + "(possibly as an alias); or implement the SchedulingConfigurer interface and call " + "ScheduledTaskRegistrar#setScheduler explicitly within the configureTasks() callback: " + ex2.getBeanNamesFound()
"Could not find default ScheduledExecutorService bean",ex2
"No @Scheduled annotations found on bean class: " + targetClass
annotatedMethods.size() + " @Scheduled methods processed on bean '" + beanName + "': " + annotatedMethods
"Could not connect to remote EJB [" + getJndiName() + "] - retrying",ex
"Could not connect to remote EJB [" + getJndiName() + "] - retrying"
"Trying to create reference to remote EJB"
"Obtained reference to remote EJB: " + ejbInstance
"Could not invoke 'remove' on remote EJB proxy",ex
"Method of local EJB [" + getJndiName() + "] threw exception",targetEx
"Trying to create reference to local EJB"
"Obtained reference to local EJB: " + ejbInstance
"Could not invoke 'remove' on local EJB proxy",ex
"Could not determine scripted object type for bean '" + beanName + "': " + ex.getMessage()
"Could not determine scripted object type for bean '" + beanName + "'",ex
getResource() + " could not be resolved in the file system - " + "current timestamp not available for script modification check",ex
"Determined server-specific load-time weaver: " + serverSpecificLoadTimeWeaver.getClass().getName()
"Found Spring's JVM agent for instrumentation"
"Using reflective load-time weaver for class loader: " + this.loadTimeWeaver.getInstrumentableClassLoader().getClass().getName()
"Could not obtain server-specific LoadTimeWeaver: " + ex.getMessage()
"Removing all registered transformers for class loader: " + this.loadTimeWeaver.getInstrumentableClassLoader().getClass().getName()
"ResourceBundle [" + basename + "] not found for MessageSource: " + ex.getMessage()
"ResourceBundleMessageSource is configured to read resources with encoding '" + encoding + "' but ResourceBundle.Control not supported in current system environment: " + ex.getMessage() + " - falling back to plain ResourceBundle.getBundle retrieval with the " + "platform default encoding. Consider setting the 'defaultEncoding' property to 'null' " + "for participating in the platform default and therefore avoiding this log message."
"Bean '" + beanName + "' of type [" + bean.getClass().getName() + "] is not eligible for getting processed by all BeanPostProcessors " + "(for example: not eligible for auto-proxying)"
"Re-caching properties for filename [" + filename + "] - file hasn't been modified"
resource + " could not be resolved in the file system - assuming that it hasn't changed",ex
"Could not parse properties file [" + resource.getFilename() + "]",ex
"No properties file found for [" + filename + "] - neither plain properties nor XML"
"Loading properties [" + resource.getFilename() + "]"
"Loading properties [" + resource.getFilename() + "] with encoding '" + encoding + "'"
"Loading properties [" + resource.getFilename() + "]"
"Clearing entire resource bundle cache"
"Added message [" + msg + "] for code [" + code + "] and Locale [" + locale + "]"
"SimpleThreadScope does not support destruction callbacks. " + "Consider using RequestScope in a web environment."
"Exception encountered during context initialization - " + "cancelling refresh attempt: " + ex
"Refreshing " + this
"Refreshing " + getDisplayName()
"Using MessageSource [" + this.messageSource + "]"
"No '" + MESSAGE_SOURCE_BEAN_NAME + "' bean, using [" + this.messageSource + "]"
"Using ApplicationEventMulticaster [" + this.applicationEventMulticaster + "]"
"No '" + APPLICATION_EVENT_MULTICASTER_BEAN_NAME + "' bean, using " + "[" + this.applicationEventMulticaster.getClass().getSimpleName() + "]"
"Using LifecycleProcessor [" + this.lifecycleProcessor + "]"
"No '" + LIFECYCLE_PROCESSOR_BEAN_NAME + "' bean, using " + "[" + this.lifecycleProcessor.getClass().getSimpleName() + "]"
"Closing " + this
"Exception thrown from ApplicationListener handling ContextClosedEvent",ex
"Exception thrown from LifecycleProcessor on context close",ex
"Starting bean '" + beanName + "' of type [" + bean.getClass().getName() + "]"
"Successfully started bean '" + beanName + "'"
"Asking bean '" + beanName + "' of type [" + bean.getClass().getName() + "] to stop"
"Bean '" + beanName + "' completed its stop procedure"
"Stopping bean '" + beanName + "' of type [" + bean.getClass().getName() + "]"
"Successfully stopped bean '" + beanName + "'"
"Failed to stop bean '" + beanName + "'",ex
"Starting beans in phase " + this.phase
"Stopping beans in phase " + this.phase
"Failed to shut down " + countDownBeanNames.size() + " bean" + (countDownBeanNames.size()GT1QUES"s":"") + " with phase value " + this.phase + " within timeout of " + this.timeout + ": " + countDownBeanNames
"Loaded " + result.size() + "] index(es)"
String.format("Ignoring request to enhance %s as it has " + "already been enhanced. This usually indicates that more than one " + "ConfigurationClassPostProcessor has been registered (e.g. via " + "<context:annotation-config>). This is harmless, but you may " + "want check your configuration and remove one CCPP if possible",configClass.getName())
String.format("Successfully enhanced %s; enhanced class name is: %s",configClass.getName(),enhancedClass.getName())
String.format("@Bean method %s.%s is non-static and returns an object " + "assignable to Spring's BeanFactoryPostProcessor interface. This will " + "result in a failure to process annotations such as @Autowired, " + "@Resource and @PostConstruct within the method's declaring " + "@Configuration class. Add the 'static' modifier to this method to avoid " + "these container lifecycle issues; see @Bean javadoc for complete details.",beanMethod.getDeclaringClass().getSimpleName(),beanMethod.getName())
String.format("@Bean method %s.%s called as bean reference " + "for type [%s] returned null bean; resolving to null value.",beanMethod.getDeclaringClass().getSimpleName(),beanMethod.getName(),beanMethod.getReturnType().getName())
"Creating interface proxy for FactoryBean '" + beanName + "' of type [" + clazz.getName() + "] for use within another @Bean method because its " + (finalClassQUES"implementation class":"getObject() method") + " is final: Otherwise a getObject() call would not be routed to the factory."
"Unable to proxy FactoryBean '" + beanName + "' of type [" + clazz.getName() + "] for use within another @Bean method because its " + (finalClassQUES"implementation class":"getObject() method") + " is final: A getObject() call will NOT be routed to the factory. " + "Consider declaring the return type as a FactoryBean interface."
"Unable to instantiate enhanced FactoryBean using Objenesis, " + "falling back to regular construction",ex
"Bean definition has already been processed as a configuration class: " + beanDef
"Cannot enhance @Configuration bean definition '" + beanName + "' since its singleton instance has been created too early. The typical cause " + "is a non-static @Bean method with a BeanDefinitionRegistryPostProcessor " + "return type: Consider declaring such methods as 'static'."
String.format("Replacing bean definition '%s' existing class '%s' with " + "enhanced class '%s'",entry.getKey(),configClass.getName(),enhancedClass.getName())
"JSR-250 'javax.annotation.ManagedBean' found and supported for component scanning"
"JSR-330 'javax.inject.Named' annotation found and supported for component scanning"
"Using candidate component class from index: " + type
"Ignored because not a concrete top-level class: " + type
"Ignored because matching an exclude filter: " + type
"Scanning " + resource
"Identified candidate component class: " + resource
"Ignored because not a concrete top-level class: " + resource
"Ignored because not matching any filter: " + resource
"Ignored because not readable: " + resource
"Could not find class file for introspecting configuration annotations: " + className,ex
"Failed to introspect @Bean methods on class [" + metadata.getClassName() + "]: " + ex
"Ignoring @PropertySource annotation on [" + sourceClass.getMetadata().getClassName() + "]. Reason: Environment must implement ConfigurableEnvironment"
"Failed to read class file via ASM for determining @Bean method order",ex
"Properties location [" + location + "] not resolvable: " + ex.getMessage()
"Failed to resolve member class [" + memberClassName + "] - not considering it as a configuration class candidate"
String.format("%s was imported but no annotations were found " + "having both 'mode' and 'proxyTargetClass' attributes of type " + "AdviceMode and boolean respectively. This means that auto proxy " + "creator registration and configuration may not have occurred as " + "intended, and components may not be proxied as expected. Check to " + "ensure that %s has been @Import'ed on the same class where these " + "annotations are declared; otherwise remove the import of %s " + "altogether.",name,name,name)
"Registered bean definition for imported class '" + configBeanName + "'"
String.format("Registering bean definition for @Bean method %s.%s()",configClass.getMetadata().getClassName(),beanName)
String.format("Skipping bean definition for %s: a definition for bean '%s' " + "already exists. This top-level bean definition is considered as an override.",beanMethod,beanName)
"Non-matching event type for listener: " + listener,ex
"Could not resolve target class for bean with name '" + beanName + "'",ex
"Could not resolve target bean for scoped proxy '" + beanName + "'",ex
"Could not resolve methods for bean with name '" + beanName + "'",ex
"No @EventListener annotations found on bean class: " + targetType.getName()
annotatedMethods.size() + " @EventListener methods processed on bean '" + beanName + "': " + annotatedMethods
"No result object given - no result to handle"
"Registering NotificationListener for MBeans " + Arrays.asList(this.actualObjectNames)
"Unable to unregister NotificationListener",ex
"Connecting to remote MBeanServer at URL [" + serviceUrl + "]"
"Attempting to locate local MBeanServer"
"Could not close JMX connector",ex
msg,ex
msg
"MBeanExporter not running in a ListableBeanFactory: autodetection of MBeans not available."
"Registering beans for JMX exposure on startup"
"Unregistering JMX-exposed beans on shutdown"
"Located MBean '" + beanKey + "': registering with JMX server as MBean [" + objectName + "]"
"Located managed bean '" + beanKey + "': registering with JMX server as MBean [" + objectName + "]"
"Located MBean '" + beanKey + "': registering with JMX server as lazy-init MBean [" + objectName + "]"
"Located simple bean '" + beanKey + "': registering with JMX server as lazy-init MBean [" + objectName + "]"
"Bean with key '" + beanKey + "' has been registered as an MBean but has no exposed attributes or operations"
"Bean with name '" + beanName + "' has been autodetected for JMX exposure"
"Bean with name '" + beanName + "' is already registered for JMX exposure"
"Unable to unregister NotificationListener",ex
"Failed to retrieve target for JMX-exposed bean [" + this.objectName + "]: " + ex
"Loading JMX object name mappings file from " + location
"No existing MBeanServer found - creating new one"
"Ignoring existing MBean at [" + objectName + "]"
"Replacing existing MBean at [" + objectName + "]"
"Unable to replace existing MBean at [" + objectName + "]",ex2
"Unregistering JMX-exposed beans"
"Could not unregister MBean [" + objectName + "] as said MBean " + "is not registered (perhaps already unregistered by an external process)"
"Could not unregister MBean [" + objectName + "]",ex
"Found more than one MBeanServer instance" + (agentIdBANGEQnullQUES" with agent id [" + agentId + "]":"") + ". Returning first from list."
"Found MBeanServer: " + server
"JMX connector server started: " + this.connectorServer
"Stopping JMX connector server: " + this.connectorServer
"Using JTA UserTransaction: " + this.userTransaction
"Using JTA TransactionManager: " + this.transactionManager
"No JTA TransactionManager found: transaction suspension not available"
"Using JTA TransactionSynchronizationRegistry: " + this.transactionSynchronizationRegistry
"Retrieving JTA UserTransaction from JNDI location [" + userTransactionName + "]"
"Retrieving JTA TransactionManager from JNDI location [" + transactionManagerName + "]"
"Retrieving JTA TransactionSynchronizationRegistry from JNDI location [" + registryName + "]"
"JTA UserTransaction found at default JNDI location [" + jndiName + "]"
"No JTA UserTransaction found at default JNDI location [" + jndiName + "]",ex
"JTA UserTransaction object [" + ut + "] implements TransactionManager"
"JTA TransactionManager found at fallback JNDI location [" + jndiName + "]"
"No JTA TransactionManager found at fallback JNDI location [" + jndiName + "]",ex
"JTA TransactionSynchronizationRegistry found at default JNDI location [" + jndiName + "]"
"No JTA TransactionSynchronizationRegistry found at default JNDI location [" + jndiName + "]",ex
"Rollback failure with transaction already marked as rolled back: " + ex
"Rollback failure with transaction already marked as rolled back: " + ex
"Setting JTA transaction rollback-only"
"Registering after-completion synchronization with existing JTA transaction"
"Participating in existing JTA transaction that has been marked for rollback: " + "cannot register Spring after-completion callbacks with outer JTA transaction - " + "immediately performing Spring after-completion callbacks with outcome status 'rollback'. " + "Original exception: " + ex
"Participating in existing JTA transaction, but unexpected internal transaction " + "state encountered: cannot register Spring after-completion callbacks with outer JTA " + "transaction - processing Spring after-completion callbacks with outcome status 'unknown'" + "Original exception: " + ex
"Failed to reset transaction timeout after JTA completion",ex
"Could not set JTA transaction rollback-only",ex
"No JTA transaction handle available and/or running on WebLogic - " + "relying on JTA provider to mark the transaction as rollback-only based on " + "the exception thrown from beforeCompletion"
"Retrieving WebSphere UOWManager from JNDI location [" + uowManagerName + "]"
"Retrieving WebSphere UOWManager from default JNDI location [" + DEFAULT_UOW_MANAGER_NAME + "]"
"WebSphere UOWManager is not available at default JNDI location [" + DEFAULT_UOW_MANAGER_NAME + "] - falling back to UOWManagerFactory lookup"
"Creating new transaction with name [" + definition.getName() + "]: " + definition
"Invoking WebSphere UOW action: type=" + uowType + ", join=" + joinTx
"Returned from WebSphere UOW action: type=" + uowType + ", join=" + joinTx
"Application exception overridden by rollback exception",appEx
"Rolling back on application exception from transaction callback",ex
"Transaction callback has explicitly requested rollback"
"Retrieving JTA UserTransaction from WebLogic TransactionHelper"
"Retrieving JTA TransactionManager from WebLogic TransactionHelper"
"WebLogic TransactionHelper found"
"Support for WebLogic transaction names available"
"Support for WebLogic transaction names not available"
"WebLogic ClientTransactionManager found"
"Support for WebLogic forceResume available"
"Support for WebLogic forceResume not available"
"Standard JTA resume threw InvalidTransactionException: " + ex.getMessage() + " - trying WebLogic JTA forceResume"
"Adding transactional method '" + methodIdentification + "' with attribute: " + txAttr
"Applying rules to determine whether transaction should rollback on " + ex
"Winning rollback rule is: " + winner
"No relevant rollback rule found: applying default rules"
"Replacing attribute for transactional method [" + method + "]: current name '" + name + "' is more specific than '" + regMethodName + "'"
"Keeping attribute for transactional method [" + method + "]: current name '" + name + "' is not more specific than '" + regMethodName + "'"
"Adding transactional method [" + method + "] with attribute [" + attr + "]"
"Adding transactional method [" + methodName + "] with attribute [" + attr + "]"
"Application exception overridden by commit exception",throwableHolder.throwable
"Application exception overridden by commit exception",throwableHolder.throwable
"Skipping transactional joinpoint [" + joinpointIdentification + "] because no transaction manager has been configured"
"Getting transaction for [" + txInfo.getJoinpointIdentification() + "]"
"Don't need to create transaction for [" + joinpointIdentification + "]: This method isn't transactional."
"Completing transaction for [" + txInfo.getJoinpointIdentification() + "]"
"Completing transaction for [" + txInfo.getJoinpointIdentification() + "] after exception: " + ex
"Application exception overridden by rollback exception",ex
"Application exception overridden by rollback exception",ex
"Application exception overridden by commit exception",ex
"Application exception overridden by commit exception",ex
"Initiating transaction rollback on application exception",ex
"Application exception overridden by rollback exception",ex
"Application exception overridden by rollback exception",ex
"TransactionSynchronization.beforeCompletion threw exception",tsex
"TransactionSynchronization.afterCompletion threw exception",tsex
"Retrieved value [" + value + "] for key [" + actualKey + "] bound to thread [" + Thread.currentThread().getName() + "]"
"Bound value [" + value + "] for key [" + actualKey + "] to thread [" + Thread.currentThread().getName() + "]"
"Removed value [" + value + "] for key [" + actualKey + "] from thread [" + Thread.currentThread().getName() + "]"
"Initializing transaction synchronization"
"Clearing transaction synchronization"
"Creating new transaction with name [" + definition.getName() + "]: " + definition
"Custom isolation level specified but no actual transaction initiated; " + "isolation level will effectively be ignored: " + definition
"Suspending current transaction"
"Suspending current transaction, creating new transaction with name [" + definition.getName() + "]"
"Creating nested transaction with name [" + definition.getName() + "]"
"Participating in existing transaction"
exMessage,beginEx
"Transactional code has requested rollback"
"Global transaction is marked as rollback-only but transactional code requested commit"
"Releasing transaction savepoint"
"Initiating transaction commit"
"Rolling back transaction to savepoint"
"Initiating transaction rollback"
"Participating transaction failed - marking existing transaction as rollback-only"
"Participating transaction failed - letting transaction originator decide on rollback"
"Should roll back transaction but cannot - no transaction available"
"Initiating transaction rollback after commit exception",ex
"Marking existing transaction as rollback-only after commit exception",ex
"Commit exception overridden by rollback exception",ex
"Triggering beforeCommit synchronization"
"Triggering beforeCompletion synchronization"
"Triggering afterCommit synchronization"
"Triggering afterCompletion synchronization"
"Resuming suspended transaction after completion of inner transaction"
"Cannot register Spring after-completion synchronization with existing transaction - " + "processing Spring after-completion callbacks immediately, with outcome status 'unknown'"
"Processing " + event + " as a fallback execution on AFTER_ROLLBACK phase"
"No transaction is active - skipping " + event
"Established shared CCI Connection: " + this.target
"Could not close shared CCI Connection",ex
"Acquired Connection [" + con + "] for local CCI transaction"
"Committing CCI local transaction on Connection [" + con + "]"
"Rolling back CCI local transaction on Connection [" + con + "]"
"Setting CCI local transaction [" + txObject.getConnectionHolder().getConnection() + "] rollback-only"
"Releasing CCI Connection [" + con + "] after transaction"
"Opening CCI Connection"
"Could not close CCI Connection",ex
"Could not close CCI Interaction",ex
"Could not close CCI ResultSet",ex
"Transaction marked as rollback-only after endpoint exception",ex
"Failed to complete transaction after endpoint delivery",ex
"Could not complete unfinished transaction on endpoint release",ex
"Starting SpringContextResourceAdapter with BootstrapContext: " + bootstrapContext
"Stopping SpringContextResourceAdapter"
WebSocketMessageBrokerStats.this.toString()),initialDelay,this.loggingPeriod,TimeUnit.MILLISECONDS
"Connecting to WebSocket at " + getUri()
"Successfully connected"
"Failed to connect",ex
"Connecting to " + uri
"Starting " + getClass().getSimpleName()
"Stopping " + getClass().getSimpleName()
"Failed to stop WebSocket connection",ex
"Failed to stop Jetty WebSocketClient",ex
"Handshake request headers: " + requestHeaders
"Handshake response headers: " + response.getHeaders()
"Connecting to WebSocket at " + getUri()
"Successfully connected to WebSocket"
"Failed to connect to WebSocket",ex
"Connecting to WebSocket at " + getUri()
"Successfully connected to WebSocket"
"Failed to connect to WebSocket",ex
"Incomplete STOMP frame content received in session " + session + ", bufferSize=" + decoder.getBufferSize() + ", bufferSizeLimit=" + decoder.getBufferSizeLimit() + "."
"Failed to parse " + webSocketMessage + " in session " + session.getId() + ". Sending STOMP ERROR to client.",ex
"From client: " + headerAccessor.getShortLogMessage(message.getPayload())
"Failed to send client message to application via MessageChannel" + " in session " + session.getId() + ". Sending STOMP ERROR to client.",ex
"Error publishing " + event,ex
"Expected byte[] payload. Ignoring " + message + "."
"No STOMP \"subscription\" header in " + message
"Failed to send WebSocket message to client in session " + session.getId(),ex
"ReadInactivityTask failure",ex
"WriteInactivityTask failure",ex
"Failed to close session: " + session.getId(),ex
"Incomplete STOMP frame content received, bufferSize=" + this.bufferingDecoder.getBufferSize() + ", bufferSizeLimit=" + this.bufferingDecoder.getBufferSizeLimit() + "."
"Looking for @MessageExceptionHandler mappings: " + context
"Detected @MessageExceptionHandler methods in " + bean
"No sub-protocols for " + handler
"Failed to close '" + holder.getSession() + "': " + ex
"Could not find session id in " + message
"Terminating '" + session + "'",ex
"Failure while closing session " + sessionId + ".",secondException
"Failed to send message to client in " + session + ": " + message,ex
"No messages received after " + timeSinceCreated + " ms. " + "Closing " + holder.getSession() + "."
"Failed to close unreliable " + session,ex
"Clearing session " + session.getId()
"Successful request upgrade: " + upgradeResponse.getHeaders()
message
"Using @ServerEndpoint singleton " + endpoint
"Using @ServerEndpoint singleton " + endpoint
"Creating new @ServerEndpoint instance of type " + endpointClass
"Registering @ServerEndpoint class: " + endpointClass
"Registering ServerEndpointConfig: " + endpointConfig
servletRequest.getMethod() + " " + servletRequest.getRequestURI()
"Handshake request rejected, Origin header value " + request.getHeaders().getOrigin() + " not allowed"
"Processing request " + request.getURI() + " with headers=" + headers
"Handshake failed due to unexpected HTTP method: " + request.getMethod()
"Missing \"Sec-WebSocket-Key\" header"
"Upgrading to WebSocket, subProtocol=" + subProtocol + ", extensions=" + extensions
"Handshake failed due to invalid Upgrade header: " + request.getHeaders().getUpgrade()
"Handshake failed due to invalid Connection header " + request.getHeaders().getConnection()
"Handshake failed due to unsupported WebSocket version: " + version + ". Supported versions: " + Arrays.toString(getSupportedVersions())
interceptor + " returns false from beforeHandshake - precluding handshake"
interceptor + " threw exception in afterHandshake: " + ex
"Sending " + message + ", " + this
"Closing " + this
"Unhandled exception after connection closed for " + this,ex
"Unhandled on-close exception for " + this.wsSession,ex
"New " + session
"Handling " + message + " in " + session
"Transport error in " + session,exception
session + " closed with " + closeStatus
String.format("Another send already in progress: " + "session id '%s':, \"in-progress\" send time %d (ms), buffer size %d bytes",getId(),getTimeSinceSendStarted(),getBufferSize())
"Dropped " + i + " messages, buffer size: " + getBufferSize()
"Changing close status " + status + " to SESSION_NOT_RELIABLE."
"Error while destroying " + handler,ex
"Unhandled exception after connection closed for " + this,ex
"Closing session due to exception for " + session,exception
"No transport handlers specified for TransportHandlingSockJsService"
"No handler configured for raw WebSocket messages"
"Unknown transport type for " + request.getURI()
"No TransportHandler for " + request.getURI()
"Session not found, sessionId=" + sessionId + ". The session may have been closed " + "(e.g. missed heart-beat) while a message was coming in."
"The user for the session does not match the user for the request."
"Session type does not match the transport type for the request."
"Origin check enabled but transport '" + transport + "' does not support it."
"Closed " + removedIds.size() + " sessions: " + removedIds
"Broken data received. Terminating WebSocket connection abruptly",ex
"Writing " + frame
this.messageCache.size() + " message(s) to flush in session " + getId()
"Session is active, ready to flush."
"Session is not active, not ready to flush."
"Writing to HTTP response: " + formattedFrame
"Closing SockJS session " + getId() + " with " + status
"Failure while sending SockJS close frame",ex
"Error from WebSocketHandler.afterConnectionClosed in " + this,ex
"Scheduled heartbeat in session " + getId()
"Cancelling heartbeat in session " + getId()
"Preparing to write " + frame
"Terminating connection after failure to send message to client",ex
"Closing due to transport error for " + this
"Failure while closing " + this,closeException
this.byteCount + " bytes written so far, " + getMessageCache().size() + " more messages not flushed"
"Streamed bytes limit reached, recycling current request"
request.getMethod() + " " + request.getURI()
"Connection already closed (but not removed yet) for " + sockJsSession
"Starting " + getTransportType() + " async request."
"Another " + getTransportType() + " connection still open for " + sockJsSession
"Failed to read message",ex
"Failed to read message",ex
"Received message(s): " + Arrays.asList(messages)
"Failed to create a default WebSocketTransportHandler",ex
"Initial SockJS \"Info\" request to server failed, url=" + url,exception
"Starting " + this
"Scheduling connect to time out after " + this.timeoutValue + " ms."
"Connect timeout task not scheduled (no TaskScheduler configured)."
"Connect success/failure already handled for " + DefaultTransportRequest.this
message
DefaultTransportRequest.this + " failed. Falling back on next transport.",ex
"No more fallback transports after " + DefaultTransportRequest.this,ex
"Transport failed to run timeout tasks for " + DefaultTransportRequest.this,ex2
"Connect success/failure events already took place for " + DefaultTransportRequest.this + ". Ignoring this additional failure event.",ex
"Starting XHR receive request, url=" + receiveUrl
"XHR receive headers: " + response.getHeaders()
"SockJS sockJsSession closed, closing response."
"XHR receive completed"
"XHR receive content: " + content
"Starting XHR receive request for " + url
"XHR receive headers: " + toHttpHeaders(response.getResponseHeaders())
"SockJS sockJsSession closed, closing response."
"XHR content received: " + content
"XHR receive request completed."
"Starting XHR " + (isXhrStreamingDisabled()QUES"Polling":"Streaming") + "session url=" + receiveUrl
"Executing SockJS Info request, url=" + infoUrl
"SockJS Info request (url=" + infoUrl + ") failed: " + response
"SockJS Info request (url=" + infoUrl + ") response: " + response
"Starting XHR send, url=" + url
"XHR send request (url=" + url + ") failed: " + response
"XHR send request (url=" + url + ") response: " + response
"Starting XHR receive request, url=" + url
"SockJS sockJsSession closed, closing response."
"XHR content received: " + content
"XHR receive request completed."
"Starting WebSocket session on " + url
"Failed to close " + this + " after transport timeout",ex
"Sending message " + messageToSend + " in " + this
"Closing session with " + status + " in " + this
"Failed to close " + this,ex
"Ignoring close since connect() was never invoked"
"Ignoring close (already closing or closed): current state " + this.state
"Received heartbeat in " + this
"Processing SockJS open frame in " + this
"WebSocketHandler.afterConnectionEstablished threw exception in " + this,ex
"Open frame received in " + getId() + " but we're not connecting (current state " + this.state + "). The server might have been restarted and lost track of the session."
"Ignoring received message due to state " + this.state + " in " + this
"Failed to decode data for SockJS \"message\" frame: " + frame + " in " + this,ex
"Processing SockJS message frame " + frame.getContent() + " in " + this
"WebSocketHandler.handleMessage threw an exception on " + frame + " in " + this,ex
"Processing SockJS close frame with " + closeStatus + " in " + this
"Failed to decode data for " + frame + " in " + this,ex
"Transport error in " + this,error
"WebSocketHandler.handleTransportError threw an exception",ex
"Transport closed with " + cs + " in " + this
"WebSocketHandler.afterConnectionClosed threw an exception",ex
"Expected SockJS path. Failing request: " + request.getURI()
"Processing transport request: " + requestInfo
"Processing transport request: " + requestInfo
"Iframe support is disabled when an origin check is required. " + "Ignoring transport request: " + requestInfo
"Processing transport request: " + requestInfo
"Processing transport request: " + requestInfo
"WebSocket disabled. Ignoring transport request: " + requestInfo
"Invalid SockJS path '" + sockJsPath + "' - required to have 3 path segments"
"Ignoring transport request: " + requestInfo
"WebSocket disabled. Ignoring transport request: " + requestInfo
"Ignoring transport request: " + requestInfo
"Processing transport request: " + requestInfo
"No server, session, or transport path segment in SockJS request."
"Either server or session contains a \".\" which is not allowed by SockJS protocol."
"Origin header value '" + request.getHeaders().getOrigin() + "' not allowed."
"Sending Method Not Allowed (405)"
"Using non-singleton proxies with singleton targets is often undesirable. " + "Enable prototype proxies by setting the 'targetName' property."
"Creating copy of prototype ProxyFactoryBean config: " + this
"Using ProxyCreatorSupport copy: " + copy
"Bean with name '" + finalName + "' concluding interceptor chain " + "is not an advisor class: treating it as a target or TargetSource"
"Could not determine type of bean with name '" + beanName + "' - assuming it is neither an Advisor nor an Advice"
"Configuring advisor or advice '" + name + "'"
"Refreshing bean named '" + pa.getBeanName() + "'"
"Adding advisor with name '" + name + "'"
"Not refreshing target: Bean name not specified in 'interceptorNames'."
"Refreshing target with name '" + this.targetName + "'"
"Advice has changed; recaching singleton instance"
"Creating JDK dynamic proxy: " + this.advised.getTargetSource()
"Unable to instantiate proxy using Objenesis, " + "falling back to regular proxy construction",ex
"Creating CGLIB proxy: " + this.advised.getTargetSource()
"Unable to proxy interface-implementing method [" + method + "] because " + "it is marked as final: Consider using interface-based JDK proxies instead!"
"Final method [" + method + "] cannot get proxied via CGLIB: " + "Calls to this method will NOT be routed to the target instance and " + "might lead to NPEs against uninitialized fields in the proxy instance."
"Method [" + method + "] is package-visible across different ClassLoaders " + "and cannot get proxied via CGLIB: Declare this method as public or protected " + "if you need to support invocations through the proxy."
"Found finalize() method - using NO_OVERRIDE"
"Method is declared on Advised interface: " + method
"Found 'equals' method: " + method
"Found 'hashCode' method: " + method
"Must expose proxy on advised method: " + method
"Method has advice and optimizations are enabled: " + method
"Unable to apply any optimizations to advised method: " + method
"Method return type is assignable from target type and " + "may therefore return 'this' - using INVOKE_TARGET: " + method
"Method return type ensures 'this' cannot be returned - " + "using DISPATCH_TARGET: " + method
"Did not attempt to auto-proxy infrastructure class [" + beanClass.getName() + "]"
"TargetSourceCreator [" + tsc + "] found custom TargetSource for bean with name '" + beanName + "'"
"Creating implicit proxy for bean '" + beanName + "' with " + nrOfCommonInterceptors + " common interceptors and " + nrOfSpecificInterceptors + " specific interceptors"
"Skipping currently created advisor '" + name + "'"
"Skipping advisor '" + name + "' with dependency on currently created bean: " + ex.getMessage()
"Configuring AbstractBeanFactoryBasedTargetSource: " + targetSource
"Found exception handler method on throws advice: " + method
"Trying to find handler for exception of type [" + exceptionClass.getName() + "]"
"Found handler for exception of type [" + exceptionClass.getName() + "]: " + handler
"Could not find unique TaskExecutor bean",ex
"More than one TaskExecutor bean found within the context, and none is named " + "'taskExecutor'. Mark one of them as primary or name it 'taskExecutor' (possibly " + "as an alias) in order to use it for async processing: " + ex.getBeanNamesFound()
"Could not find default TaskExecutor bean",ex
"No task executor bean found for async processing: " + "no bean of type TaskExecutor and no bean named 'taskExecutor' either"
"Exception handler for async method '" + method.toGenericString() + "' threw unexpected exception itself",ex2
message,ex
message
"Unexpected exception occurred invoking async method: " + method,ex
"No target for prototype '" + getTargetBeanName() + "' bound to thread: " + "creating one and binding it to thread '" + Thread.currentThread().getName() + "'"
"Destroying ThreadLocalTargetSource bindings"
"Creating Commons object pool"
"Closing Commons ObjectPool"
"Getting bean with name '" + this.targetBeanName + "' for type determination"
"Creating new instance of bean '" + getTargetBeanName() + "'"
"Destroying instance of bean '" + getTargetBeanName() + "'"
"Destroy method on bean with name '" + getTargetBeanName() + "' threw an exception",ex
"Disconnecting TargetSource [" + this + "]"
msg,ex
"Initializing lazy target object"
"Attempting to refresh target"
"Target refreshed successfully"
"Refresh check delay elapsed - checking whether refresh is required"
makeMessageFor(message)
makeMessageFor(message)
makeMessageFor(message)
makeMessageFor(message)
makeMessageFor(message)
"PointcutExpression matching rejected target class - trying fallback expression",ex
"PointcutExpression matching rejected target class",ex
"Could not access current invocation - matching with limited context: " + ex
"Failed to evaluate join point for arguments " + Arrays.asList(args) + " - falling back to non-match",ex
"Failed to create fallback PointcutExpression",ex
"Found AspectJ method: " + candidateAdviceMethod
"Processing pointcut '" + candidateAdviceMethod.getName() + "'"
"SpEL: compiling " + expression.toStringAST()
"SpEL: unable to compile " + expression.toStringAST()
expressionToCompile.getClass().getSimpleName() + ".generateCode opted out of compilation: " + ex.getMessage()
"HandlerInterceptor.afterCompletion threw exception",ex2
"Interceptor [" + interceptorsLBRACKETiRBRACKET + "] failed in afterConcurrentHandlingStarted",ex
"Failed to set bean properties on servlet '" + getServletName() + "'",ex
"Initializing Servlet '" + getServletName() + "'"
"Context initialization failed",ex
"enableLoggingRequestDetails='" + this.enableLoggingRequestDetails + "': request parameters and headers will be " + value
"Completed initialization in " + (System.currentTimeMillis()SUBstartTime) + " ms"
"Unresolved failure from \"" + dispatchType + "\" dispatch: " + failureCause
"Failed to complete request",failureCause
"Failed to complete request: " + failureCause
"Exiting but response remains open for further handling"
"Exiting from \"" + dispatchType + "\" dispatch, status " + status + headers
"Completed " + (httpStatusBANGEQnullQUEShttpStatus:status) + headers
"Detected " + this.multipartResolver
"Detected " + this.multipartResolver.getClass().getSimpleName()
"No MultipartResolver '" + MULTIPART_RESOLVER_BEAN_NAME + "' declared"
"Detected " + this.localeResolver
"Detected " + this.localeResolver.getClass().getSimpleName()
"No LocaleResolver '" + LOCALE_RESOLVER_BEAN_NAME + "': using default [" + this.localeResolver.getClass().getSimpleName() + "]"
"Detected " + this.themeResolver
"Detected " + this.themeResolver.getClass().getSimpleName()
"No ThemeResolver '" + THEME_RESOLVER_BEAN_NAME + "': using default [" + this.themeResolver.getClass().getSimpleName() + "]"
"No HandlerMappings declared for servlet '" + getServletName() + "': using default strategies from DispatcherServlet.properties"
"No HandlerAdapters declared for servlet '" + getServletName() + "': using default strategies from DispatcherServlet.properties"
"No HandlerExceptionResolvers declared in servlet '" + getServletName() + "': using default strategies from DispatcherServlet.properties"
"Detected " + this.viewNameTranslator.getClass().getSimpleName()
"Detected " + this.viewNameTranslator
"No RequestToViewNameTranslator '" + REQUEST_TO_VIEW_NAME_TRANSLATOR_BEAN_NAME + "': using default [" + this.viewNameTranslator.getClass().getSimpleName() + "]"
"No ViewResolvers declared for servlet '" + getServletName() + "': using default strategies from DispatcherServlet.properties"
"Detected " + this.flashMapManager.getClass().getSimpleName()
"Detected " + this.flashMapManager
"No FlashMapManager '" + FLASH_MAP_MANAGER_BEAN_NAME + "': using default [" + this.flashMapManager.getClass().getSimpleName() + "]"
"ModelAndViewDefiningException encountered",exception
"No view rendering, null ModelAndView returned."
"Request already resolved to MultipartHttpServletRequest, e.g. by MultipartFilter"
"Multipart resolution previously failed for current request - " + "skipping re-resolution for undisturbed error rendering"
"Multipart resolution failed for error dispatch",ex
"Using resolved error view: " + exMv,ex
"Using resolved error view: " + exMv
"Rendering view [" + view + "] "
"Error rendering view [" + view + "]",ex
error,ex
error + ": " + ex.getMessage()
"Resource path \"" + resourcePath + "\" was successfully resolved " + "but resource \"" + resource.getURL() + "\" is neither under the " + "current location \"" + location.getURL() + "\" nor under any of the " + "allowed locations " + (allowedLocationsBANGEQnullQUESArrays.asList(allowedLocations):"[]")
"Resolved resource path contains encoded \"../\" or \"..\\\": " + resourcePath
"Skipping " + resource + ": Manifest does not start with 'CACHE MANIFEST'"
"Locations list is empty. No resources will be served unless a " + "custom ResourceResolver is configured as an alternative to PathResourceResolver."
"Resource not found"
"Resource not modified"
"Path with \"WEB-INF\" or \"META-INF\": [" + path + "]"
"Path represents URL or has \"url:\" prefix: [" + path + "]"
"Path contains \"../\" after call to StringUtils#cleanPath: [" + path + "]"
"No resource handling mappings found"
"No match for \"" + lookupPath + "\""
"Unexpected syntax for @import link at index " + index
"Resource resolved from cache"
"No " + coding + " resource for [" + resource.getFilename() + "]",ex
"Found resource for \"" + requestPath + "\", but version [" + candidateVersion + "] does not match"
"ResourceUrlProvider not available via request attribute " + "ResourceUrlProviderExposingInterceptor.RESOURCE_URL_PROVIDER_ATTR"
"No gzip resource for [" + resource.getFilename() + "]",ex
"Resource resolved from cache"
"Path resolved from cache"
"Returning view name '" + viewName + "'"
"Included servlet [" + this.servletName + "] in ServletForwardingController '" + this.beanName + "'"
"Forwarded to servlet [" + this.servletName + "] in ServletForwardingController '" + this.beanName + "'"
"Applying " + cacheControl
"Applying cacheSeconds " + cacheSeconds
"Applying default cacheSeconds"
"ControllerAdvice beans: none"
"ControllerAdvice beans: " + modelSize + " @ModelAttribute, " + binderSize + " @InitBinder, " + reqCount + " RequestBodyAdvice, " + resCount + " ResponseBodyAdvice"
"Async request timed out"
"Skipping URI variable '" + name + "' because request contains bind value with same name."
"ControllerAdvice beans: none"
"ControllerAdvice beans: " + handlerSize + " @ExceptionHandler, " + adviceSize + " ResponseBodyAdvice"
"Using @ExceptionHandler " + exceptionHandlerMethod
"Failure in @ExceptionHandler " + exceptionHandlerMethod,invocationEx
"\n!!!\n" + "Streaming through a reactive type requires an Executor to write to the response.\n" + "Please, configure a TaskExecutor in the MVC config under \"async support\".\n" + "The " + executorTypeName + " currently in use is not suitable under load.\n" + "-------------------------------\n" + "Controller:\t" + returnType.getContainingClass().getName() + "\n" + "Method:\t\t" + returnType.getMethod().getName() + "\n" + "Returning:\t" + ResolvableType.forMethodParameter(returnType).toString() + "\n" + "!!!"
"Connection timeout for " + this.emitter
"Send for " + this.emitter + " failed: " + ex
"Publisher for " + this.emitter + " failed: " + ex
"Publisher for " + this.emitter + " completed"
"Found 'Content-Type:" + contentType + "' in response"
"No match for " + acceptableTypes + ", supported: " + producibleTypes
"Using '" + selectedMediaType + "', given " + acceptableTypes + " and supported " + producibleTypes
"Nothing to write: null body"
"Using first of multiple paths on " + controllerType.getName()
"Using first of multiple paths on " + method.toGenericString()
"Failed to create controller proxy, falling back on default constructor",ex
formatErrorForReturnValue(returnValue),ex
"Failure while trying to resolve exception [" + ex.getClass().getName() + "]",handlerEx
"Async request timed out"
"Failure while trying to resolve exception [" + ex.getClass().getName() + "]",resolveEx
ex.getMessage(),ex
ex.getMessage(),ex
"Found bean named '" + viewName + "' but it does not implement View"
"Exposed request attributes to model: " + exposed
"Exposed session attributes to model: " + exposed
"Rendering [" + getUrl() + "]"
"No ViewResolvers configured"
"Using 406 NOT_ACCEPTABLE" + mediaTypeInfo
"View remains unresolved" + mediaTypeInfo
ex.getMessage()
"Selected '" + mediaType + "' given " + requestedMediaTypes
"View " + formatViewName() + ", model " + (modelBANGEQnullQUESmodel:Collections.emptyMap()) + (this.staticAttributes.isEmpty()QUES"":", static attributes " + this.staticAttributes)
formatKey(cacheKey) + "served from cache"
"Caching is OFF (removal not necessary)"
"Clearing all views from the cache"
"Including [" + getUrl() + "]"
"Forwarding to [" + getUrl() + "]"
"Applying stylesheet [" + url + "]"
"Ignoring invalid locale value [" + newLocale + "]: " + ex.getMessage()
"Ignoring " + cookieDescription
"Parsed cookie value [" + cookie.getValue() + "] into locale '" + locale + "'" + (timeZoneBANGEQnullQUES" and time zone '" + timeZone.getID() + "'":"")
"Found " + result.get(0)
"Resolving to default view '" + this.defaultErrorView + "'"
"Resolving to view '" + viewName + "' based on mapping [" + dominantMapping + "]"
"Applying HTTP status " + statusCode
"Matching patterns " + matchingPatterns
"URI variables " + uriTemplateVariables
"Root mapping to " + getHandlerDescription(handler)
"Default mapping to " + getHandlerDescription(handler)
"Mapped [" + urlPath + "] onto " + getHandlerDescription(handler)
"No patterns in " + formatMappingName()
"Patterns " + patterns + " in " + formatMappingName()
"Mapped to " + handler
"Mapped to " + executionChain.getHandler()
"Detected " + getHandlerMap().size() + " mappings in " + formatMappingName()
"Resolved [" + ex + "]" + (result.isEmpty()QUES"":" to " + result)
"Register \"" + mapping + "\" to " + method.toGenericString()
"Unregister mapping \"" + mapping + "\""
"Could not resolve type for bean '" + beanName + "'",ex
formatMappings(userType,methods)
total + " mappings in " + formatMappingName()
matches.size() + " matching mappings: " + matches
"Resource path \"" + resourcePath + "\" was successfully resolved " + "but resource \"" + resource.getURL() + "\" is neither under the " + "current location \"" + location.getURL() + "\" nor under any of the " + "allowed locations " + (allowedLocationsBANGEQnullQUESArrays.asList(allowedLocations):"[]")
error,ex
error + ": " + ex.getMessage()
"Resolved resource path contains encoded \"../\" or \"..\\\": " + resourcePath
exchange.getLogPrefix() + "Skipping " + resource + ": Manifest does not start with 'CACHE MANIFEST'"
"Locations list is empty. No resources will be served unless a " + "custom ResourceResolver is configured as an alternative to PathResourceResolver."
"Locations list is empty. No resources will be served unless a " + "custom ResourceResolver is configured as an alternative to PathResourceResolver."
exchange.getLogPrefix() + "Resource not found"
exchange.getLogPrefix() + "Resource not modified"
"Path with \"WEB-INF\" or \"META-INF\": [" + path + "]"
"Path represents URL or has \"url:\" prefix: [" + path + "]"
"Path contains \"../\" after call to StringUtils#cleanPath: [" + path + "]"
"No resource handling mappings found"
exchange.getLogPrefix() + "No match for \"" + lookupPath + "\""
"Unexpected syntax for @import link at index " + position
exchange.getLogPrefix() + "Resource resolved from cache"
exchange.getLogPrefix() + "No " + coding + " resource for [" + resource.getFilename() + "]",ex
logPrefix + "Found resource for \"" + requestPath + "\", but version [" + candidate + "] does not match"
logPrefix + "No gzip resource for [" + resource.getFilename() + "]",ex
logPrefix + "Resource resolved from cache"
"Path resolved from cache"
exchange.getLogPrefix() + "Found 'Content-Type:" + contentType + "' in response"
"Using '" + selected + "' given " + acceptableTypes + " and supported " + producibleTypes
exchange.getLogPrefix() + "No match for " + acceptableTypes + ", supported: " + producibleTypes
exchange.getLogPrefix() + "View " + formatViewName() + ", model " + (modelBANGEQnullQUESmodel:Collections.emptyMap())
exchange.getLogPrefix() + "Rendering [" + getUrl() + "]"
exchange.getLogPrefix() + formatArgumentError(parameter,message)
"Register \"" + mapping + "\" to " + method.toGenericString()
"Unregister mapping \"" + mapping
total + " mappings in " + formatMappingName()
"Could not resolve type for bean '" + beanName + "'",ex
formatMappings(userType,methods)
exchange.getLogPrefix() + matches.size() + " matching mappings: " + matches
exchange.getLogPrefix() + "Using @ExceptionHandler " + invocable
exchange.getLogPrefix() + "Failure in @ExceptionHandler " + invocable,invocationEx
logPrefix + (publisherINSTANCEOFMonoQUES"0..1":"0..N") + " [" + elementType + "]"
exchange.getLogPrefix() + "Decoding part '" + part.name() + "'"
"ControllerAdvice beans: none"
"ControllerAdvice beans: " + modelSize + " @ModelAttribute, " + binderSize + " @InitBinder, " + handlerSize + " @ExceptionHandler"
exchange.getLogPrefix() + (contentTypeBANGEQnullQUES"Content-Type:" + contentType:"No Content-Type, using " + MediaType.APPLICATION_OCTET_STREAM)
exchange.getLogPrefix() + "0..N [" + elementType + "]"
exchange.getLogPrefix() + "0..1 [" + elementType + "]"
"Connecting to " + url
"Connecting to " + url
"Connecting to " + url
"Started session '" + session.getId() + "' for " + url
"Connecting to " + url
exchange.getLogPrefix() + reason
getLogPrefix() + "Session id \"" + getId() + "\" for " + getHandshakeInfo().getUri()
getLogPrefix() + "Received " + message
getLogPrefix() + "Sending " + message
getLogPrefix() + "Received " + message
getLogPrefix() + "Sending " + message
logPrefix + "Cancel signal (to close connection)")).map(httpResponseARROWLBRACElogResponse(httpResponse,logPrefix
String.format("%s \"%s\" %s against value \"%s\"",prefix,desired,matchQUES"matches":"does not match",actual)
logPrefix + String.format("Matched %s",this.predicate)
logPrefix + String.format("Matched nested %s",this.predicate)
"Mapped " + routerFunction)
message
message
exchange.getLogPrefix() + "Matching patterns " + matches
"Mapped [" + urlPath + "] onto " + getHandlerDescription(handler)
"No patterns in " + formatMappingName()
"Patterns " + getHandlerMap().keySet() + " in " + formatMappingName()
exchange.getLogPrefix() + "Mapped to " + handler
"Starting HttpServer at address " + address
"Stopping HttpServer"
"Hessian skeleton invocation failed",ex
"Sending HTTP invoker request for service at [" + config.getServiceUrl() + "], with size " + baos.size()
"Class not found during deserialization",ex
"Attempting to resolve property '" + beanName + "' in root WebApplicationContext"
"Successfully resolved property '" + beanName + "' in root WebApplicationContext"
"Attempting to resolve property '" + beanName + "' in root WebApplicationContext"
"Successfully resolved property '" + beanName + "' in root WebApplicationContext"
"Resolving multipart request"
"Not a multipart request"
"Using MultipartResolver '" + beanName + "' for MultipartFilter"
"Could not decode multipart item '" + fileItem.getFieldName() + "' with encoding '" + partEncoding + "': using platform default"
"Failed to perform multipart cleanup for servlet request",ex
"Response " + (statusBANGEQnullQUESstatus:code)
"Accept=" + allSupportedMediaTypes
"Writing [" + body + "] as \"" + mediaType + "\""
"Writing [" + body + "] with " + classname
"Reading to [" + resolvableType + "]"
"Reading to [" + className + "] as \"" + contentType + "\""
"No content-type, using 'application/octet-stream'"
"Async " + method.name() + " request for \"" + url + "\" resulted in " + response.getRawStatusCode() + " (" + response.getStatusText() + ")"
"Async " + method.name() + " request for \"" + url + "\" resulted in " + response.getRawStatusCode() + " (" + response.getStatusText() + "); invoking error handler"
"Cleared thread-bound request context: " + request
"Bound request context to thread: " + request
msg,ex
"Filter '" + filterConfig.getFilterName() + "' configured for use"
message
message
"Added cookie [" + getCookieName() + "=" + cookieValue + "]"
"Removed cookie '" + getCookieName() + "'"
"Could not decode request string [" + source + "] with encoding '" + enc + "': falling back to platform default encoding; exception message: " + ex.getMessage()
"Could not introspect WebSphere web container properties: " + ex
"Skip: response already contains \"Access-Control-Allow-Origin\""
"Skip: request is from same origin"
"Reject: '" + requestOrigin + "' origin is not allowed"
"Reject: HTTP '" + requestMethod + "' is not allowed"
"Reject: headers '" + requestHeaders + "' are not allowed"
"Skip: response already contains \"Access-Control-Allow-Origin\""
"Skip: request is from same origin"
"Reject: '" + requestOrigin + "' origin is not allowed"
"Reject: HTTP '" + requestMethod + "' is not allowed"
"Reject: headers '" + requestHeaders + "' are not allowed"
"Arguments: " + Arrays.toString(args)
formatArgumentError(parameter,error)
"enableLoggingRequestDetails='" + this.enableLoggingRequestDetails + "': form data and headers will be " + value
logPrefix + "500 Server Error for " + formatRequest(request),ex
logPrefix + formatError(ex,exchange.getRequest())
"Root WebApplicationContext: initialization started"
"Root WebApplicationContext initialized in " + elapsedTime + " ms"
"Context initialization failed",ex
"No ContextLoaderListener registered, as " + "createRootApplicationContext() did not return an application context"
"Couldn't invoke destroy method of attribute with name '" + attrName + "'",ex
"Could not register destruction callback [" + callback + "] for attribute '" + name + "' because FacesRequestAttributes does not support such callbacks"
"Ignoring failure in afterCompletion method",ex
"Ignoring failure in postProcess method",ex
"Ignoring failure in afterCompletion method",ex
"Async request timeout for " + formatRequestUri()
"Async request error for " + formatRequestUri() + ": " + ex
"\n!!!\n" + "An Executor is required to handle java.util.concurrent.Callable return values.\n" + "Please, configure a TaskExecutor in the MVC config under \"async support\".\n" + "The " + executorTypeName + " currently in use is not suitable under load.\n" + "-------------------------------\n" + "Request URI: '" + formatRequestUri() + "'\n" + "!!!"
"Async result set but request already complete: " + formatRequestUri()
"Async " + (isErrorQUES"error":"result set") + ", dispatch to " + formatRequestUri()
"Started async request"
"Failed to process async result",ex
"Failed to handle timeout result",ex
"Failed to handle error result",ex
"Replacing existing ServletContext attribute with name '" + attributeName + "'"
"Exported ServletContext attribute with name '" + attributeName + "'"
"Registering annotated classes: [" + StringUtils.collectionToCommaDelimitedString(this.annotatedClasses) + "]"
"Scanning base packages: [" + StringUtils.collectionToCommaDelimitedString(this.basePackages) + "]"
"Registering [" + configLocation + "]"
"Could not load class for config location [" + configLocation + "] - trying package scan. " + ex
"No annotated classes found for specified class/package [" + configLocation + "]"
"Current WebApplicationContext is not available for processing of " + ClassUtils.getShortName(target.getClass()) + ": " + "Make sure this class gets constructed in a Spring web application. Proceeding without injection."
"Searching jar file [" + jarFilePath + "] for entries matching [" + entryPattern + "]"
"Cannot search for matching resources in jar file [" + jarFilePath + "] because the jar cannot be opened through the file system",ex
"Failed to create default value - falling back to null: " + ex.getMessage()
msg,cause
msg,cause
msg + ": " + cause
"For Jackson Kotlin classes support please add " + "\"com.fasterxml.jackson.module:jackson-module-kotlin\" to the classpath"
"Processing of external entities could not be disabled",ex
"Created asynchronous " + method.name() + " request for \"" + url + "\""
"HTTP " + method.name() + " " + url
getLogPrefix() + "Wrote " + written + " of " + total + " bytes"
request.getLogPrefix() + "Failed to complete: " + ex.getMessage())).doOnSuccess(aVoidARROWlogger.trace(request.getLogPrefix() + "Handling completed")
request.getLogPrefix() + "Handling completed")
"Failed to get request URI: " + ex.getMessage()
getLogPrefix() + "Wrote " + written + " of " + remaining + " bytes"
"Found servlet mapping prefix '" + path + "' for '" + name + "'"
"Failed to get request  URL: " + ex.getMessage()
this.logPrefix + "Timeout notification"
this.logPrefix + "Error notification: " + (exBANGEQnullQUESex:"<no Throwable>")
this.logPrefix + "Failed to complete: " + ex.getMessage()
this.logPrefix + "Dispatch to container, to raise the error on servlet thread"
this.logPrefix + "Setting ServletResponse status to 500 Server Error"
this.logPrefix + "Handling completed"
getLogPrefix() + "Could not decode query value [" + value + "] as 'UTF-8'. " + "Falling back on default encoding: " + ex.getMessage()
"Failed to get request URI: " + ex.getMessage()
this.logPrefix + "Failed to complete: " + ex.getMessage()
this.logPrefix + "Closing connection"
this.logPrefix + "Setting HttpServerExchange status to 500 Server Error"
this.logPrefix + "Handling completed"
Hints.getLogPrefix(hints) + "Resource associated with '" + mediaType + "'"
Hints.getLogPrefix(hints) + "Zero-copy " + formatted + "[" + resource + "]"
"Loading FreeMarker configuration from " + this.configLocation
"Template loader path [" + path + "] resolved to file path [" + file.getAbsolutePath() + "]"
"Cannot resolve template loader path [" + templateLoaderPath + "] to [java.io.File]: using SpringTemplateLoader as fallback",ex
"No FreeMarker TemplateLoaders specified"
"SpringTemplateLoader for FreeMarker: using resource loader [" + this.resourceLoader + "] and template loader path [" + this.templateLoaderPath + "]"
"Looking for FreeMarker template with name [" + name + "]"
"Could not find FreeMarker template: " + resource
"Could not obtain last-modified timestamp for FreeMarker template in " + resource + ": " + ex
"Invalidating entire cache '" + cache.getName() + "' for operation " + context.getOperation()
"No bean of type [" + type.getName() + "] found in application context",ex
"Invalidating key [" + key + "] on cache '" + cache.getName() + "' for operation " + context.getOperation()
"Adding cacheable method '" + method.getName() + "' with operation: " + operation
"Computed cache key " + key + " for operation " + context.getOperation()
"Using default EhCache CacheManager for cache region '" + cacheName + "'"
"Using existing EhCache cache region '" + cacheName + "'"
"Creating new EhCache cache region '" + cacheName + "'"
"Initializing EhCache CacheManager" + (this.cacheManagerNameBANGEQnullQUES" '" + this.cacheManagerName + "'":"")
"Shutting down EhCache CacheManager" + (this.cacheManagerNameBANGEQnullQUES" '" + this.cacheManagerName + "'":"")
"Could not cancel CommonJ Timer",ex
"Job registration exception overridden by rollback exception",ex
"Unexpectedly encountered existing trigger on rescheduling, assumably due to " + "cluster race condition: " + ex.getMessage() + " - can safely be ignored"
"Unexpectedly encountered existing trigger on job scheduling, assumably due to " + "cluster race condition: " + ex.getMessage() + " - can safely be ignored"
"Scheduler shutdown exception after registration failure",ex2
"Loading Quartz config from [" + this.configLocation + "]"
"Starting Quartz Scheduler now"
"Will start Quartz Scheduler [" + scheduler.getSchedulerName() + "] in " + startupDelay + " seconds"
"Starting Quartz Scheduler now, after delay of " + startupDelay + " seconds"
"Shutting down Quartz Scheduler"
"Could not load " + resource
"Could not load " + resource
"Task has been rejected by TaskExecutor",ex
String.format("Setting field '%s' of type [%s] on %s or target class [%s] to value [%s]",name,type,safeToString(targetObject),targetClass,value)
String.format("Getting field '%s' from %s or target class [%s]",name,safeToString(targetObject),targetClass)
String.format("Invoking setter method '%s' on %s with value [%s]",setterMethodName,safeToString(target),value)
String.format("Invoking getter method '%s' on %s",getterMethodName,safeToString(target))
String.format("Invoking method '%s' on %s with arguments %s",name,safeToString(target),ObjectUtils.nullSafeToString(args))
"MvcResult details:\n" + stringWriter
String.format("%s %s \"%s\"",message,httpMethod,uri)
"Deleted " + rowCount + " rows from table " + tableName
"Deleted " + rowCount + " rows from table " + tableName
"Dropped table " + tableName
String.format("Instantiating BootstrapContext using constructor [%s]",constructor)
String.format("Instantiating CacheAwareContextLoaderDelegate from class [%s]",clazz.getName())
String.format("Instantiating TestContextBootstrapper for test class [%s] from class [%s]",testClass.getName(),clazz.getName())
"Registering TestExecutionListener: " + listener
"beforeTestClass(): class [" + testClass.getName() + "]"
"prepareTestInstance(): instance [" + testInstance + "]"
"Caught exception while allowing TestExecutionListener [" + testExecutionListener + "] to prepare test instance [" + testInstance + "]",ex
"afterTestClass(): class [" + testClass.getName() + "]"
String.format("%s(): instance [%s], method [%s]",callbackName,testInstance,testMethod)
String.format("%s(): instance [%s], method [%s], exception [%s]",callbackName,testInstance,testMethod,exception)
String.format("Caught exception while invoking '%s' callback on " + "TestExecutionListener [%s] for test class [%s]",callbackName,testExecutionListener,testClass),ex
String.format("Caught exception while invoking '%s' callback on " + "TestExecutionListener [%s] for test method [%s] and test instance [%s]",callbackName,testExecutionListener,testMethod,testInstance),ex
String.format("Failed to invoke copy constructor for [%s]; " + "concurrent test execution is therefore likely not supported.",testContext),ex
String.format("Test class [%s] has been configured with @ContextConfiguration's 'locations' (or 'value') %s " + "and 'classes' %s attributes. Most SmartContextLoader implementations support " + "only one declaration of resources per @ContextConfiguration annotation.",declaringClass.getName(),ObjectUtils.nullSafeToString(locations),ObjectUtils.nullSafeToString(classes))
"SpringJUnit4ClassRunner constructor called with [" + clazz + "]"
msg
"Applying SpringClassRule to test class [" + testClass.getName() + "]"
"Applying SpringMethodRule to test method [" + testMethod + "]"
String.format("Repetition %d of test %s#%s()",(i + 1),this.testMethod.getDeclaringClass().getSimpleName(),this.testMethod.getName())
reason
reason
reason
String.format("@%s(\"%s\") could not be evaluated on [%s] since the test " + "ApplicationContext [%s] is not a ConfigurableApplicationContext",annotationType.getSimpleName(),expression,element,contextType)
String.format("Storing ApplicationContext in cache under key [%s]",mergedContextConfiguration)
String.format("Retrieved ApplicationContext from cache with key [%s]",mergedContextConfiguration)
"Registering annotated classes: " + ObjectUtils.nullSafeToString(annotatedClasses)
msg
String.format("Loading WebApplicationContext for merged context configuration %s.",webMergedConfig)
String.format("Resetting RequestContextHolder for test context %s.",testContext)
String.format("Setting up MockHttpServletRequest, MockHttpServletResponse, ServletWebRequest, and RequestContextHolder for test context %s.",testContext)
msg
String.format("Could not find an 'annotation declaring class' for annotation type [%s] and class [%s]",annotationType.getName(),testClass.getName())
String.format("Retrieved @ActiveProfiles [%s] for declaring class [%s].",annotation,declaringClass.getName())
String.format("@TestExecutionListeners is not present for class [%s]: using defaults.",clazz.getName())
String.format("Retrieved @TestExecutionListeners [%s] for declaring class [%s].",testExecutionListeners,declaringClass.getName())
String.format("Merging default listeners with listeners configured via " + "@TestExecutionListeners for class [%s].",descriptor.getRootDeclaringClass().getName())
"Using TestExecutionListeners: " + listeners
String.format("Skipping candidate TestExecutionListener [%s] due to a missing dependency. " + "Specify custom listener classes or make the default listener classes " + "and their required dependencies available. Offending class: [%s]",listenerClass.getName(),ex.getCause().getMessage())
"Could not load default TestExecutionListener class [" + className + "]. Specify custom listener classes or make the default listener classes available.",ex
String.format("Loaded default TestExecutionListener class names from location [%s]: %s",SpringFactoriesLoader.FACTORIES_RESOURCE_LOCATION,classNames)
String.format("Neither @ContextConfiguration nor @ContextHierarchy found for test class [%s], using %s",testClass.getName(),contextLoader.getClass().getSimpleName())
String.format("Processing locations and classes for context configuration attributes %s",configAttributes)
String.format("Using ContextLoader class [%s] for test class [%s]",contextLoaderClass.getName(),testClass.getName())
String.format("Resolving ContextLoader for context configuration attributes %s",configAttributes)
String.format("Found explicit ContextLoader class [%s] for context configuration attributes %s",contextLoaderClass.getName(),configAttributes)
"Performing dependency injection for test context [" + testContext + "]."
"Reinjecting dependencies for test context [" + testContext + "]."
String.format("Could not find an 'annotation declaring class' for annotation type [%s] and class [%s]",annotationType.getName(),testClass.getName())
String.format("Retrieved @ActiveProfiles [%s] for declaring class [%s]",annotation,declaringClass.getName())
msg
String.format("Detected default resource location \"%s\" for test class [%s]",prefixedResourcePath,clazz.getName())
String.format("Did not detect default resource location for test class [%s]: " + "%s does not exist",clazz.getName(),classPathResource)
String.format("Could not detect default resource locations for test class [%s]: " + "no resource found for suffixes %s.",clazz.getName(),ObjectUtils.nullSafeToString(suffixes))
msg
"Registering annotated classes: " + ObjectUtils.nullSafeToString(annotatedClasses)
msg
String.format("%s detected default locations for context configuration %s.",name(getXmlLoader()),configAttributes)
String.format("%s detected default configuration classes for context configuration %s.",name(getAnnotationConfigLoader()),configAttributes)
msg
String.format("Delegating to %s to process context configuration %s.",name(loader),configAttributes)
String.format("Delegating to %s to load context from %s.",name(loader),mergedConfig)
String.format("Loading ApplicationContext for merged context configuration [%s].",mergedConfig)
String.format("Loading ApplicationContext for locations [%s].",StringUtils.arrayToCommaDelimitedString(locations))
msg
msg
msg
String.format("Retrieved @ContextConfiguration [%s] for declaring class [%s].",contextConfiguration,declaringClass.getName())
"Resolved context configuration attributes: " + attributes
String.format("Retrieved @TestPropertySource [%s] for declaring class [%s].",testPropertySource,rootDeclaringClass.getName())
"Resolved TestPropertySource attributes: " + attributes
String.format("Processing locations for TestPropertySource attributes %s",attrs)
String.format("Processing inlined properties for TestPropertySource attributes %s",attrs)
"Adding inlined properties to environment: " + ObjectUtils.nullSafeToString(inlinedProperties)
String.format("Detected default properties file \"%s\" for test class [%s]",prefixedResourcePath,testClass.getName())
msg
"Processing context initializers for configuration attributes " + configAttributes
String.format("%s test method: context %s, class annotated with @DirtiesContext [%s] " + "with mode [%s], method annotated with @DirtiesContext [%s] with mode [%s].",phase,testContext,classAnnotated,classMode,methodAnnotated,methodMode)
String.format("%s test class: context %s, class annotated with @DirtiesContext [%s] with mode [%s].",phase,testContext,classAnnotated,classMode)
String.format("Ignoring class [%s]; it must be static, non-private, non-final, and annotated " + "with @Configuration to be considered a default configuration class.",candidate.getName())
String.format("Could not detect default configuration classes for test class [%s]: " + "%s does not declare any static, non-private, non-final, nested classes " + "annotated with @Configuration.",declaringClass.getName(),declaringClass.getSimpleName())
msg
String.format("Processing %s for execution phase [%s] and test context %s.",mergedSqlConfig,executionPhase,testContext)
"Executing SQL scripts: " + ObjectUtils.nullSafeToString(scriptResources)
String.format("Detected default SQL script \"%s\" for test %s [%s]",prefixedResourcePath,elementType,elementName)
msg
String.format("Failed to retrieve DataSource named '%s' for test context %s",name,testContext),ex
String.format("Failed to retrieve transaction manager named '%s' for test context %s",name,testContext),ex
String.format("Caught exception while retrieving %s for test context %s",beanType.getSimpleName(),testContext),ex
"Explicit transaction definition [" + transactionAttribute + "] found for test context " + testContext
"Executing @BeforeTransaction method [" + method + "] for test context " + testContext
"Exception encountered while executing @BeforeTransaction methods for test context " + testContext + ".",ex.getTargetException()
"Executing @AfterTransaction method [" + method + "] for test context " + testContext
"Exception encountered while executing @AfterTransaction method [" + method + "] for test context " + testContext,targetException
"Exception encountered while executing @AfterTransaction method [" + method + "] for test context " + testContext,ex
String.format("Caught exception while retrieving transaction manager with qualifier '%s' for test context %s",qualifier,testContext),ex
String.format("Retrieved default @Rollback(%s) for test class [%s].",defaultRollback,testClass.getName())
String.format("Method-level @Rollback(%s) overrides default rollback [%s] for test context %s.",rollbackOverride,rollback,testContext)
String.format("No method-level @Rollback override: using default rollback [%s] for test context %s.",rollback,testContext)
String.format("Began transaction (%s) for test context %s; transaction manager [%s]; rollback [%s]",transactionsStarted,this.testContext,this.transactionManager,this.flaggedForRollback)
String.format("Ending transaction for test context %s; transaction status [%s]; rollback [%s]",this.testContext,this.transactionStatus,this.flaggedForRollback)
(this.flaggedForRollbackQUES"Rolled back":"Committed") + " transaction for test: " + this.testContext
"Retrieved @ProfileValueSourceConfiguration [" + config + "] for test class [" + testClass.getName() + "]"
"Retrieved ProfileValueSource type [" + profileValueSourceType + "] for class [" + testClass.getName() + "]"
"Could not instantiate a ProfileValueSource of type [" + profileValueSourceType + "] for class [" + testClass.getName() + "]: using default.",ex
"Activating simple JNDI environment"
"Deactivating simple JNDI environment"
"Static JNDI binding: [" + name + "] = [" + obj + "]"
"Listing name/class pairs under [" + root + "]"
"Listing bindings under [" + root + "]"
"Static JNDI lookup: [" + name + "]"
"Static JNDI binding: [" + this.root + name + "] = [" + obj + "]"
"Static JNDI remove: [" + this.root + name + "]"
"MockRequestDispatcher: forwarding to [" + this.resource + "]"
"MockRequestDispatcher: including [" + this.resource + "]"
"Could not get resource paths for " + resource,ex
"Could not get URL for " + resource,ex
"Could not open InputStream for " + resource,ex
message
message,ex
message,ex
"Could not determine real path of resource " + resource,ex
"Concurrency count " + this.concurrencyCount + " has reached limit " + this.concurrencyLimit + " - blocking"
"Entering throttle at concurrency count " + this.concurrencyCount
"Returning from throttle at concurrency count " + this.concurrencyCount
"Resolved placeholder '" + placeholder + "'"
this.buffer.toString()
this.buffer.toString()
"XSLT transformation warning",ex
"XSLT transformation error",ex
"Ignored XML validation warning",ex
"Cannot find '.class' file for class [" + clazz + "] - unable to determine constructor/method parameter names"
"Exception thrown while reading '.class' file for class [" + clazz + "] - unable to determine constructor/method parameter names",ex
"ASM ClassReader failed to parse class file [" + clazz + "], probably due to a new Java class file version that isn't supported yet " + "- unable to determine constructor/method parameter names",ex
"Found 'spring.properties' file in local classpath"
"Could not load 'spring.properties' file from local classpath: " + ex
"Could not retrieve system property '" + key + "': " + ex
"Kotlin reflection implementation not found at runtime, related features won't be available."
"Alias definition '" + alias + "' ignored since it points to same name"
"Overriding alias '" + alias + "' definition for registered name '" + registeredName + "' with new target name '" + name + "'"
"Alias definition '" + alias + "' registered for name '" + name + "'"
logMessage
logMessage
"Loaded [" + factoryClass.getName() + "] names: " + factoryNames
"Could not retrieve resources for pattern '" + pattern + "'",ex
"Found Equinox FileLocator for OSGi bundle URL resolution"
"Resolved classpath location [" + location + "] to resources " + result
"Cannot search for matching files underneath [" + url + "] because it cannot be converted to a valid 'jar:' URL: " + ex.getMessage()
"Cannot introspect jar files since ClassLoader [" + classLoader + "] does not support 'getURLs()': " + ex
"Cannot introspect jar files in parent ClassLoader since [" + classLoader + "] does not support 'getParent()': " + ex
"Cannot search for matching files underneath [" + path + "] because it cannot be converted to a valid 'jar:' URL: " + ex.getMessage()
"Failed to evaluate 'java.class.path' manifest entries: " + ex
"Resolved location pattern [" + locationPattern + "] to resources " + result
"Skipping invalid jar classpath entry [" + urlFile + "]"
"Looking for matching resources in jar file [" + jarFileUrl + "]"
"Cannot search for matching files underneath " + rootDirResource + " in the file system: " + ex.getMessage()
"Failed to resolve " + rootDirResource + " in the file system: " + ex
"Looking for matching resources in directory tree [" + rootDir.getPath() + "]"
"Skipping [" + rootDir.getAbsolutePath() + "] because it does not exist"
"Skipping [" + rootDir.getAbsolutePath() + "] because it does not denote a directory"
"Skipping search for matching files underneath directory [" + rootDir.getAbsolutePath() + "] because the application is not allowed to read the directory"
"Searching directory [" + dir.getAbsolutePath() + "] for files matching pattern [" + fullPattern + "]"
"Skipping subdirectory [" + dir.getAbsolutePath() + "] because the application is not allowed to read the directory"
"Could not retrieve contents of directory [" + dir.getAbsolutePath() + "]"
"Loading properties file from " + location
"Properties resource not found: " + ex.getMessage()
"PropertySource '" + getName() + "' does not contain property '" + name + "', but found equivalent '" + actualName + "'"
"Searching for key '" + key + "' in PropertySource '" + propertySource.getName() + "'"
"Could not find key '" + key + "' in any property source"
"Found key '" + key + "' in PropertySource '" + propertySource.getName() + "' with value of type " + value.getClass().getSimpleName()
"Activating profiles " + Arrays.asList(profiles)
"Activating profile '" + profile + "'"
"Caught AccessControlException when accessing system property '" + attributeName + "'; its value will be returned [null]. Reason: " + ex.getMessage()
"Caught AccessControlException when accessing system environment variable '" + attributeName + "'; its value will be returned [null]. Reason: " + ex.getMessage()
"Failed to introspect meta-annotations on " + annotationClass + ": " + ex
"Failed to introspect meta-annotations on " + annotation + ": " + ex
"Failed to classload enum type while reading annotation metadata",ex
"Could not access enum value while reading annotation metadata",ex
"Could not read super class [" + metadata.getSuperClassName() + "] of type-filtered class [" + metadata.getClassName() + "]"
"Could not read interface [" + ifc + "] for type-filtered class [" + metadata.getClassName() + "]"
logPrefix + "Writing " + dataBuffer.readableByteCount() + " bytes"
Hints.getLogPrefix(hints) + "Writing region " + position + "-" + (position + count) + " of [" + resource + "]"
Hints.getLogPrefix(hints) + "Read " + buffer.readableByteCount() + " bytes"
Hints.getLogPrefix(hints) + "Read " + byteCount + " bytes"
logPrefix + "Writing [" + resource + "]"
Hints.getLogPrefix(hints) + "Read " + result.length + " bytes"
Hints.getLogPrefix(hints) + "Read " + bytes.length + " bytes"
logPrefix + "Writing " + buffer.readableByteCount() + " bytes"
logPrefix + "Writing " + dataBuffer.readableByteCount() + " bytes"
"Invalid request message will be discarded: " + requestMessage
"Failed to destroy message listener container",ex
"Ignoring recovery interval in DefaultJmsListenerContainerFactory in favor of BackOff"
"Received message of type [" + message.getClass() + "] from consumer [" + consumerToUse + "] of " + (transactionalQUES"transactional ":"") + "session [" + sessionToUse + "]"
"Rolling back transaction because of listener exception thrown: " + ex
"Consumer [" + consumerToUse + "] of " + (transactionalQUES"transactional ":"") + "session [" + sessionToUse + "] did not receive a message"
"Initiating transaction rollback on listener exception",ex
"Listener exception overridden by rollback exception",ex
"Listener exception overridden by rollback error",ex
"Could not connect on initialization - registering message consumers lazily",ex
"Trying to recover from JMS Connection exception: " + ex
"Successfully refreshed JMS Connection"
"Failed to recover JMS Connection",recoverEx
"Encountered non-recoverable JMSException",ex
"Closing JMS MessageConsumers"
"Closing JMS Sessions"
"Waiting for shutdown of message listener invokers"
"Still waiting for shutdown of " + this.activeInvokerCount + " message listener invokers (iteration " + waitCount + ")"
"Raised scheduled invoker count: " + this.scheduledInvokers.size()
"Could not establish shared JMS Connection - " + "leaving it up to asynchronous invokers to establish a Connection as soon as possible",ex
"Connection start failed - relying on listeners to perform recovery",ex
"Connection stop failed - relying on listeners to perform recovery after restart",ex
"JMS message listener invoker needs to establish shared Connection"
"Setup of JMS message listener invoker failed - already recovered by other invoker",ex
msg,ex
msg
"Successfully refreshed JMS Connection"
msg,ex
msg
msg.toString()
"Lowered scheduled invoker count: " + scheduledInvokers.size()
"All scheduled consumers have been paused, probably due to tasks having been rejected. " + "Check your thread pool configuration! Manual recovery necessary through a start() call."
"Number of scheduled consumers has dropped below concurrentConsumers limit, probably " + "due to tasks having been rejected. Check your thread pool configuration! Automatic recovery " + "to be triggered by remaining consumers."
"Shutting down JMS listener container"
"Could not stop JMS Connection on shutdown",ex
"Established shared JMS Connection"
"Ignoring Connection start exception - assuming already started: " + ex
"Ignoring Connection stop exception - assuming already stopped: " + ex
"Resumed paused task: " + task
"Listener container task [" + task + "] has been rejected and paused: " + ex
"Rejecting received message because of the listener container " + "having been stopped in the meantime: " + message
"Initiating transaction rollback on application exception",ex
"Could not roll back because Session already closed",ex2
"Application exception overridden by rollback error",ex
"Execution of JMS message listener failed, and no ErrorHandler has been set.",ex
"No default <Provider>ActivationSpec class found: " + specClassName
"No default <Provider>ActivationSpecImpl class found: " + specClassName
"No default ActivationSpecImpl class found in provider package: " + specClassName
"No default ActivationSpecImpl class found in inbound subpackage: " + specClassName
"Listener execution failed",ex
"Listener method returned result [" + result + "] - generating response message for it"
"Listener method returned result [" + result + "]: not generating response message for it because of no JMS Session given"
"Processing [" + message + "]"
"No result object given - no result to handle"
"No result object given - no result to handle"
"Failed to set JMSCorrelationID - skipping",ex
"Failed to set JMSReplyTo - skipping",ex
"Failed to set JMSType - skipping",ex
"Skipping reserved header '" + headerName + "' since it cannot be set by client"
"Failed to map message header '" + headerName + "' to JMS property",ex
"Error occurred while mapping from MessageHeaders to JMS properties",ex
"Failed to read JMSCorrelationID property - skipping",ex
"Failed to read JMSDestination property - skipping",ex
"Failed to read JMSDeliveryMode property - skipping",ex
"Failed to read JMSExpiration property - skipping",ex
"Failed to read JMSMessageID property - skipping",ex
"Failed to read JMSPriority property - skipping",ex
"Failed to read JMSReplyTo property - skipping",ex
"Failed to read JMSRedelivered property - skipping",ex
"Failed to read JMSType property - skipping",ex
"Failed to read JMSTimestamp property - skipping",ex
"Error occurred while mapping JMS property '" + propertyName + "' to Message header",ex
"Error occurred while mapping from JMS properties to MessageHeaders",ex
"Ignoring Connection state exception - assuming already closed: " + ex
"Could not close JMS Connection",ex
"Could not close JMS Session",ex
"Could not close JMS MessageProducer",ex
"Could not close JMS MessageConsumer",ex
"Could not close JMS QueueBrowser",ex
"Could not close JMS QueueRequestor",ex
"Destination [" + destinationName + "] not found in JNDI",ex
"No @JmsListener annotations found on bean type: " + targetClass
annotatedMethods.size() + " @JmsListener methods processed on bean '" + beanName + "': " + annotatedMethods
"No working getDataSource method found on ConnectionFactory: " + ex2
"Could not close synchronized JMS Session after transaction",ex
"Established shared JMS Connection: " + this.connection
"Encountered a JMSException - resetting the underlying JMS Connection",ex
"Closing shared JMS Connection: " + con
"Ignoring Connection state exception - assuming already closed: " + ex
"Could not close shared JMS Connection",ex
"Failed to close newly obtained JMS Session",ex
"Could not close cached JMS Session",ex
"Found cached JMS Session for mode " + mode + ": " + (sessionINSTANCEOFSessionProxyQUES((SessionProxy)session).getTargetSession():session)
"Registering cached JMS Session for mode " + mode + ": " + targetSession
"Logical close of cached JMS Session failed - discarding it",ex
"Found cached JMS MessageProducer for destination [" + dest + "]: " + producer
"Registering cached JMS MessageProducer for destination [" + dest + "]: " + producer
"Found cached JMS MessageConsumer for destination [" + dest + "]: " + consumer
"Registering cached JMS MessageConsumer for destination [" + dest + "]: " + consumer
"Returned cached Session: " + this.target
"Closing cached Session: " + this.target
"Created JMS transaction on Session [" + session + "] from Connection [" + con + "]"
"Committing JMS transaction on Session [" + session + "]"
"Rolling back JMS transaction on Session [" + session + "]"
"Could not stop JMS Connection before closing it",ex
"Could not close JMS Connection",ex
"Executing callback on JMS Session: " + sessionToUse
"Sending created message: " + message
"Sending created message: " + requestMessage
"Executing callback on JMS Session: " + session
"Creating JAXBContext with context path [" + this.contextPath + "]"
"Creating JAXBContext with classes to be bound [" + StringUtils.arrayToCommaDelimitedString(classesToBeBound) + "]"
"Creating JAXBContext by scanning packages [" + StringUtils.arrayToCommaDelimitedString(packagesToScan) + "]"
"Found JAXB2 classes: [" + StringUtils.arrayToCommaDelimitedString(jaxb2Classes) + "]"
"Setting validation schema to " + StringUtils.arrayToCommaDelimitedString(this.schemaResources)
"Processing of external entities could not be disabled",ex
"Could not flush HierarchicalStreamWriter",ex
"Configured for target class [" + this.targetClass + "] using binding [" + this.bindingName + "]"
"Configured for target class [" + this.targetClass + "]"
"Configured for target package [" + this.targetPackage + "] using binding [" + this.bindingName + "]"
String.valueOf(message)
String.valueOf(message),exception
String.valueOf(message)
String.valueOf(message),exception
String.valueOf(message)
String.valueOf(message),exception
String.valueOf(message)
String.valueOf(message),exception
String.valueOf(message)
String.valueOf(message),exception
"Loading struts2 Guice support..."
"Injector created successfully."
"Creating injector..."
"Installing " + module + "..."
"No module found. Set 'guice.module' to a Module " + "class name if you'd like to use one."
"Injector created successfully."
loggers.get(""
sm.getString("response.encoding.invalid",charsetValue),e
sm.getString("response.notNonBlocking")
"Received [" + NEWString(byteBuffer.array(),byteBuffer.position(),byteBuffer.remaining(),StandardCharsets.ISO_8859_1) + "]"
sm.getString("iib.available.readFail"),ioe
message
sm.getString("http11processor.request.prepare") + " Unsupported transfer encoding [" + encodingName + "]"
sm.getString("http11processor.header.parse"),e
message,t
message,t
sm.getString("http11processor.request.prepare"),t
sm.getString("http11processor.request.process"),e
sm.getString("http11processor.request.process"),t
sm.getString("http11processor.request.prepare") + " Unsupported HTTP version \"" + protocolMB + "\""
sm.getString(errorKey)
sm.getString("http11processor.request.finish"),t
sm.getString("http11processor.response.finish"),t
sm.getString("http11processor.socket.ssl"),ioe
sm.getString("http11processor.sendfile.error")
"Flushing the compression stream!"
"Ignored exception while flushing gzip filter",e
sm.getString("upgrade.sos.onErrorFail"),t2
sm.getString("upgrade.sos.errorCloseFail"),ioe
sm.getString("upgrade.sis.onErrorFail"),t2
sm.getString("upgrade.sis.errorCloseFail"),ioe
sm.getString("upgradeProcessor.stop")
sm.getString("upgradeProcessor.isCloseFail",ioe)
sm.getString("upgradeProcessor.osCloseFail",ioe)
sm.getString("upgradeProcessor.unexpectedState")
sm.getString("upgradeProcessor.requiredClose",Boolean.valueOf(upgradeServletInputStream.isClosed()),Boolean.valueOf(upgradeServletOutputStream.isClosed()))
sm.getString("ajpmessage.null"),NEWNullPointerException()
sm.getString("ajpmessage.null"),NEWNullPointerException()
sm.getString("ajpmessage.overflow","" + numBytes,"" + pos),NEWArrayIndexOutOfBoundsException()
sm.getString("ajpmessage.invalid","" + mark)
"Received " + len + " " + bufLBRACKET0RBRACKET
prefix + ": " + HexUtils.toHexString(buf) + " " + pos + "/" + (len + 4)
hexLine(buf,j,len)
msg
sm.getString("abstractStream.windowSizeInc",getConnectionId(),getIdentifier(),Integer.toString(increment),Long.toString(windowSize))
sm.getString("abstractStream.windowSizeDec",getConnectionId(),getIdentifier(),Integer.toString(decrement),Long.toString(windowSize))
msg
sm.getString("upgradeHandler.rst.debug",connectionId,Integer.toString(se.getStreamId()),se.getError(),se.getMessage())
sm.getString("upgradeHandler.writeBody",connectionId,stream.getIdentifier(),Integer.toString(len))
sm.getString("stream.reprioritisation.debug",getConnectionId(),getIdentifier(),Boolean.toString(exclusive),parent.getIdentifier(),Integer.toString(weight))
sm.getString("stream.reprioritisation.debug",getConnectionId(),getIdentifier(),Boolean.FALSE,parent.getIdentifier(),Integer.toString(weight))
sm.getString("stream.reset.receive",getConnectionId(),getIdentifier(),Long.toString(errorCode))
sm.getString("stream.header.debug",getConnectionId(),getIdentifier(),name,value)
sm.getString("stream.reset.send",getConnectionId(),getIdentifier(),se.getError())
sm.getString("stream.outputBuffer.flush.debug",getConnectionId(),getIdentifier(),Integer.toString(buffer.position()),Boolean.toString(writeInProgress),Boolean.toString(closed))
sm.getString("stream.inputBuffer.empty")
sm.getString("stream.inputBuffer.copy",Integer.toString(written))
sm.getString("stream.inputBuffer.dispatch")
sm.getString("stream.inputBuffer.signal")
sm.getString("hpackEncoder.encodeHeader",headerName,val)
msg,e
sm.getString("streamProcessor.service.error"),e
sm.getString("streamProcessor.flushBufferedWrite.entry",stream.getConnectionId(),stream.getIdentifier())
sm.getString("connectionSettings.debug",connectionId,getEndpointName(),setting,Long.toString(value))
sm.getString("http2Parser.error",connectionId,Integer.valueOf(streamId),frameType),e
sm.getString("windowAllocationManager.waitFor.stream",stream.getConnectionId(),stream.getIdentifier(),Long.toString(timeout))
sm.getString("windowAllocationManager.waitFor.connection",stream.getConnectionId(),stream.getIdentifier(),Long.toString(timeout))
sm.getString("windowAllocationManager.waitForNonBlocking.stream",stream.getConnectionId(),stream.getIdentifier())
sm.getString("windowAllocationManager.waitForNonBlocking.connection",stream.getConnectionId(),stream.getIdentifier())
sm.getString("windowAllocationManager.notify",stream.getConnectionId(),stream.getIdentifier(),Integer.toString(waitingFor),Integer.toString(notifyTarget))
sm.getString("windowAllocationManager.notified",stream.getConnectionId(),stream.getIdentifier())
sm.getString("windowAllocationManager.dispatched",stream.getConnectionId(),stream.getIdentifier())
sm.getString("streamStateMachine.debug.change",stream.getConnectionId(),stream.getIdentifier(),oldState,newState)
sm.getString("http2Parser.processFrameData.lengths",connectionId,Integer.toString(streamId),Integer.toString(dataLength),padding)
sm.getString("http2Parser.processFrameWindowUpdate.debug",connectionId,Integer.toString(streamId),Integer.toString(windowSizeIncrement))
sm.getString("http2Parser.processFrameHeaders.payload",connectionId,Integer.valueOf(streamId),Integer.valueOf(payloadSize))
sm.getString("http2Parser.swallow.debug",connectionId,Integer.toString(streamId),Integer.toString(len))
sm.getString("http2Parser.processFrame",connectionId,Integer.toString(streamId),frameType,Integer.toString(flags),Integer.toString(payloadSize))
sm.getString("upgradeHandler.upgrade",connectionId)
sm.getString("upgradeHandler.init",connectionId,connectionState.get())
msg,e
sm.getString("upgradeHandler.prefaceReceived",connectionId)
sm.getString("upgradeHandler.upgradeDispatch.entry",connectionId,status)
sm.getString("upgradeHandler.connectionError"),ce
sm.getString("upgradeHandler.ioerror",connectionId),ioe
sm.getString("upgradeHandler.upgradeDispatch.exit",connectionId,result)
sm.getString("upgradeHandler.pause.entry",connectionId)
sm.getString("upgradeHandler.rst.debug",connectionId,Integer.toString(se.getStreamId()),se.getError(),se.getMessage())
msg
sm.getString("upgradeHandler.writeHeaders",connectionId,stream.getIdentifier())
sm.getString("upgradeHandler.writePushHeaders",connectionId,stream.getIdentifier(),Integer.valueOf(pushedStreamId),Boolean.valueOf(endOfStream))
headerFrameBuffers.getPayload().limit() + " bytes"
sm.getString("upgradeHandler.writeBody",connectionId,stream.getIdentifier(),Integer.toString(len))
sm.getString("upgradeHandler.noAllocation",connectionId,stream.getIdentifier())
sm.getString("upgradeHandler.releaseBacklog",connectionId,stream.getIdentifier())
sm.getString("upgradeHandler.allocate.debug",getConnectionId(),stream.getIdentifier(),Integer.toString(allocation))
sm.getString("upgradeHandler.allocate.left",getConnectionId(),stream.getIdentifier(),Integer.toString(leftToAllocate))
sm.getString("upgradeHandler.allocate.recipient",getConnectionId(),stream.getIdentifier(),recipient.getIdentifier(),Integer.toString(recipient.getWeight()))
sm.getString("upgradeHandler.socketCloseFailed"),e
sm.getString("upgradeHandler.pruneStart",connectionId,Long.toString(max),Integer.toString(streams.size()))
sm.getString("upgradeHandler.pruned",connectionId,streamIdToRemove)
sm.getString("upgradeHandler.pruned",connectionId,streamIdToRemove)
sm.getString("upgradeHandler.pruned",connectionId,streamIdToRemove)
sm.getString("upgradeHandler.prunedPriority",connectionId,streamIdToRemove)
sm.getString("upgradeHandler.pruneIncomplete",connectionId,Integer.toString(streamId),Integer.toString(toClose))
sm.getString("upgradeHandler.noNewStreams",connectionId,Integer.toString(streamId))
sm.getString("upgradeHandler.goaway.debug",connectionId,Integer.toString(lastStreamId),Long.toHexString(errorCode),debugData)
sm.getString("pingManager.roundTripTime",connectionId,Long.valueOf(roundTripTime))
"Generating web mapping for file " + file + " using compilation context " + clctxt
Localizer.getMessage("jspc.delete.fail",webXml2.toString())
Localizer.getMessage("jspc.delete.fail",webxmlFile)
"Processing file: " + file
jspUri + " is out dated, compiling..."
"Built File: " + file
Localizer.getMessage("jspc.error.generalException",file),rootCause
Localizer.getMessage("jspc.error.fileDoesNotExist",e.getMessage())
"execute() starting for " + pages.size() + " pages."
Localizer.getMessage("jspc.error.fileDoesNotExist",fjsp.toString())
Localizer.getMessage("jspc.error.compilation"),e
msg
Localizer.getMessage("jspc.warning.tldInWebInfLib")
Localizer.getMessage("jspc.implicit.uriRoot",uriRoot)
Localizer.getMessage("jsp.error.compiler"),e
Localizer.getMessage("jsp.error.compiler"),e
Localizer.getMessage("jsp.error.lastModified",getJspFile()),e
Localizer.getMessage("jsp.error.lastModified",getJspFile()),e
Localizer.getMessage("jsp.warning.keepgen")
Localizer.getMessage("jsp.warning.trimspaces"),iae
Localizer.getMessage("jsp.warning.enablePooling")
Localizer.getMessage("jsp.warning.mappedFile")
Localizer.getMessage("jsp.warning.classDebugInfo")
Localizer.getMessage("jsp.warning.checkInterval")
Localizer.getMessage("jsp.warning.modificationTestInterval")
Localizer.getMessage("jsp.warning.recompileOnFail")
Localizer.getMessage("jsp.warning.development")
Localizer.getMessage("jsp.warning.suppressSmap")
Localizer.getMessage("jsp.warning.dumpSmap")
Localizer.getMessage("jsp.warning.genchararray")
Localizer.getMessage("jsp.warning.errBean")
Localizer.getMessage("jsp.info.ignoreSetting","scratchdir",dir)
Localizer.getMessage("jsp.error.no.scratch.dir")
Localizer.getMessage("jsp.error.bad.scratch.dir",scratchDir.getAbsolutePath())
Localizer.getMessage("jsp.warning.fork")
Localizer.getMessage("jsp.warning.xpoweredBy")
Localizer.getMessage("jsp.warning.displaySourceFragment")
Localizer.getMessage("jsp.warning.maxLoadedJsps","" + this.maxLoadedJsps)
Localizer.getMessage("jsp.warning.jspIdleTimeout","" + this.jspIdleTimeout)
Localizer.getMessage("jsp.warning.strictQuoteEscaping")
Localizer.getMessage("jsp.warning.quoteAttributeEL")
Localizer.getMessage("jsp.warning.tagRelease",tag.getClass().getName()),t
Localizer.getMessage("jsp.warning.tagPreDestroy",tag.getClass().getName()),t
Localizer.getMessage("jsp.error.securityPreload"),ex
Localizer.getMessage("jsp.error.servlet.destroy.failed"),t
Localizer.getMessage("jsp.info.ignoreSetting","engineOptionsClass",engineOptionsName)
Localizer.getMessage("jsp.message.scratch.dir.is",options.getScratchDir().toString())
Localizer.getMessage("jsp.message.dont.modify.servlets")
"JspEngine --> " + jspUri
"\t     ServletPath: " + request.getServletPath()
"\t        PathInfo: " + request.getPathInfo()
"\t        RealPath: " + context.getRealPath(jspUri)
"\t      RequestURI: " + request.getRequestURI()
"\t     QueryString: " + request.getQueryString()
"JspServlet.destroy()"
Localizer.getMessage("jsp.error.file.not.found",jspUri)
Localizer.getMessage(MSG + ".onStartup",context.getServletContextName())
Localizer.getMessage(MSG + ".webxmlSkip",resourcePath,taglibURI)
Localizer.getMessage(MSG + ".webxmlAdd",resourcePath,taglibURI)
Localizer.getMessage(MSG + ".webxmlFailPathDoesNotExist",resourcePath,taglibURI)
Localizer.getMessage("jsp.tldCache.tldInResourcePath",startPath)
Localizer.getMessage("jsp.tldCache.noTldInResourcePath",startPath)
Localizer.getMessage("jsp.tldCache.noTldSummary")
Localizer.getMessage("jsp.tldCache.tldInJar",jarFileUrl.toString())
Localizer.getMessage("jsp.tldCache.noTldInJar",jarFileUrl.toString())
Localizer.getMessage("jsp.tldCache.tldInDir",file.getAbsolutePath())
Localizer.getMessage("jsp.tldCache.noTldInDir",file.getAbsolutePath())
"Generated " + javaFileName + " total=" + (t4SUBt1) + " generate=" + (t4SUBt3) + " validate=" + (t2SUBt1)
Localizer.getMessage("jsp.warning.compiler.javafile.delete.fail",file.getAbsolutePath())
"Compiler: outdated: " + targetFile + " " + targetLastModified
"Problem accessing resource. Treat as outdated.",e
"Deleting " + javaFile
Localizer.getMessage("jsp.warning.compiler.javafile.delete.fail",javaFile.getAbsolutePath())
"Deleting " + classFile
Localizer.getMessage("jsp.warning.compiler.classfile.delete.fail",classFile.getAbsolutePath())
"Compiler " + options.getCompiler()
"Using classpath: " + System.getProperty("java.class.path") + File.pathSeparator + classpath
Localizer.getMessage("jsp.error.javac"),e
Localizer.getMessage("jsp.error.javac.env") + info.toString()
Localizer.getMessage("jsp.error.compilation",javaFileName,errorReportString)
"Compiled " + ctxt.getServletJavaFileName() + " " + (t2SUBt1) + "ms"
Localizer.getMessage("jsp.error.compilation.source",sourceFile),e
Localizer.getMessage("jsp.error.compilation.dependent",className),exc
Localizer.getMessage("jsp.error.compilation.dependent",className),exc
Localizer.getMessage("jsp.warning.unknown.sourceVM",opt)
Localizer.getMessage("jsp.warning.unknown.targetVM",opt)
Localizer.getMessage("jsp.error.compilation.jdtProblemError"),e
Localizer.getMessage("jsp.error.compilation.jdt"),exc
Localizer.getMessage("jsp.warning.unsupported.sourceVM",requestedSource,actualSource)
Localizer.getMessage("jsp.warning.unsupported.targetVM",requestedTarget,actualTarget)
"Compiled " + ctxt.getServletJavaFileName() + " " + (t2SUBt1) + "ms"
Localizer.getMessage("jsp.warning.bad.urlpattern.propertygroup",urlPattern)
"constant pool count: " + constantPoolCount
"SourceDebugExtension not found, installed at: " + sdeIndex
"SourceDebugExtension found at: " + sdeIndex
"interfaceCount: " + interfaceCount
"class attrCount: " + attrCount
"class attrCount incremented"
"members count: " + count
"member attr count: " + attrCount
"SDE attr found"
"attr len: " + len
i + " copying 2 bytes"
i + " copying 3 bytes"
i + " copying 4 bytes"
i + " copying 8 bytes"
i + " read class attr -- '" + str + "'"
Localizer.getMessage("jsp.warning.loadSmap",className),ioe
Localizer.getMessage("jsp.warning.loadSmap",className),ioe
Localizer.getMessage("jsp.error.file.cannot.read",fname),ex
"Exception closing reader: ",any
Localizer.getMessage("jsp.message.parent_class_loader_is",loader.toString())
Localizer.getMessage("jsp.message.parent_class_loader_is","<none>")
Localizer.getMessage("jsp.message.jsp_queue_created","" + options.getMaxLoadedJsps(),context.getContextPath())
Localizer.getMessage("jsp.message.jsp_added",jsw.getJspUri(),context.getContextPath())
Localizer.getMessage("jsp.message.jsp_removed_excess",replaced.getJspUri(),context.getContextPath())
Localizer.getMessage("jsp.message.jsp_queue_update",jsw.getJspUri(),context.getContextPath())
Localizer.getMessage("jsp.warning.classpathUrl"),e
"Compilation classpath initialized: " + path
Localizer.getMessage("jsp.message.jsp_unload_check",context.getContextPath(),"" + jsps.size(),"" + queueLength)
Localizer.getMessage("jsp.message.jsp_removed_idle",jsw.getJspUri(),context.getContextPath(),"" + (nowSUBjsw.getLastUsageTime()))
sm.getString("selectorContext.methodUsingName","lookup",name)
sm.getString("selectorContext.methodUsingString","lookup",name)
sm.getString("selectorContext.methodUsingName","list",name)
sm.getString("selectorContext.methodUsingString","list",name)
sm.getString("selectorContext.methodUsingName","listBindings",name)
sm.getString("selectorContext.methodUsingString","listBindings",name)
sm.getString("selectorContext.methodUsingName","lookupLink",name)
sm.getString("selectorContext.methodUsingString","lookupLink",name)
msg,e
msg,ne
msg,ne
sm.getString("wsWebSocketContainer.invalidHeader",line)
sm.getString("wsWebSocketContainer.sessionCloseFail",session.getId()),ioe
sm.getString("wsSession.created",id)
sm.getString("wsSession.doClose",id)
sm.getString("wsSession.flushFailOnClose"),e
sm.getString("wsSession.flushFailOnClose"),e
sm.getString("wsSession.sendCloseFail",id),e
msg
sm.getString("backgroundProcessManager.processFailed"),t
sm.getString("asyncChannelWrapperSecure.closeFail")
sm.getString("wsFrameClient.ioe"),e
sm.getString("pojoEndpointBase.closeSessionFail"),ioe
sm.getString("pojoEndpointBase.onCloseFail",pojo.getClass().getName()),t
sm.getString("pojoEndpointBase.onError",pojo.getClass().getName()),throwable
sm.getString("pojoEndpointBase.onErrorFail",pojo.getClass().getName()),t
sm.getString("wsRemoteEndpointServer.closeFailed"),e
"wsFrameServer.onDataAvailable"
sm.getString("wsFrameServer.bytesRead",Integer.toString(read))
sm.getString("wsHttpUpgradeHandler.destroyFailed"),e
"Failed to complete JMX registration for " + objectName,e
"Failed to complete JMX unregistration for " + objectName,e
"The requested JMX name [" + requestedName + "] was not valid and will be ignored."
"The requested JMX name [" + requestedName + "] was not valid and will be ignored."
warning
infoMessage
Utils.getMessage("poolableConnectionFactory.validateObject.fail"),e
Utils.getMessage("swallowedExceptionLogger.onSwallowedException"),e
Utils.getMessage("poolingDataSource.factoryConfig")
"IntrospectionUtils: setProperty(" + o.getClass() + " " + name + "=" + value + ")"
"IntrospectionUtils: Unable to resolve host name:" + value
"IntrospectionUtils: Unknown type " + paramType.getName()
sm.getString("introspectionUtils.setPropertyError",name,value,o.getClass()),e
sm.getString("introspectionUtils.setPropertyError",name,value,o.getClass()),e
sm.getString("introspectionUtils.getPropertyError",name,o.getClass()),e
sm.getString("introspectionUtils.getPropertyError",name,o.getClass()),e
"IntrospectionUtils: callMethod1 " + target.getClass().getName() + " " + param1.getClass().getName() + " " + typeParam1
"IntrospectionUtils: Can't find method " + methodN + " in " + target + " CLASS " + target.getClass()
"IntrospectionUtils:" + sb.toString()
"IntrospectionUtils: Unable to resolve host name:" + object
"IntrospectionUtils: Unknown type " + paramType.getName()
"Could not set threadContentionMonitoringEnabled to " + enable + ", got " + checkValue + " instead"
"Could not set threadCpuTimeEnabled to " + enable + ", got " + checkValue + " instead"
"Could not set verbose class loading to " + verbose + ", got " + checkValue + " instead"
"Could not set logger level for logger '" + loggerName + "' to '" + levelName + "', got '" + checkValue + "' instead"
"Could not set verbose garbage collection logging to " + verbose + ", got " + checkValue + " instead"
sm.getString("xmlErrorHandler.warning",e.getMessage(),source)
sm.getString("xmlErrorHandler.error",e.getMessage(),source)
sm.getString("digesterFactory.missingSchema",name)
sm.getString("webXmlParser.applicationStart",source.getSystemId())
sm.getString("webXmlParser.applicationParse",source.getSystemId()),e
sm.getString("webXmlParser.applicationPosition","" + e.getLineNumber(),"" + e.getColumnNumber())
sm.getString("webXmlParser.applicationParse",source.getSystemId()),e
sm.getString("webXml.version.unknown",version)
sm.getString("webXml.unrecognisedPublicId",publicId)
sm.getString("webXml.mergeConflictDisplayName",fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictFilter",entry.getKey(),fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictLoginConfig",fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictServlet",entry.getKey(),fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictSessionTimeout",fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictSessionCookieName",fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictSessionCookieDomain",fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictSessionCookiePath",fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictSessionCookieComment",fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictSessionCookieHttpOnly",fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictSessionCookieSecure",fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictSessionCookieMaxAge",fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictSessionTrackingMode",fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictResource",resourceName,fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictString",mapName,key,fragment.getName(),fragment.getURL())
sm.getString("webXml.mergeConflictString",mapName,key,fragment.getName(),fragment.getURL())
sm.getString("webXml.wrongFragmentName",requestedName)
sm.getString("securityConstraint.uncoveredHttpMethodFix",pattern,msg.toString().trim())
sm.getString("securityConstraint.uncoveredHttpMethod",pattern,msg.toString().trim())
sm.getString("securityConstraint.uncoveredHttpOmittedMethodFix",pattern,msg.toString().trim())
sm.getString("securityConstraint.uncoveredHttpOmittedMethod",pattern,msg.toString().trim())
sm.getString("jarScan.webinflibStart")
sm.getString("jarScan.webinflibJarScan",path)
sm.getString("jarScan.webinflibFail",url),e
sm.getString("jarScan.webinflibJarNoScan",path)
sm.getString("jarScan.webinfclassesFail"),e
sm.getString("jarScan.classloaderStart")
sm.getString("jarScan.classloaderJarScan",url)
sm.getString("jarScan.classloaderFail",url),ioe
sm.getString("jarScan.classloaderJarNoScan",url)
sm.getString("jarScan.classPath.badEntry",classPathEntry),e
sm.getString("jarScan.jarUrlStart",url)
sm.getString("jarScan.invalidUri",jarURL),e
sb.toString()
"[SetNextRule]{" + digester.match + "} Call [NULL PARENT]." + methodName + "(" + child + ")"
"[SetNextRule]{" + digester.match + "} Call " + parent.getClass().getName() + "." + methodName + "(" + child + ")"
sm.getString("digester.failedToUpdateSystemProperty",name,value),e
sm.getString("digester.createParserError"),e
sm.getString("digester.error.finish"),e
sm.getString("digester.error.finish"),e
"  match='" + match + "'"
"  bodyText='" + bodyText + "'"
"  Fire body() for " + rule
sm.getString("digester.error.body"),e
sm.getString("digester.error.body"),e
sm.getString("digester.noRulesFound",match)
sm.getString("digester.noRulesFound",match)
"  Fire end() for " + rule
sm.getString("digester.error.end"),e
sm.getString("digester.error.end"),e
sm.getString("digester.encodingInvalid",enc),e
"  New match='" + match + "'"
"  Fire begin() for " + rule
sm.getString("digester.error.begin"),e
sm.getString("digester.error.begin"),e
sm.getString("digester.noRulesFound",match)
" Cannot resolve entity: '" + publicId + "'"
" Trying to resolve using system ID '" + systemId + "'"
"Invalid URI '" + baseURI + "' or '" + systemId + "'"
" Resolving to alternate DTD '" + entityURL + "'"
sm.getString("digester.parseError",Integer.valueOf(exception.getLineNumber()),Integer.valueOf(exception.getColumnNumber())),exception
sm.getString("digester.parseErrorFatal",Integer.valueOf(exception.getLineNumber()),Integer.valueOf(exception.getColumnNumber())),exception
sm.getString("digester.parseWarning",Integer.valueOf(exception.getLineNumber()),Integer.valueOf(exception.getColumnNumber()),exception)
"register('" + publicId + "', '" + entityURL + "'"
sm.getString("digester.emptyStack")
sm.getString("digester.emptyStack")
sm.getString("digester.emptyStack")
sm.getString("digester.emptyStack")
"Popping params"
sm.getString("digester.emptyStack")
"Pushing params"
sm.getString("digester.noLocator")
sm.getString("digester.failedToUpdateAttributes",newAttrs.getLocalName(i),value),e
"[ObjectCreateRule]{" + digester.match + "} Pop " + top.getClass().getName()
"[SetPropertiesRule]{" + digester.match + "} Set " + top.getClass().getName() + " properties"
"[SetPropertiesRule]{" + digester.match + "} Set NULL properties"
"[SetPropertiesRule]{" + digester.match + "} Setting property '" + name + "' to '" + value + "'"
sm.getString("rule.noProperty",digester.match,name,value)
"[CallMethodRule](" + i + ")" + parametersLBRACKETiRBRACKET
sb.toString()
"[FactoryCreateRule]{" + digester.match + "} New " + instance.getClass().getName()
sm.getString("rule.createError",((e.getMessage()EQEQnull)QUESe.getClass().getName():e.getMessage()))
"[FactoryCreateRule] Ignored exception:",e
"[FactoryCreateRule]{" + digester.match + "} New " + instance.getClass().getName()
"[FactoryCreateRule] No creation so no push so no pop"
"[FactoryCreateRule]{" + digester.match + "} Pop " + top.getClass().getName()
sm.getString("jsse.ssl3")
sm.getString("jsse.tls13.auth")
sm.getString("sslUtilBase.active",name,enabled)
msg
msg
msg,ex
sm.getString("jsseUtil.noVerificationDepth",algorithm)
msg,e
msg
sm.getString("jsseUtil.trustedCertNotChecked",alias)
"Socket: [" + this + "], Read from buffer: [" + remaining + "]"
"Socket: [" + this + "], Read from buffer: [" + nRead + "]"
sm.getString("endpoint.executor.fail",SocketWrapperBase.this),ree
sm.getString("endpoint.apr.maxConnections.unlimited",Integer.valueOf(getMaxConnections()))
sm.getString("endpoint.apr.maxConnections.running",Integer.valueOf(getMaxConnections()))
sm.getString("endpoint.apr.noSendfileWithSSL")
sm.getString("endpoint.warn.unlockAcceptorFailed",acceptor.getThreadName())
sm.getString("endpoint.err.handshake") + ": " + SSL.getLastError()
sm.getString("endpoint.alpn.negotiated",negotiatedProtocol)
sm.getString("endpoint.err.handshake"),t
sm.getString("endpoint.err.unexpected"),t
sm.getString("endpoint.poll.limitedpollsize","" + size)
sm.getString("endpoint.poll.initfail"),e
sm.getString("endpoint.debug.socket",socket)
sm.getString("endpoint.rejectedExecution",socket),x
sm.getString("endpoint.apr.remoteport",Long.valueOf(socket),Long.valueOf(addr.port))
msg,NEWException()
msg
sm.getString("endpoint.pollerThreadStop")
msg,NEWException()
msg
sm.getString("endpoint.debug.pollerRemove",Long.valueOf(socket))
sm.getString("endpoint.debug.pollerRemoved",Long.valueOf(socket))
sm.getString("endpoint.debug.socketTimeout",Long.valueOf(socket))
sm.getString("endpoint.debug.pollerAddDo",Long.valueOf(info.socket))
sm.getString("endpoint.debug.pollerProcess",Long.valueOf(descLBRACKETnSTAR2 + 1RBRACKET),Long.valueOf(descLBRACKETnSTAR2RBRACKET))
sm.getString("endpoint.apr.pollMergeEvents",Long.valueOf(descLBRACKET2STARn + 1RBRACKET),Long.valueOf(descLBRACKET2STARnRBRACKET),newValue)
sm.getString("endpoint.sendfileThreadStop")
sm.getString("endpoint.sendfile.error"),e
"Socket: [" + this + "], Read direct from socket: [" + nRead + "]"
"Socket: [" + this + "], Read into buffer: [" + nRead + "]"
"Calling [" + getEndpoint() + "].closeSocket([" + this + "])",NEWException()
sm.getString("endpoint.debug.handlerRelease"),e
sm.getString("endpoint.warn.noRemoteAddr",getSocket()),e
sm.getString("endpoint.warn.noRemoteHost",getSocket()),e
sm.getString("endpoint.warn.noRemotePort",getSocket()),e
sm.getString("endpoint.warn.noLocalName"),e
sm.getString("endpoint.warn.noLocalAddr"),e
sm.getString("endpoint.warn.noLocalPort"),e
"Skip concurrent " + (readQUES"read":"write") + " notification"
msg,t
msg,t
sm.getString("channel.nio.ssl.wrapException"),e
sm.getString("channel.nio.ssl.expandNetInBuffer",Integer.toString(newLimit))
sm.getString("channel.nio.ssl.sniDefault")
sm.getString("channel.nio.ssl.sniHostName",sc,hostName)
msg,e
sm.getString("endpoint.setAttributeError",name,value),e
sm.getString("endpoint.nio.stopLatchAwaitFail")
sm.getString("endpoint.nio.stopLatchAwaitInterrupted"),e
"Destroy initiated for " + NEWInetSocketAddress(getAddress(),getPortWithOffset())
"Destroy completed for " + NEWInetSocketAddress(getAddress(),getPortWithOffset())
sm.getString("endpoint.socketOptionsError"),t
sm.getString("endpoint.err.close"),ioe
sm.getString("endpoint.nio.registerFail"),x
sm.getString("endpoint.nio.pollerEventError"),x
sm.getString("endpoint.debug.channelCloseFail"),e
sm.getString("endpoint.nio.selectorCloseFail"),ioe
sm.getString("endpoint.nio.selectorLoopError"),x
sm.getString("endpoint.nio.keyProcessingError"),t
"Processing send file for: " + sd.fileName
"Send file complete for: " + sd.fileName
"Send file connection is being closed"
"Connection is keep alive, processing pipe-lined data"
"Connection is keep alive, registering back for OP_READ"
"OP_WRITE for sendfile: " + sd.fileName
"Unable to complete sendfile request:",e
sm.getString("endpoint.sendfile.error"),t
"timeout completed: keys processed=" + keycount + "; now=" + now + "; nextExpiration=" + prevExp + "; keyCount=" + keyCount + "; hasEvents=" + hasEvents + "; eval=" + ((nowLTprevExp)AMPAMP(keyCountGT0BARBARhasEvents)AMPAMP(BANGclose))
"Socket: [" + this + "], Read direct from socket: [" + nRead + "]"
"Socket: [" + this + "], Read into buffer: [" + nRead + "]"
"Calling [" + getEndpoint() + "].closeSocket([" + this + "])",NEWException()
sm.getString("endpoint.debug.handlerRelease"),e
sm.getString("endpoint.debug.channelCloseFail"),e
sm.getString("endpoint.sendfile.closeError"),e
"Skip concurrent " + (readQUES"read":"write") + " notification"
"Error during SSL handshake",x
sm.getString("endpoint.processing.fail"),t
"Possibly encountered sun bug 5076772 on windows JDK 1.5",x
"Possibly encountered sun bug 5076772 on windows JDK 1.5",x
sm.getString("nioBlockingSelector.selectError"),x
sm.getString("nioBlockingSelector.processingError"),t
"",ignore
"",ignore
sm.getString("nioBlockingSelector.possibleLeak")
sm.getString("sslHostConfigCertificate.mismatch",name,sslHostConfig.getHostName(),type,this.storeType)
sm.getString("endpoint.nio2.exclusiveExecutor")
sm.getString("endpoint.socketOptionsError"),t
sm.getString("endpoint.err.close"),ioe
sm.getString("endpoint.accept.fail"),t
"Socket: [" + Nio2SocketWrapper.this + "], Interest: [" + readInterest + "]"
"Socket: [" + this + "], block: [" + block + "], length: [" + len + "]"
"Socket: [" + this + "], Read in progress. Returning [0]"
"Socket: [" + this + "], Read: [" + nRead + "]"
"Socket: [" + this + "], Read in progress. Returning [0]"
"Socket: [" + this + "], Read direct from socket: [" + nRead + "]"
"Socket: [" + this + "], Read into buffer: [" + nRead + "]"
"Calling [" + getEndpoint() + "].closeSocket([" + this + "])",NEWException()
sm.getString("endpoint.debug.handlerRelease"),e
sm.getString("endpoint.debug.channelCloseFail"),e
sm.getString("endpoint.sendfile.closeError"),e
sm.getString("endpoint.warn.noRemoteHost",getSocket()),e
sm.getString("endpoint.warn.noRemotePort",getSocket()),e
sm.getString("endpoint.warn.noLocalName",getSocket()),e
sm.getString("endpoint.warn.noLocalAddr",getSocket()),e
sm.getString("endpoint.warn.noLocalPort",getSocket()),e
sm.getString("endpoint.err.handshake"),x
sm.getString("endpoint.processing.fail"),t
sm.getString("endpoint.launch.fail"),npe
sm.getString("channel.nio.ssl.wrapException"),e
sm.getString("channel.nio.ssl.expandNetInBuffer",Integer.toString(newLimit))
sm.getString("channel.nio.ssl.sniDefault")
sm.getString("channel.nio.ssl.sniHostName",sc,hostName)
sm.getString("sslHostConfig.mismatch",name,getHostName(),configType,this.configType)
sm.getString("sslHostConfig.prefix_missing",trimmed,getHostName())
sm.getString("jsseSupport.clientCertError"),t
sm.getString("jsseSupport.certTranslationError",certsLBRACKETiRBRACKET),ex
"Cert #" + i + " = " + x509CertsLBRACKETiRBRACKET
sm.getString("jsse.excludeProtocol",protocol)
sm.getString("jsse.noDefaultProtocols")
sm.getString("engine.ciphersFailure"),e
sm.getString("engine.unsupportedCipher",cipherSuite,converted)
sm.getString("engine.openSSLError",Long.toString(error),err)
msg,e
msg
msg,e
msg
sm.getString("openssl.makeConf")
sm.getString("openssl.doubleInit")
sm.getString("openssl.certificateVerificationFailed"),e
sm.getString("openssl.addedClientCaCert",caCert.toString())
sm.getString("openssl.checkConf")
sm.getString("openssl.errCheckConf")
sm.getString("openssl.applyConf")
sm.getString("openssl.errApplyConf")
sm.getString("openssl.errorSSLCtxInit"),e
sm.getString("opensslconf.noCommandName",value)
sm.getString("opensslconf.checkCommand",name,value)
sm.getString("opensslconf.checkFailed")
sm.getString("opensslconf.failedCommand",name,value,Integer.toString(rc))
sm.getString("opensslconf.resultCommand",name,value,Integer.toString(rc))
sm.getString("opensslconf.checkFailed")
sm.getString("opensslconf.noCommandName",value)
sm.getString("opensslconf.applyCommand",name,value)
sm.getString("opensslconf.applyFailed")
sm.getString("opensslconf.failedCommand",name,value,Integer.toString(rc))
sm.getString("opensslconf.resultCommand",name,value,Integer.toString(rc))
sm.getString("opensslconf.finishFailed",Integer.toString(rc))
sm.getString("opensslconf.applyFailed")
sm.getString("jsse.openssl.unknownElement",alias)
sm.getString("jsse.openssl.effectiveCiphers",displayResult(ciphers,true,","))
"Counting up[" + Thread.currentThread().getName() + "] latch=" + getCount()
"Counting down[" + Thread.currentThread().getName() + "] latch=" + result
sm.getString("byteBufferUtils.cleaner"),e
sm.getString("byteBufferUtils.cleaner"),e
"ByteCache generation time: " + (t2SUBt1) + "ms"
"CharCache generation time: " + (t2SUBt1) + "ms"
sm.getString("registry.noDisable")
sm.getString("registry.objectNameCreateError"),e
sm.getString("registry.initError"),t
sm.getString("registry.noMetadata",oname)
sm.getString("registry.noMetadata",oname)
sm.getString("registry.unregisterError"),t
"Using existing MBeanServer " + (System.currentTimeMillis()SUBt1)
"Creating MBeanServer" + (System.currentTimeMillis()SUBt1)
"Looking for descriptor "
"Introspecting "
sm.getString("registry.noTypeMetadata",type)
"load " + source
"Managed= " + oname
sm.getString("registry.nullBean",oname)
"Unregistering existing component " + oname
sm.getString("registry.registerError",oname),ex
"Finding descriptor " + res
"Found " + dURL
sm.getString("registry.loadError",dURL)
"Invoke " + name
sm.getString("baseModelMBean.invokeError",name),t
sm.getString("baseModelMBean.invokeError",name),e
"Setting attribute " + this + " " + attribute
sm.getString("baseModelMBean.invokeError",name),e
sm.getString("baseModelMBean.notificationError",name),ex
"addAttributeNotificationListener " + listener
"AttributeChangeNotification " + notification
"addNotificationListener " + listener
"addAttributeNotificationListener " + listener
"preRegister " + resource + " " + name
sm.getString("modules.readDescriptorsError"),ex
"Not public " + methodsLBRACKETjRBRACKET
"Unsupported type " + methodsLBRACKETjRBRACKET
"Unsupported type " + methodsLBRACKETjRBRACKET + " " + ret
"Unsupported type " + methodsLBRACKETjRBRACKET + " " + paramsLBRACKET0RBRACKET
"Introspected attribute " + name + " " + gm + " " + sm
"Setting name: " + type
sm.getString("modules.digesterParseError"),e
"Set encoding to " + charset.name()
"Set query string encoding to " + queryStringCharset.name()
"Decoding query " + decodedQuery + " " + queryStringCharset.name()
sm.getString("parameters.bytes",NEWString(bytes,start,len,DEFAULT_BODY_CHARSET))
sm.getString("parameters.noequal",Integer.valueOf(nameStart),Integer.valueOf(nameEnd),NEWString(bytes,nameStart,nameEndSUBnameStart,DEFAULT_BODY_CHARSET))
sm.getString("parameters.emptyChunk")
message
message
message
message
sm.getString("parameters.decodeFail.debug",origName.toString(),origValue.toString()),e
message
message
message
message
"Cookies: Parsing b[]: " + cookieValue.toString()
"Cookies: Parsing b[]: " + cookieValue.toString()
message
message
message
message
message
message
message
message
sm.getString("jre9Compat.invalidModuleUri",uri),e
sm.getString("jarResource.getInputStreamFail",getResource().getName(),getBaseUrl()),e
sm.getString("dirResourceSet.manifestFail",mf.getAbsolutePath()),e
sm.getString("fileResource.getCanonicalPathFail",resource.getPath()),ioe
sm.getString("fileResource.getCreationFail",resource.getPath()),e
sm.getString("fileResource.getUrlFail",resource.getPath()),e
sm.getString("cache.addFail",path,root.getContext().getName())
sm.getString("cache.addFail",path)
sm.getString("cache.backgroundEvictFail",Long.valueOf(TARGET_FREE_PERCENT_BACKGROUND),root.getContext().getName(),Long.valueOf(newSizeSLASH1024))
sm.getString("cache.objectMaxSizeTooBigBytes",Integer.valueOf(objectMaxSize))
sm.getString("cache.objectMaxSizeTooBig",Integer.valueOf(objectMaxSizeSLASH1024),Integer.valueOf((int)limitSLASH1024))
sm.getString("standardRoot.lockedFile",context.getName(),trackedResource.getName()),trackedResource.getCreatedBy()
sm.getString("fileResource.getUrlFail",url),e
"store " + elementDesc.getTag() + "( " + aElement + " )"
"Descriptor for element" + aElement.getClass() + " not configured or element class not StandardManager!"
"store " + elementDesc.getTag() + "( " + aElement + " )"
sm.getString("storeFactory.noDescriptor",aElement.getClass(),"NamingResources")
sm.getString("config.emptyObjectName")
sm.getString("config.storeServerError"),e
sm.getString("config.objectNameNotFound",aServerName)
sm.getString("config.emptyObjectName")
sm.getString("config.missingContextFile",aContext.getPath())
sm.getString("config.storeContextError",aContextName),e
sm.getString("config.objectNameNotFound",aContextName)
sm.getString("config.storeServerError"),e
sm.getString("config.storeContextError",aContext.getName()),e
sm.getString("config.missingContextFile",aContext.getPath())
"store " + elementDesc.getTag() + "( " + aElement + " )"
sm.getString("storeFactory.noDescriptor",aElement.getClass(),"WrapperLifecycle")
sm.getString("factory.storeTag",elementDesc.getTag(),aElement)
sm.getString("factory.storeNoDescriptor",aElement.getClass())
sm.getString("factory.storeTag",elementDesc.getTag(),aElement)
sm.getString("factory.storeNoDescriptor",aElement.getClass())
sm.getString("factory.storeTag",elementDesc.getTag(),aElement)
sm.getString("factory.storeNoDescriptor",aElement.getClass())
"store " + elementDesc.getTag() + "( " + aElement + " )"
sm.getString("storeFactory.noDescriptor",aElement.getClass(),"WrapperListener")
sm.getString("factory.storeTag",elementDesc.getTag(),aElement)
sm.getString("factory.storeNoDescriptor",aElement.getClass())
sm.getString("factory.storeNoDescriptor",aTagElement.getClass())
sm.getString("storeConfigListener.notServer")
sm.getString("storeConfigListener.loadError"),e
sm.getString("storeConfigListener.registerError"),ex
sm.getString("globalNamingResourcesSF.noFactory")
sm.getString("storeFactory.noDescriptor",aElement.getClass(),"GlobalNamingResources")
sm.getString("globalNamingResourcesSF.wrongElement",aElement.getClass())
"store " + elementDesc.getTag() + "( " + aElement + " )"
sm.getString("storeFactory.noDescriptor",aElement.getClass(),"WatchedResource")
sm.getString("standardContextSF.storeContext",aContext.getPath(),config)
sm.getString("standardContextSF.storeContextWithBackup",aContext.getPath(),mover.getConfigSave())
sm.getString("standardContextSF.canonicalPathError"),e
"search descriptor " + id
sm.getString("registry.loadClassFailed",id),e
"find descriptor " + id + "#" + desc.getTag() + "#" + desc.getStoreFactoryClass()
("Can't find descriptor for key " + id)
"register store descriptor " + key + "#" + desc.getTag() + "#" + desc.getTagClass()
sm.getString("factory.storeTag",elementDesc.getTag(),aElement)
sm.getString("factory.storeNoDescriptor",aElement.getClass())
"Unable to load properties using CatalinaProperties",ex
sm.getString("SecurityListener.checkUmaskParseFail",prop)
sm.getString("SecurityListener.checkUmaskSkip")
sm.getString("SecurityListener.checkUmaskNone",UMASK_PROPERTY_NAME,getMinimumUmask())
sm.getString("SecurityUtil.doAsPrivilege"),e
sm.getString("persistentManager.isLoadedError",id),e
sm.getString("persistentManager.storeClearError"),e.getException()
sm.getString("persistentManager.storeClearError"),e
"Start expire sessions " + getName() + " at " + timeNow + " sessioncount " + sessions.length
"End expire sessions " + getName() + " processingTime " + (timeEndSUBtimeNow) + " expired sessions: " + expireHere
sm.getString("persistentManager.storeLoadKeysError"),e.getException()
sm.getString("persistentManager.storeLoadKeysError"),e
sm.getString("persistentManager.loading",String.valueOf(n))
sm.getString("persistentManager.storeLoadError"),e
sm.getString("persistentManager.removeError"),e.getException()
sm.getString("persistentManager.removeError"),e
sm.getString("persistentManager.unloading",String.valueOf(n))
sm.getString("persistentManager.storeSizeException")
sm.getString("persistentManager.storeKeysException")
sm.getString("persistentManager.swapInInvalid",id)
sm.getString("persistentManager.swapIn",id)
msg,e
sm.getString("persistentManager.swapInException",id),e
sm.getString("persistentManager.serializeError",session.getIdInternal(),exception)
sm.getString("persistentManager.serializeError",session.getIdInternal(),e)
"No Store configured, persistence disabled"
"Stopping"
sm.getString("persistentManager.swapMaxIdle",session.getIdInternal(),Integer.valueOf(timeIdle))
sm.getString("persistentManager.tooManyActive",Integer.valueOf(sessions.length))
sm.getString("persistentManager.swapTooManyActive",session.getIdInternal(),Integer.valueOf(timeIdle))
sm.getString("persistentManager.backupMaxIdle",session.getIdInternal(),Integer.valueOf(timeIdle))
"Unreported exception in load() ",exception
"Start: Loading persisted sessions"
sm.getString("standardManager.loading",pathname)
"Loading " + n + " persisted sessions"
sm.getString("standardManager.deletePersistedFileFail",file)
"No persisted data file found"
"Finish: Loading persisted sessions"
"Unreported exception in unLoad()",exception
sm.getString("standardManager.unloading.debug")
sm.getString("standardManager.unloading.nosessions")
sm.getString("standardManager.unloading",pathname)
"Unloading " + sessions.size() + " sessions"
"Expiring " + list.size() + " persisted sessions"
"Unloading complete"
sm.getString("standardManager.managerLoad"),t
"Stopping"
sm.getString("standardManager.managerUnload"),t
"Start expire sessions " + getName() + " at " + timeNow + " sessioncount " + sessions.length
"End expire sessions " + getName() + " processingTime " + (timeEndSUBtimeNow) + " expired sessions: " + expireHere
"Force random number initialization starting"
"Force random number initialization completed"
msg
msg
msg
msg
sm.getString("managerBase.sessionNotFound",sessionId)
sm.getString("managerBase.sessionNotFound",sessionId)
sm.getString("managerBase.sessionNotFound",sessionId)
sm.getString("managerBase.sessionNotFound",sessionId)
sm.getString("managerBase.sessionNotFound",sessionId)
sm.getString("managerBase.sessionNotFound",sessionId)
sm.getString("managerBase.sessionNotFound",sessionId)
sm.getString("managerBase.sessionNotFound",sessionId)
sm.getString("managerBase.sessionNotFound",sessionId)
"add ClusterListener " + this.toString() + " to cluster" + cluster
"remove ClusterListener " + this.toString() + " from cluster"
sm.getString("backupManager.startUnable",getName()),x
"Backup manager, Setting map name to:" + name
sm.getString("backupManager.stopped",getName())
sm.getString("deltaSession.notifying",((ClusterManager)manager).getName(),Boolean.valueOf(isPrimarySession()),expiredId)
sm.getString("deltaSession.readSession",id)
sm.getString("deltaSession.writeSession",id)
sm.getString("standardSession.notSerializable",saveNames.get(i),id),e
sm.getString("deltaRequest.showPrincipal",p.getName(),getSessionId())
sm.getString("deltaRequest.wrongPrincipalClass",p.getClass().getName())
sm.getString("deltaRequest.removeUnable"),x
"Session.setAttribute('" + info.getName() + "', '" + info.getValue() + "')"
"Session.removeAttribute('" + info.getName() + "')"
"Session.setNew('" + info.getValue() + "')"
"Session.setMaxInactiveInterval('" + info.getValue() + "')"
sm.getString("deltaRequest.removeUnable"),x
sm.getString("deltaRequest.removeUnable"),x
sm.getString("deltaManager.createSession.newSession",session.getId(),Integer.valueOf(sessions.size()))
sm.getString("deltaManager.sendMessage.newSession",name,sessionId)
sm.getString("deltaManager.unableSerializeSessionID",newSessionID),e
sm.getString("deltaManager.loading.existing.session",session.getIdInternal())
sm.getString("deltaManager.loading.cnfe",e),e
sm.getString("deltaManager.loading.ioe",e),e
sm.getString("deltaManager.unloading.ioe",e),e
sm.getString("deltaManager.noCluster",getName())
sm.getString("deltaManager.registerCluster",getName(),type,cluster.getClusterName())
sm.getString("deltaManager.startClustering",getName())
sm.getString("deltaManager.managerLoad"),t
sm.getString("deltaManager.waitForSessionState",getName(),mbr,Integer.valueOf(getStateTransferTimeout()))
sm.getString("deltaManager.dropMessage",getName(),smsg.getEventTypeString(),NEWDate(stateTransferCreateSendTime),NEWDate(smsg.getTimestamp()))
sm.getString("deltaManager.noMembers",getName())
sm.getString("deltaManager.noMasterMember",getName(),"")
sm.getString("deltaManager.foundMasterMember",getName(),mbr)
sm.getString("deltaManager.noSessionState",getName(),NEWDate(beforeSendTime),Long.valueOf(reqNowSUBbeforeSendTime))
sm.getString("deltaManager.noContextManager",getName(),NEWDate(beforeSendTime),Long.valueOf(reqNowSUBbeforeSendTime))
sm.getString("deltaManager.sessionReceived",getName(),NEWDate(beforeSendTime),Long.valueOf(reqNowSUBbeforeSendTime))
sm.getString("deltaManager.stopped",getName())
sm.getString("deltaManager.expireSessions",getName())
sm.getString("deltaManager.createMessage.unableCreateDeltaRequest",sessionId),x
sm.getString("deltaManager.createMessage.accessChangePrimary",getName(),sessionId)
sm.getString("deltaManager.createMessage.delta",getName(),sessionId)
sm.getString("deltaManager.createMessage.access",getName(),sessionId)
sm.getString("deltaManager.createMessage.expire",getName(),id)
"Start expire all sessions " + getName() + " at " + timeNow + " sessioncount " + sessions.length
"End expire sessions " + getName() + " expire processingTime " + (timeEndSUBtimeNow) + " expired direct sessions: " + expireDirect + " expired direct sessions: " + expireIndirect
sm.getString("deltaManager.receiveMessage.eventType",getName(),msg.getEventTypeString(),sender)
sm.getString("deltaManager.receiveMessage.error",getName()),x
sm.getString("deltaManager.receiveMessage.transfercomplete",getName(),sender.getHost(),Integer.valueOf(sender.getPort()))
sm.getString("deltaManager.receiveMessage.delta.unknown",getName(),msg.getSessionID())
sm.getString("deltaManager.receiveMessage.delta",getName(),msg.getSessionID())
sm.getString("deltaManager.receiveMessage.accessed",getName(),msg.getSessionID())
sm.getString("deltaManager.receiveMessage.expired",getName(),msg.getSessionID())
sm.getString("deltaManager.receiveMessage.createNewSession",getName(),msg.getSessionID())
sm.getString("deltaManager.receiveMessage.allSessionDataBegin",getName())
sm.getString("deltaManager.receiveMessage.allSessionDataAfter",getName())
sm.getString("deltaManager.receiveMessage.unloadingBegin",getName())
sm.getString("deltaManager.createMessage.allSessionTransfered",getName())
sm.getString("deltaManager.receiveMessage.noContextManager",getName(),sender.getHost(),Integer.valueOf(sender.getPort()))
sm.getString("deltaManager.receiveMessage.unloadingAfter",getName())
sm.getString("deltaManager.createMessage.allSessionData",getName())
"no ReplicationValve found for CrossContext Support"
sm.getString("jvmRoute.missingJvmRouteAttribute")
sm.getString("jvmRoute.turnoverInfo",Long.valueOf(time))
sm.getString("jvmRoute.foundManager",manager,request.getContext().getName())
sm.getString("jvmRoute.notFoundManager",request.getContext().getName())
sm.getString("jvmRoute.failover",requestJvmRoute,localJvmRoute,sessionId)
sm.getString("jvmRoute.cannotFindSession",sessionId)
sm.getString("jvmRoute.changeSession",sessionId,newSessionID)
sm.getString("jvmRoute.set.orignalsessionid",sessionIdAttribute,sessionId)
sm.getString("jvmRoute.valve.started")
sm.getString("jvmRoute.noCluster")
sm.getString("jvmRoute.valve.stopped")
sm.getString("clusterSessionListener.noManager",entry.getKey())
sm.getString("clusterSessionListener.noManager",ctxname)
"open file " + f + " write " + openForWrite
"Message " + msg + " data " + HexUtils.toHexString(msg.getData()) + " data length " + msg.getDataLength() + " out " + out
sm.getString("fileMessageFactory.deleteFail",file)
sm.getString("farmWarDeployer.hostOnly")
sm.getString("farmWarDeployer.hostParentEngine",host.getName())
sm.getString("farmWarDeployer.mbeanNameFail",engine.getName(),hostname),e
sm.getString("farmWarDeployer.watchDir",getWatchDir())
sm.getString("farmWarDeployer.started")
sm.getString("farmWarDeployer.stopped")
sm.getString("farmWarDeployer.msgRxDeploy",fmsg.getContextName(),fmsg.getFileName())
sm.getString("farmWarDeployer.renameFail",factory.getFile(),deployable)
sm.getString("farmWarDeployer.deployEnd",contextName)
sm.getString("farmWarDeployer.servicingDeploy",contextName,name)
sm.getString("farmWarDeployer.fileMessageError"),ex
sm.getString("farmWarDeployer.msgRxUndeploy",contextName)
sm.getString("farmWarDeployer.undeployEnd",contextName)
sm.getString("farmWarDeployer.servicingUndeploy",contextName)
sm.getString("farmWarDeployer.undeployMessageError"),ex
sm.getString("farmWarDeployer.msgIoe"),x
sm.getString("farmWarDeployer.sendStart",contextName,webapp)
sm.getString("farmWarDeployer.sendFragment",contextName,webapp,membersLBRACKETiRBRACKET)
sm.getString("farmWarDeployer.sendEnd",contextName,webapp)
sm.getString("farmWarDeployer.removeStart",contextName)
sm.getString("farmWarDeployer.removeTxMsg",contextName)
sm.getString("farmWarDeployer.removeFailRemote",contextName)
sm.getString("farmWarDeployer.removeFailLocal",contextName),ex
sm.getString("farmWarDeployer.alreadyDeployed",cn.getName())
sm.getString("farmWarDeployer.modInstall",cn.getName(),deployWar.getAbsolutePath())
sm.getString("farmWarDeployer.servicingDeploy",cn.getName(),deployWar.getName())
sm.getString("farmWarDeployer.modInstallFail"),x
sm.getString("farmWarDeployer.removeLocal",cn.getName())
sm.getString("farmWarDeployer.removeLocalFail"),x
sm.getString("farmWarDeployer.undeployLocal",contextName)
sm.getString("farmWarDeployer.deleteFail",war)
sm.getString("farmWarDeployer.deleteFail",xml)
sm.getString("farmWarDeployer.deleteFail",file)
sm.getString("farmWarDeployer.deleteFail",dir)
sm.getString("fileNewFail",to)
sm.getString("farmWarDeployer.fileCopyFail",from,to),e
sm.getString("farmWarDeployer.fileCopyFail",from,to),e
sm.getString("warWatcher.checkingWars",watchDir)
sm.getString("warWatcher.cantListWatchDir",watchDir)
sm.getString("warWatcher.listedFileDoesNotExist",listLBRACKETiRBRACKET,watchDir)
sm.getString("warWatcher.checkingWar",info.getWar())
sm.getString("warWatcher.checkWarResult",Integer.valueOf(check),info.getWar())
sm.getString("replicatedContext.startUnable",getName()),x
sm.getString("multiCastSender.multiCastFailed"),ex
sm.getString("multiCastSender.sendFailed"),ex
sm.getString("heartbeatListener.errorCollectingInfo"),ex
sm.getString("heartbeatListener.senderInitError"),ex
sm.getString("heartbeatListener.refreshError"),ex
sm.getString("heartbeatListener.sendError"),ex
sm.getString("tcpSender.notInitialized")
sm.getString("tcpSender.connectionFailed"),ex
sm.getString("tcpSender.sendFailed"),ex
sm.getString("tcpSender.responseError")
sm.getString("tcpSender.responseErrorCode",Integer.valueOf(status))
sm.getString("tcpSender.readError")
sm.getString("ReplicationValve.filter.loading",filter)
sm.getString("ReplicationValve.filter.failure",filter),pse
sm.getString("ReplicationValve.crossContext.registerSession",session.getIdInternal(),session.getManager().getContext().getName())
sm.getString("ReplicationValve.crossContext.add")
sm.getString("ReplicationValve.crossContext.remove")
sm.getString("ReplicationValve.nocluster")
sm.getString("ReplicationValve.crossContext.sendDelta",session.getManager().getContext().getName())
sm.getString("ReplicationValve.resetDeltaRequest",session.getManager().getContext().getName())
sm.getString("ReplicationValve.invoke.uri",uri)
sm.getString("ReplicationValve.send.invalid.failure",invalidIdsLBRACKETiRBRACKET),x
sm.getString("ReplicationValve.stats",NEWObjectLBRACKETRBRACKETLBRACELong.valueOf(totalRequestTimeSLASHnrOfRequests),Long.valueOf(totalSendTimeSLASHnrOfRequests),Long.valueOf(nrOfRequests),Long.valueOf(nrOfSendRequests),Long.valueOf(nrOfCrossContextSendRequests),Long.valueOf(nrOfFilterRequests),Long.valueOf(totalRequestTime),Long.valueOf(totalSendTime)RBRACE)
sm.getString("ReplicationValve.session.indicator",request.getContext().getName(),id,primaryIndicatorName,Boolean.valueOf(cses.isPrimarySession()))
sm.getString("ReplicationValve.session.found",request.getContext().getName(),id)
sm.getString("ReplicationValve.session.invalid",request.getContext().getName(),id)
"Creating ClusterManager for context " + name + " using class " + getManagerTemplate().getClass().getName()
sm.getString("simpleTcpCluster.clustermanager.cloneFailed"),x
sm.getString("simpleTcpCluster.clustermanager.notImplement",manager)
sm.getString("simpleTcpCluster.start")
sm.getString("simpleTcpCluster.startUnable"),x
"Invoking addValve on " + getContainer() + " with class=" + valve.getClass().getName()
"Invoking removeValve on " + getContainer() + " with class=" + valve.getClass().getName()
sm.getString("simpleTcpCluster.stopUnable"),x
sm.getString("simpleTcpCluster.unableSend.localMember",msg)
"No members in cluster, ignoring message:" + msg
sm.getString("simpleTcpCluster.sendFailed"),x
sm.getString("simpleTcpCluster.member.added",member)
sm.getString("simpleTcpCluster.member.addFailed"),x
sm.getString("simpleTcpCluster.member.disappeared",member)
sm.getString("simpleTcpCluster.member.removeFailed"),x
"Assuming clocks are synched: Replication for " + message.getUniqueId() + " took=" + (System.currentTimeMillis()SUB(message).getTimestamp()) + " ms."
"Message " + message.toString() + " from type " + message.getClass().getName() + " transferred but no listener registered"
message.toString()
sm.getString("remoteIpFilter.invalidHostWithPort",hostHeaderValue,hostHeader)
sm.getString("remoteIpFilter.invalidHostHeader",hostHeaderValue,hostHeader)
"Incoming request " + request.getRequestURI() + " with originalRemoteAddr [" + request.getRemoteAddr() + "], originalRemoteHost=[" + request.getRemoteHost() + "], originalSecure=[" + request.isSecure() + "], originalScheme=[" + request.getScheme() + "], originalServerName=[" + request.getServerName() + "], originalServerPort=[" + request.getServerPort() + "] will be seen as newRemoteAddr=[" + xRequest.getRemoteAddr() + "], newRemoteHost=[" + xRequest.getRemoteHost() + "], newSecure=[" + xRequest.isSecure() + "], newScheme=[" + xRequest.getScheme() + "], newServerName=[" + xRequest.getServerName() + "], newServerPort=[" + xRequest.getServerPort() + "]"
"Skip RemoteIpFilter for request " + request.getRequestURI() + " with originalRemoteAddr '" + request.getRemoteAddr() + "'"
"Invalid port value [" + portHeaderValue + "] provided in header [" + getPortHeader() + "]"
sm.getString("expiresFilter.responseAlreadyCommited",httpRequest.getRequestURL())
sm.getString("expiresFilter.useMatchingConfiguration",configuration,contentType,contentType,result)
sm.getString("expiresFilter.useMatchingConfiguration",configuration,contentTypeWithoutCharset,contentType,result)
sm.getString("expiresFilter.useMatchingConfiguration",configuration,majorType,contentType,result)
sm.getString("expiresFilter.useDefaultConfiguration",defaultExpiresConfiguration,contentType,result)
sm.getString("expiresFilter.noExpirationConfiguredForContentType",contentType)
sm.getString("expiresFilter.unknownParameterIgnored",name,value)
sm.getString("expiresFilter.filterInitialized",this.toString())
sm.getString("expiresFilter.expirationHeaderAlreadyDefined",request.getRequestURI(),Integer.valueOf(response.getStatus()),response.getContentType())
sm.getString("expiresFilter.skippedStatusCode",request.getRequestURI(),Integer.valueOf(response.getStatus()),response.getContentType())
sm.getString("expiresFilter.noExpirationConfigured",request.getRequestURI(),Integer.valueOf(response.getStatus()),response.getContentType())
sm.getString("expiresFilter.setExpirationDate",request.getRequestURI(),Integer.valueOf(response.getStatus()),response.getContentType(),expirationDate)
message
message
sb.toString()
sm.getString("webappLoader.starting")
sm.getString("webappLoader.noResources",context)
sm.getString("webappLoader.stopping")
sm.getString("webappLoader.stopError"),e
sm.getString("webappLoader.reloadable",event.getNewValue().toString())
sm.getString("webappLoader.unknownClassLoader",loader,loader.getClass())
sm.getString("webappClassLoader.addPermisionNoCanonicalFile",url.toExternalForm())
sm.getString("webappClassLoader.addTransformer",transformer,getContextName())
sm.getString("webappClassLoader.removeTransformer",transformer,getContextName())
"modified()"
sm.getString("webappClassLoader.resourceModified",entry.getKey(),NEWDate(cachedLastModified),NEWDate(lastModified))
sm.getString("webappClassLoader.jarsRemoved",resources.getContext().getName())
"    findClass(" + name + ")"
"      securityManager.checkPackageDefinition"
"      -->Exception-->ClassNotFoundException",se
"      findClassInternal(" + name + ")"
sm.getString("webappClassLoader.securityException",name,ace.getMessage()),ace
"      -->RuntimeException Rethrown",e
sm.getString("webappClassLoader.securityException",name,ace.getMessage()),ace
"      -->RuntimeException Rethrown",e
"    --> Returning ClassNotFoundException"
"    --> Passing on ClassNotFoundException"
"      Returning class " + clazz
"      Loaded by " + cl.toString()
"    findResource(" + name + ")"
"    --> Returning '" + url.toString() + "'"
"    --> Resource not found, returning null"
"    findResources(" + name + ")"
"getResource(" + name + ")"
"  Delegating to parent classloader " + parent
"  --> Returning '" + url.toString() + "'"
"  --> Returning '" + url.toString() + "'"
"  --> Returning '" + url.toString() + "'"
"  --> Resource not found, returning null"
"getResourceAsStream(" + name + ")"
"  Delegating to parent classloader " + parent
"  --> Returning stream from parent"
"  Searching local repositories"
"  --> Returning stream from local"
"  Delegating to parent classloader unconditionally " + parent
"  --> Returning stream from parent"
"  --> Resource not found, returning null"
"loadClass(" + name + ", " + resolve + ")"
"  Returning class from cache"
"  Returning class from cache"
error,se
"  Delegating to parent classloader1 " + parent
"  Loading class from parent"
"  Searching local repositories"
"  Loading class from local repository"
"  Delegating to parent classloader at end: " + parent
"  Loading class from parent"
msg,ise
sm.getString("webappClassLoader.superCloseFail"),ioe
sm.getString("webappClassLoader.clearJdbc",getContextName(),name)
sm.getString("webappClassLoader.jdbcRemoveFailed",getContextName()),t
sm.getString("webappClassLoader.checkThreadsHttpClient")
sm.getString("webappClassLoader.stackTraceRequestThread",getContextName(),threadName,getStackTrace(thread))
sm.getString("webappClassLoader.stackTrace",getContextName(),threadName,getStackTrace(thread))
sm.getString("webappClassLoader.stopThreadFail",thread.getName(),getContextName()),e
sm.getString("webappClassLoader.warnTimerThread",getContextName(),thread.getName())
sm.getString("webappClassLoader.stopTimerThreadFail",thread.getName(),getContextName()),t
sm.getString("webappClassLoader.checkThreadLocalsForLeaksFail",getContextName()),t
sm.getString("webappClassLoader.checkThreadLocalsForLeaks.badKey",argsLBRACKET1RBRACKET),e
sm.getString("webappClassLoader.checkThreadLocalsForLeaks.badValue",argsLBRACKET3RBRACKET),e
sm.getString("webappClassLoader.checkThreadLocalsForLeaks",args)
sm.getString("webappClassLoader.checkThreadLocalsForLeaksNull",args)
sm.getString("webappClassLoader.checkThreadLocalsForLeaksNone",args)
sm.getString("webappClassLoader.loadedByThisOrChildFail",clazz.getName(),getContextName()),e
msg,se
msg
sm.getString("webappClassLoader.clearRmi",stubObject.getClass().getName(),stubObject)
sm.getString("webappClassLoader.clearRmiInfo",getContextName()),e
sm.getString("webappClassLoader.clearRmiFail",getContextName()),e
sm.getString("webappClassLoader.clearObjectStreamClassCachesFail",getContextName()),e
sm.getString("webappClassLoader.transformError",name),e
sm.getString("webappClassLoaderParallel.registrationFailed")
sm.getString("namingResources.mbeanCreateFail",environment.getName()),e
sm.getString("namingResources.mbeanCreateFail",resource.getName()),e
sm.getString("namingResources.mbeanCreateFail",resourceLink.getName()),e
sm.getString("namingResources.mbeanDestroyFail",environment.getName()),e
sm.getString("namingResources.mbeanDestroyFail",resource.getName()),e
sm.getString("namingResources.mbeanDestroyFail",resourceLink.getName()),e
sm.getString("namingResources.mbeanCreateFail",cr.getName()),e
sm.getString("namingResources.mbeanCreateFail",ce.getName()),e
sm.getString("namingResources.mbeanCreateFail",crl.getName()),e
sm.getString("namingResources.cleanupNoContext",container),e
sm.getString("namingResources.cleanupNoResource",cr.getName(),container),e
sm.getString("namingResources.cleanupCloseSecurity",closeMethod,name,container)
sm.getString("namingResources.cleanupNoClose",name,container,closeMethod)
sm.getString("namingResources.cleanupCloseFailed",closeMethod,name,container),e
sm.getString("namingResources.cleanupCloseFailed",closeMethod,name,container),t
sm.getString("namingResources.mbeanDestroyFail",crl.getName()),e
sm.getString("namingResources.mbeanDestroyFail",ce.getName()),e
sm.getString("namingResources.mbeanDestroyFail",cr.getName()),e
String.format("Failed to parse [%s] as integer, channelSendOptions possibly set by name(s)",input)
"Publishing backup data:" + msg + " to: " + next.getName()
"Data published:" + msg + " msg Id:" + id
sm.getString("lazyReplicatedMap.unableReplicate.backup",key,next,x.getMessage()),x
"Publishing proxy data:" + msg + " to: " + Arrays.toNameString(proxies)
"Member[" + member + "] disappeared, but was not present in the map."
sm.getString("replicatedMap.member.disappeared",member)
sm.getString("replicatedMap.unable.relocate",entry.getKey()),x
sm.getString("replicatedMap.unable.relocate",entry.getKey()),x
sm.getString("replicatedMap.relocate.complete",Long.toString(complete))
sm.getString("abstractReplicatedMap.init.start",mapContextName)
"Created Lazy Map with name:" + mapContextName + ", bytes:" + Arrays.toString(this.mapContextName)
sm.getString("abstractReplicatedMap.unableSend.startMessage")
sm.getString("abstractReplicatedMap.init.completed",mapContextName,Long.toString(complete))
sm.getString("abstractReplicatedMap.ping.stateTransferredMember",member)
sm.getString("abstractReplicatedMap.mapMember.unavailable",member)
sm.getString("abstractReplicatedMap.unable.deserialize.MapMessage"),e
sm.getString("abstractReplicatedMap.ping.timeout",member,mapname)
sm.getString("abstractReplicatedMap.broadcast.noReplies")
"Replicate invoked on key:" + key
"Not replicating:" + key + ", no change made"
sm.getString("abstractReplicatedMap.unable.diffObject"),x
sm.getString("abstractReplicatedMap.unable.replicate"),x
sm.getString("abstractReplicatedMap.transferState.noReplies")
sm.getString("abstractReplicatedMap.unable.transferState"),x
sm.getString("abstractReplicatedMap.unable.transferState"),x
sm.getString("abstractReplicatedMap.unable.transferState"),x
sm.getString("abstractReplicatedMap.leftOver.pingMsg",member)
sm.getString("abstractReplicatedMap.leftOver.ignored",mapmsg.getTypeDesc())
sm.getString("abstractReplicatedMap.unable.deserialize.MapMessage"),x
sm.getString("abstractReplicatedMap.unable.deserialize.MapMessage"),x
"Map[" + mapname + "] received message:" + mapmsg
sm.getString("abstractReplicatedMap.unable.deserialize.MapMessage"),x
sm.getString("abstractReplicatedMap.unable.deserialize.MapMessage"),x
"Map message received from:" + sender.getName() + " msg:" + mapmsg
sm.getString("abstractReplicatedMap.unableApply.diff",entry.getKey()),x
"Map[" + mapname + "] accepting...." + msg
"Msg[" + mapname + "] accepted[" + result + "]...." + msg
sm.getString("abstractReplicatedMap.mapMemberAdded.nullMember",member)
sm.getString("abstractReplicatedMap.mapMemberAdded.added",mapMember)
sm.getString("abstractReplicatedMap.unableSelect.backup"),x
"Member[" + member + "] disappeared, but was not present in the map."
sm.getString("abstractReplicatedMap.member.disappeared",member)
"[1] Primary choosing a new backup"
sm.getString("abstractReplicatedMap.unable.relocate",entry.getKey()),x
"[2] Primary disappeared"
"[3] Removing orphaned proxy"
"[4] Backup becoming primary"
sm.getString("abstractReplicatedMap.unable.relocate",entry.getKey()),x
sm.getString("abstractReplicatedMap.relocate.complete",Long.toString(complete))
sm.getString("abstractReplicatedMap.heartbeat.failed"),x
sm.getString("abstractReplicatedMap.unable.remove"),x
"Requesting id:" + key + " entry:" + entry
sm.getString("abstractReplicatedMap.unable.get"),x
"Requesting id:" + key + " result:" + entry.getValue()
sm.getString("abstractReplicatedMap.unable.put"),x
sm.getString("PooledSender.senderDisconnectFail"),e
"Starting replication listener on address:" + host
sm.getString("receiverBase.bind.failed",host),ioe
sm.getString("receiverBase.socket.bind",addr)
sm.getString("receiverBase.unable.bind",addr)
sm.getString("receiverBase.udp.bind",addr)
sm.getString("receiverBase.unable.bind.udp",addr)
sm.getString("nioSender.unable.disconnect",x.getMessage())
sm.getString("nioSender.unable.disconnect",x.getMessage()),x
"Error sending message",x
"Error while processing send to " + sender.getDestination().getName(),x
sm.getString("parallelNioSender.send.fail.retrying",sender.getDestination().getName())
sm.getString("parallelNioSender.send.fail",sender.getDestination().getName()),x
sm.getString("parallelNioSender.sender.disconnected.notRetry",sender.getDestination().getName())
"Failed to close selector",e
sm.getString("parallelNioSender.error.keepalive",sender),x
sm.getString("nioReceiver.threadpool.fail"),x
sm.getString("nioReceiver.start.fail"),x
"Adding event to selector:" + event
"Processing event in selector:" + r
sm.getString("nioReceiver.eventsError"),x
"",e
"",e
"",e
sm.getString("nioReceiver.threadsExhausted",Integer.valueOf(getTimeout()),Boolean.valueOf(ka.isCancelled()),key,NEWjava.sql.Timestamp(ka.getLastAccess()))
sm.getString("nioReceiver.alreadyStarted")
sm.getString("nioReceiver.clientDisconnect")
sm.getString("nioReceiver.requestError"),t
"Unable to close datagram channel.",iox
sm.getString("nioReceiver.stop.threadRunning")
sm.getString("nioReceiver.stop.fail"),x
sm.getString("nioReceiver.cleanup.fail"),ignore
sm.getString("nioReceiver.run.fail"),x
"No TcpReplicationThread available"
"Servicing key:" + key
"No object reader, cancelling:" + key
"Draining channel:" + key
"IOException in replication worker, unable to drain channel. Probable cause: Keep alive socket closed[" + e.getMessage() + "].",e
sm.getString("nioReplicationTask.unable.drainChannel.ioe",e.getMessage())
"About to service key:" + key
sm.getString("nioReplicationTask.process.clusterMsg.failed"),e
sm.getString("nioReplicationTask.process.clusterMsg.failed"),e
"Channel closed on the remote end, disconnecting"
"Adding key for read event:" + key
"Registering key for read:" + key
"CKX Cancelling key:" + key
sm.getString("nioReplicationTask.error.register.key",key),x
"Adding key for cancel event:" + key
"Cancelling key:" + key
"ACK sent to " + ((channelINSTANCEOFSocketChannel)QUES((SocketChannel)channel).socket().getInetAddress():((DatagramChannel)channel).socket().getInetAddress())
sm.getString("nioReplicationTask.unable.ack",x.getMessage())
sm.getString("bioReceiver.threadpool.fail"),x
sm.getString("bioReceiver.start.fail"),x
sm.getString("bioReceiver.socket.closeFailed"),x
sm.getString("bioReceiver.run.fail"),x
sm.getString("bioReceiver.already.started")
sm.getString("bioReceiver.threads.busy")
sm.getString("bioSender.disconnect",getAddress().getHostAddress(),Integer.valueOf(getPort()),Long.valueOf(0))
sm.getString("bioSender.send.again",getAddress().getHostAddress(),Integer.valueOf(getPort())),x
sm.getString("bioSender.openSocket",getAddress().getHostAddress(),Integer.valueOf(getPort()),Long.valueOf(0))
sm.getString("bioSender.openSocket.failure",getAddress().getHostAddress(),Integer.valueOf(getPort()),Long.valueOf(0)),ex1
sm.getString("bioSender.closeSocket",getAddress().getHostAddress(),Integer.valueOf(getPort()),Long.valueOf(0))
errmsg,x
errmsg,x
sm.getString("bioReplicationTask.unable.service"),x
sm.getString("bioReplicationTask.socket.closeFailed"),e
sm.getString("bioReplicationTask.reader.closeFailed"),e
sm.getString("bioReplicationTask.messageDataReceived.error"),x
"ACK sent to " + socket.getPort()
sm.getString("bioReplicationTask.unable.sendAck",x.getMessage())
sm.getString("bioReplicationTask.socket.closeFailed"),e
sm.getString("bioReplicationTask.reader.closeFailed"),e
sm.getString("bufferPool.created",Integer.toString(DEFAULT_POOL_SIZE),pool.getClass().getName())
sm.getString("xByteBuffer.discarded.invalidHeader")
sm.getString("xByteBuffer.discarded.invalidHeader")
sm.getString("McastService.parseTTL",properties.getProperty("mcastTTL")),x
sm.getString("McastService.parseSoTimeout",properties.getProperty("mcastSoTimeout")),x
sm.getString("McastService.stopFail",Integer.valueOf(svc)),x
sm.getString("McastService.payload"),x
sm.getString("McastService.domain"),x
sm.getString("staticMembershipService.stopFail",Integer.valueOf(level)),e
sm.getString("staticMembershipProvider.startMembership.noReplies")
sm.getString("staticMembershipProvider.stopMembership.sendFailed"),e
sm.getString("staticMembershipProvider.replyRequest.ignored",memMsg.getTypeDesc())
sm.getString("staticMembershipProvider.leftOver.ignored",memMsg.getTypeDesc())
sm.getString("staticMembershipProvider.heartbeat.failed"),e
sm.getString("staticMembershipProvider.pingThread.failed"),x
sm.getString("mcastServiceImpl.bind",address,Integer.toString(port))
sm.getString("mcastServiceImpl.setInterface",mcastBindAddress)
sm.getString("mcastServiceImpl.setSoTimeout",Integer.toString(mcastSoTimeout))
sm.getString("mcastServiceImpl.setTTL",Integer.toString(mcastTTL))
sm.getString("mcastServiceImpl.unable.join")
sm.getString("mcastServiceImpl.waitForMembers.start",Long.toString(memberwait),Integer.toString(level))
sm.getString("mcastServiceImpl.waitForMembers.done",Integer.toString(level))
sm.getString("mcastServiceImpl.packet.tooLong",Integer.toString(receivePacket.getLength()))
"Mcast receive ping from member " + m
"Member has shutdown:" + m
"Mcast add member " + m
"Mcast received broadcasts."
"Unable to decode message.",ise
sm.getString("mcastServiceImpl.unableReceive.broadcastMessage"),t
"Mcast expire  member " + expiredLBRACKETiRBRACKET
sm.getString("mcastServiceImpl.memberDisappeared.failed"),x
"Mcast send ping from member " + member
"Sending message broadcast " + packet.getLength() + " bytes from " + member
"Invalid member mcast package.",ax
sm.getString("mcastServiceImpl.error.receiving"),x
"Error receiving mcast package" + (doRunReceiverQUES". Sleeping 500ms":"."),x
sm.getString("mcastServiceImpl.send.failed"),x
"Unable to send mcast message.",x
sm.getString("mcastServiceImpl.recovery.stopFailed"),x
sm.getString("mcastServiceImpl.recovery.startFailed"),x
sm.getString("mcastServiceImpl.recovery")
sm.getString("mcastServiceImpl.recovery.successful")
sm.getString("mcastServiceImpl.recovery.failed",Integer.toString(PLUSPLUSattempt),Long.toString(parent.recoverySleepTime))
"Using membershipProvider: " + provider
sm.getString("cloudMembershipService.stopFail",Integer.valueOf(level)),e
String.format("setLocalMemberProperties(%s, %d, %d, %d)",listenHost,Integer.toString(listenPort),Integer.toString(securePort),Integer.toString(udpPort))
String.format("Namespace [%s] set; clustering enabled",namespace)
sm.getString("dnsMembershipProvider.dnsError",namespace),exception
sm.getString("kubernetesMembershipProvider.memberError"),e
String.format("Namespace [%s] set; clustering enabled",namespace)
sm.getString("kubernetesMembershipProvider.streamError"),e
sm.getString("kubernetesMembershipProvider.streamError"),e
sm.getString("kubernetesMembershipProvider.invalidPodsList","no items")
sm.getString("kubernetesMembershipProvider.invalidPod")
sm.getString("kubernetesMembershipProvider.invalidPod")
sm.getString("kubernetesMembershipProvider.invalidPod")
sm.getString("kubernetesMembershipProvider.invalidPod")
sm.getString("kubernetesMembershipProvider.invalidPod")
sm.getString("kubernetesMembershipProvider.invalidPod")
sm.getString("kubernetesMembershipProvider.jsonError"),e
String.format("%s opening connection: url [%s], headers [%s], connectTimeout [%s], readTimeout [%s]",getClass().getSimpleName(),url,headers,Integer.toString(connectTimeout),Integer.toString(readTimeout))
String.format("Using HttpsURLConnection with SSLSocketFactory [%s] for url [%s].",getSocketFactory(),url)
String.format("Using URLConnection for url [%s].",url)
sm.getString("abstractStream.fileNotFound",caCertFile)
sm.getString("abstractStream.trustManagerError",caCertFile)
sm.getString("abstractStream.CACertUndefined")
sm.getString("certificateStream.clientCertError",clientCertFile,clientKeyFile)
sm.getString("kubernetesMembershipProvider.noNamespace")
"Member added: " + member
"Member disappeared: " + member
sm.getString("uuidGenerator.createRandom",secrand.getAlgorithm(),Long.valueOf(time))
sm.getString("groupChannel.unable.deserialize",msg),sx
sm.getString("groupChannel.receiving.error"),x
sm.getString("groupChannel.sendFail.noRpcChannelReply"),x
sm.getString("groupChannel.warn.noUtilityExecutor")
sm.getString("groupChannel.unable.sendHeartbeat"),e
sm.getString("rpcChannel.replyFailed"),x
sm.getString("staticMembershipInterceptor.no.failureDetector")
sm.getString("staticMembershipInterceptor.no.pingInterceptor")
sm.getString("staticMembershipInterceptor.sendLocalMember.failed"),cx
sm.getString("staticMembershipInterceptor.sendShutdown.failed"),cx
"Received a failure detector packet:" + msg
sm.getString("tcpFailureDetector.memberDisappeared.verify",member)
sm.getString("tcpFailureDetector.already.disappeared",member)
sm.getString("tcpFailureDetector.member.disappeared",member)
sm.getString("tcpFailureDetector.still.alive",member)
sm.getString("tcpFailureDetector.heartbeat.failed"),x
sm.getString("tcpFailureDetector.performBasicCheck.memberAdded",membersLBRACKETiRBRACKET)
sm.getString("tcpFailureDetector.suspectMember.dead",m)
sm.getString("tcpFailureDetector.suspectMember.alive",m)
sm.getString("tcpFailureDetector.failureDetection.failed",mbr),x
sm.getString("messageDispatchInterceptor.warning.optionflag")
sm.getString("messageDispatchInterceptor.completeMessage.failed"),ex
sm.getString("messageDispatchInterceptor.AsyncMessage.failed"),x
sm.getString("messageDispatchInterceptor.errorMessage.failed"),ex
sm.getString("domainFilterInterceptor.message.refused",msg.getAddress())
sm.getString("domainFilterInterceptor.member.refused",member)
sm.getString("throughputInterceptor.report",msgTxCnt,df.format(mbTx),df.format(mbAppTx),df.format(timeTx),df.format(mbTxSLASHtimeTx),df.format(mbAppTxSLASHtimeTx),msgTxErr,msgRxCnt,df.format(mbRxSLASH((System.currentTimeMillis()SUBrxStart)SLASH(double)1000)),df.format(mbRx))
sm.getString("tcpPingInterceptor.ping.failed"),x
"Received a TCP ping packet:" + msg
sm.getString("tcpPingInterceptor.pingFailed.pingThread"),x
sm.getString("twoPhaseCommitInterceptor.originalMessage.missing",Arrays.toString(id.getBytes()))
sm.getString("twoPhaseCommitInterceptor.expiredMessage",entry.id)
sm.getString("twoPhaseCommitInterceptor.heartbeat.failed"),x
sm.getString("fragmentationInterceptor.heartbeat.failed"),x
sm.getString("gzipInterceptor.compress.failed")
sm.getString("gzipInterceptor.decompress.failed"),x
sm.getString("gzipInterceptor.report",Integer.valueOf(getCount()),Integer.valueOf(getCountCompressedTX()),Integer.valueOf(getCountUncompressedTX()),Integer.valueOf(getCountCompressedRX()),Integer.valueOf(getCountUncompressedRX()),Long.valueOf(getSizeTX()),Long.valueOf(getCompressedSizeTX()),Long.valueOf(getUncompressedSizeTX()),Long.valueOf(getSizeRX()),Long.valueOf(getCompressedSizeRX()),Long.valueOf(getUncompressedSizeRX()))
sm.getString("nonBlockingCoordinator.electionMessage.sendfailed",msg.getMembers()LBRACKETcurrentRBRACKET)
sm.getString("nonBlockingCoordinator.memberAlive.failed"),x
sm.getString("nonBlockingCoordinator.processCoordinationMessage.failed"),x
sm.getString("nonBlockingCoordinator.memberAdded.failed"),x
sm.getString("nonBlockingCoordinator.memberDisappeared.failed"),x
sm.getString("nonBlockingCoordinator.heartbeat.failed"),x
event
sm.getString("encryptInterceptor.encrypt.failed")
sm.getString("encryptInterceptor.decrypt.failed"),gse
sm.getString("jmxRegistry.no.domain")
sm.getString("jmxRegistry.objectName.failed",sb.toString()),e
sm.getString("jmxRegistry.registerJmx.notCompliant",bean),e
sm.getString("jmxRegistry.objectName.failed",oNameStr),e
sm.getString("jmxRegistry.registerJmx.failed",bean,oNameStr),e
sm.getString("jmxRegistry.unregisterJmx.notFound",oname),e
sm.getString("jmxRegistry.unregisterJmx.failed",oname),e
sm.getString("jmxRegistry.objectName.failed",query),e
sm.getString("coyoteResponse.encoding.invalid",mLBRACKET1RBRACKET),e
sm.getString("coyoteResponse.encoding.invalid",charset),e
sm.getString("coyoteResponse.encoding.invalid",charset),e
sm.getString("response.sendRedirectFail",location),e
sm.getString("coyoteConnector.protocolHandlerInstantiationFailed"),e
sm.getString("coyoteConnector.invalidEncoding",URIEncoding,uriCharset.name()),e
sm.getString("coyoteConnector.protocolHandlerPauseFailed"),e
sm.getString("coyoteConnector.protocolHandlerResumeFailed"),e
sm.getString("coyoteAdapter.asyncDispatch"),t
sm.getString("coyoteAdapter.accesslogFail"),t
sm.getString(messageKey),NEWRecycleRequiredException()
sm.getString(messageKey),NEWRecycleRequiredException()
sm.getString("coyoteAdapter.authenticate",username)
sm.getString("coyoteAdapter.authorize",username)
sm.getString("coyoteAdapter.debug","uriBC",uriBC.toString())
sm.getString("coyoteAdapter.debug","semicolon",String.valueOf(semicolon))
sm.getString("coyoteAdapter.debug","enc",charset.name())
sm.getString("coyoteAdapter.debug","pathParamStart",String.valueOf(pathParamStart))
sm.getString("coyoteAdapter.debug","pathParamEnd",String.valueOf(pathParamEnd))
sm.getString("coyoteAdapter.debug","pv",pv)
sm.getString("coyoteAdapter.debug","equals",String.valueOf(equals))
sm.getString("coyoteAdapter.debug","name",name)
sm.getString("coyoteAdapter.debug","value",value)
" Requested cookie session id is " + request.getRequestedSessionId()
sm.getString("coyoteRequest.sessionEndAccessFail"),t
sm.getString("request.fragmentInDispatchPath",path)
sm.getString("coyoteRequest.noAsync",StringUtils.join(getNonAsyncClassNames())),ise
sm.getString("coyoteRequest.gssLifetimeFail",userPrincipal.getName()),e
sm.getString("coyoteRequest.uploadCreate",location.getAbsolutePath(),getMappingData().wrapper.getName())
sm.getString("coyoteRequest.uploadCreateFail",location.getAbsolutePath())
sm.getString("inputBuffer.requiresNonBlocking")
sm.getString("globalResources.noNamingContext")
sm.getString("globalResources.createError"),e
"Creating MBeans for Global JNDI Resources in Context '" + prefix + "'"
"Checking resource " + name
sm.getString("globalResources.userDatabaseCreateError",name),e
sm.getString("globalResources.createError.runtime"),ex
sm.getString("globalResources.createError.operation"),ex
"Creating UserDatabase MBeans for resource " + name
"Database=" + database
"  Creating Role MBean for role " + role
"  Creating Group MBean for group " + group
"  Creating User MBean for user " + user
"Destroying MBeans for Global JNDI Resources"
sm.getString("mBeanDumper.getAttributeError",attName,oname),rme
sm.getString("mBeanDumper.getAttributeError",attName,oname),rme
sm.getString("mBeanDumper.getAttributeError",attName,oname),rme
sm.getString("mBeanDumper.getAttributeError",attName,oname),t
sm.getString("mBeanFactory.noDeployer",pname.getKeyProperty("host"))
sm.getString("mBeanFactory.noDeployer",hostName)
sm.getString("mBeanFactory.contextDestroyError"),e
sm.getString("jmxRemoteLifecycleListener.invalidSSLConfiguration"),e
sm.getString("jmxRemoteLifecycleListener.createRegistryFailed",serverName,Integer.toString(theRmiRegistryPort)),e
sm.getString("jmxRemoteLifecycleListener.invalidURL",serverName,url),e
sm.getString("jmxRemoteLifecycleListener.start",Integer.toString(theRmiRegistryPort),Integer.toString(theRmiServerPort),serverName)
sm.getString("jmxRemoteLifecycleListener.createServerFailed",serverName),e
sm.getString("jmxRemoteLifecycleListener.destroyServerFailed",serverName),e
sm.getString("jmxRemoteLifecycleListener.invalidRmiBindAddress",address),e
sm.getString("jmxRemoteLifecycleListener.invalidRmiBindAddress",address),e
sm.getString("extensionValidator.failload",item),e
sm.getString("extensionValidator.extension-validation-error",appName,failureCount + "")
sm.getString("extensionValidator.failload",filesLBRACKETiRBRACKET),e
sm.getString("lifecycleMBeanBase.registerFail",obj,name),e
sm.getString("lifecycleMBeanBase.registerFail",obj,name),e
sm.getString("introspection.classLoadFailed",className),e
sm.getString("introspection.classLoadFailed",className),t
sm.getString("sessionIdGeneratorBase.random",secureRandomClass),e
sm.getString("sessionIdGeneratorBase.randomAlgorithm",secureRandomAlgorithm),e
sm.getString("sessionIdGeneratorBase.randomProvider",secureRandomProvider),e
sm.getString("sessionIdGeneratorBase.randomAlgorithm",secureRandomAlgorithm),e
sm.getString("sessionIdGeneratorBase.createRandom",result.getAlgorithm(),Long.valueOf(t2SUBt1))
sm.getString("lifecycleBase.alreadyStarted",toString()),e
sm.getString("lifecycleBase.alreadyStarted",toString())
sm.getString("lifecycleBase.alreadyStopped",toString()),e
sm.getString("lifecycleBase.alreadyStopped",toString())
sm.getString("lifecycleBase.alreadyDestroyed",toString()),e
sm.getString("lifecycleBase.setState",this,state)
msg,t
msg
msg
sm.getString("authenticator.noAuthHeader")
sm.getString("spnegoAuthenticator.authHeaderNotNego")
sm.getString("spnegoAuthenticator.authHeaderNoToken")
sm.getString("spnegoAuthenticator.serviceLoginFail"),e
sm.getString("spnegoAuthenticator.ticketValidateFail")
sm.getString("spnegoAuthenticator.ticketValidateFail"),e
sm.getString("spnegoAuthenticator.serviceLoginFail"),e
sm.getString("spnegoAuthenticator.serviceLoginFail"),e
"Invalid Authorization" + iae.getMessage()
"Checking for reauthenticate in session " + session
"Reauthenticating username '" + username + "'"
"Reauthentication failed, proceed normally"
"Restore request from session '" + session.getIdInternal() + "'"
"Proceed to restored request"
"Restore of original request failed"
"Save request in session '" + session.getIdInternal() + "'"
"Request body too big to save during authentication"
"Authenticating username '" + username + "'"
"Authentication of '" + username + "' was successful"
"Redirecting to original '" + requestURI + "'"
sm.getString("formAuthenticator.forwardLogin",request.getRequestURI(),request.getMethod(),config.getLoginPage(),context.getName())
msg
msg,t
msg
msg,t
"Security checking request " + request.getMethod() + " " + request.getRequestURI()
"We have cached auth type " + session.getAuthType() + " for principal " + principal
" Not subject to any constraint"
" Calling hasUserDataPermission()"
" Failed hasUserDataPermission() test"
" Calling authenticate()"
" Failed authenticate() test"
" Calling accessControl()"
" Failed accessControl() test"
" Successfully passed all security constraints"
sm.getString("authenticator.jaspicSecureResponseFail"),e
sm.getString("authenticator.jaspicServerAuthContextFail"),e
sm.getString("authenticator.loginFail"),e
"Authenticated user: " + principal
sm.getString("authenticator.check.found",principal.getName())
sm.getString("authenticator.check.sso",ssoId)
sm.getString("authenticator.check.authorize",username)
sm.getString("authenticator.check.authorizeFail",username)
" Reauthenticated cached principal '" + request.getUserPrincipal().getName() + "' with auth type '" + request.getAuthType() + "'"
"Authenticated '" + name + "' with type '" + authType + "'"
sm.getString("authenticator.changeSessionId",oldId,session.getId())
sm.getString("authenticator.jaspicCleanSubjectFail"),e
sm.getString("authenticator.tomcatPrincipalLogoutFail"),t
"Found SingleSignOn Valve at " + sso
"No SingleSignOn Valve is present"
sm.getString("authConfigFactoryImpl.registerClass",className,layer,appContext)
sm.getString("authConfigFactoryImpl.registerInstance",provider.getClass().getName(),layer,appContext)
sm.getString("authConfigFactoryImpl.load",CONFIG_FILE.getAbsolutePath())
sm.getString("callbackHandlerImpl.jaspicCallbackMissing",callback.getClass().getName())
sm.getString("persistentProviderRegistrations.deleteFail",configFileNew.getAbsolutePath())
sm.getString("persistentProviderRegistrations.deleteFail",configFileOld.getAbsolutePath())
"Request Attribute: " + attr + ": [ " + req.getAttribute(attr) + "]"
"Character Encoding: [" + req.getCharacterEncoding() + "]"
"Content Length: [" + req.getContentLengthLong() + "]"
"Content Type: [" + req.getContentType() + "]"
"Locale: [" + locale + "]"
"Request Parameter: " + param + ":  [" + value + "]"
"Protocol: [" + req.getProtocol() + "]"
"Remote Address: [" + req.getRemoteAddr() + "]"
"Remote Host: [" + req.getRemoteHost() + "]"
"Scheme: [" + req.getScheme() + "]"
"Secure: [" + req.isSecure() + "]"
"Server Name: [" + req.getServerName() + "]"
"Server Port: [" + req.getServerPort() + "]"
"Auth Type: [" + req.getAuthType() + "]"
"Context Path: [" + req.getContextPath() + "]"
"Cookie: " + cookie.getName() + ": [" + cookie.getValue() + "]"
"HTTP Header: " + header + ": [" + req.getHeader(header) + "]"
"Method: [" + req.getMethod() + "]"
"Path Info: [" + req.getPathInfo() + "]"
"Path Translated: [" + req.getPathTranslated() + "]"
"Query String: [" + req.getQueryString() + "]"
"Remote User: [" + req.getRemoteUser() + "]"
"Requested Session ID: [" + req.getRequestedSessionId() + "]"
"Requested Session ID From Cookie: [" + req.isRequestedSessionIdFromCookie() + "]"
"Requested Session ID From URL: [" + req.isRequestedSessionIdFromURL() + "]"
"Requested Session ID Valid: [" + req.isRequestedSessionIdValid() + "]"
"Request URI: [" + req.getRequestURI() + "]"
"Servlet Path: [" + req.getServletPath() + "]"
"User Principal: [" + req.getUserPrincipal() + "]"
"ID: [" + session.getId() + "]"
"Creation Time: [" + NEWDate(session.getCreationTime()) + "]"
"Last Accessed Time: [" + NEWDate(session.getLastAccessedTime()) + "]"
"Max Inactive Interval: [" + session.getMaxInactiveInterval() + "]"
"Session Attribute: " + attr + ": [" + session.getAttribute(attr) + "]"
"Servlet Name: [" + getServletConfig().getServletName() + "]"
"Servlet Init Param: " + param + ": [" + value + "]"
"Major Version: [" + getServletContext().getMajorVersion() + "]"
"Minor Version: [" + getServletContext().getMinorVersion() + "]"
"Real Path for '/': [" + getServletContext().getRealPath("/") + "]"
"Server Info: [" + getServletContext().getServerInfo() + "]"
"Servlet Context Init Param: " + param + ": [" + value + "]"
"Servlet Context Attribute: " + attr + ": [" + getServletContext().getAttribute(attr) + "]"
cgiEnvLine
sm.getString("cgiServlet.invalidArgumentEncoded",encodedArgument,cmdLineArgumentsEncodedPattern.toString())
sm.getString("cgiServlet.invalidArgumentDecoded",decodedArgument,cmdLineArgumentsDecodedPattern.toString())
sm.getString("cgiServlet.find.path",pathInfo,webAppRootDir)
sm.getString("cgiServlet.find.location",currentLocation.getAbsolutePath())
sm.getString("cgiServlet.find.location",currentLocation.getAbsolutePath())
sm.getString("cgiServlet.find.found",name,path,scriptname,cginame)
sm.getString("cgiServlet.expandCreateDirFail",dir.getAbsolutePath())
sm.getString("cgiServlet.expandOk",srcPath,destPath)
sm.getString("cgiServlet.expandFail",srcPath,destPath),ioe
sm.getString("cgiServlet.expandDeleteFail",f.getAbsolutePath())
sm.getString("cgiServlet.expandCloseFail",srcPath),e
"envp: [" + env + "], command: [" + command + "]"
"addHeader(\"" + line + "\")"
sm.getString("cgiServlet.runBadHeader",line)
"output " + bufRead + " bytes of data"
sm.getString("cgiServlet.runFail"),e
sm.getString("cgiServlet.runHeaderReaderFail"),ioe
sm.getString("cgiServlet.runOutputStreamFail"),ioe
sm.getString("cgiServlet.runReaderInterrupt")
sm.getString("cgiServlet.runStdErr",line)
sm.getString("cgiServlet.runStdErrFail"),e
sm.getString("cgiServlet.runStdErrFail"),e
sm.getString("cgiServlet.runStdErrCount",Integer.valueOf(lineCount))
message
message
"decodePattern, pattern =" + pattern
sm.getString("extendedAccessLogValve.emptyPattern")
"token = " + token
"finished decoding with element size of: " + list.size()
sm.getString("extendedAccessLogValve.patternParseError",pattern),e
sm.getString("extendedAccessLogValve.decodeError",token)
sm.getString("extendedAccessLogValve.noClosing")
sm.getString("extendedAccessLogValve.decodeError",tokenizer.getRemains())
sm.getString("extendedAccessLogValve.noClosing")
sm.getString("extendedAccessLogValve.decodeError",tokenizer.getRemains())
sm.getString("extendedAccessLogValve.decodeError",token)
sm.getString("extendedAccessLogValve.badXParam")
sm.getString("extendedAccessLogValve.badXParam")
sm.getString("extendedAccessLogValve.noClosing")
sm.getString("extendedAccessLogValve.badXParamValue",token)
sm.getString("extendedAccessLogValve.badXParamValue",parameter)
sm.getString("accessLogValve.invalidLocale",name)
sm.getString("accessLogValve.invalidPortType",type)
sm.getString("remoteIpValve.invalidHostWithPort",hostHeaderValue,hostHeader)
sm.getString("remoteIpValve.invalidHostHeader",hostHeaderValue,hostHeader)
"Incoming request " + request.getRequestURI() + " with originalRemoteAddr [" + originalRemoteAddr + "], originalRemoteHost=[" + originalRemoteHost + "], originalSecure=[" + originalSecure + "], originalScheme=[" + originalScheme + "], originalServerName=[" + originalServerName + "], originalServerPort=[" + originalServerPort + "] will be seen as newRemoteAddr=[" + request.getRemoteAddr() + "], newRemoteHost=[" + request.getRemoteHost() + "], newSecure=[" + request.isSecure() + "], newScheme=[" + request.getScheme() + "], newServerName=[" + request.getServerName() + "], newServerPort=[" + request.getServerPort() + "]"
"Skip RemoteIpValve for request " + request.getRequestURI() + " with originalRemoteAddr '" + request.getRemoteAddr() + "'"
sm.getString("remoteIpValve.invalidPortHeader",portHeaderValue,portHeader),nfe
request.hashCode() + ": ClientIdentifier=" + clientIdentifier + ", RequestedSessionId=" + request.getRequestedSessionId()
request.hashCode() + ": UserAgent=" + uaHeader
request.hashCode() + ": Bot found. UserAgent=" + uaHeader
request.hashCode() + ": Bot found. IP=" + clientIp
request.hashCode() + ": SessionID=" + sessionId
request.hashCode() + ": New bot session. SessionID=" + s.getId()
request.hashCode() + ": Bot session accessed. SessionID=" + sessionId
sm.getString("sslValve.certError",strcerts),e
sm.getString("sslValve.invalidProvider",providerName),e
"Monitoring stuck threads with threshold = " + threshold + " sec"
msg,th
"thread interrupted after the request is finished, ignoring",e
msg,th
sm.getString("accessLogValve.deleteFail",file.getAbsolutePath())
sm.getString("accessLogValve.rotateFail"),e
sm.getString("accessLogValve.openDirFail",dir)
sm.getString("accessLogValve.openDirFail",parent)
sm.getString("accessLogValve.renameFail",rotatedLogFile,newLogFile)
sm.getString("accessLogValve.renameFail",rotatedLogFile,newLogFile),e
sm.getString("accessLogValve.renameFail",currentLogFile,newLogFile)
sm.getString("accessLogValve.renameFail",currentLogFile,newLogFile),e
sm.getString("accessLogValve.alreadyExists",currentLogFile,newLogFile)
sm.getString("accessLogValve.closeFail"),e
sm.getString("accessLogValve.writeFail",message.toString()),ioe
sm.getString("accessLogValve.unsupportedEncoding",encoding),ex
sm.getString("accessLogValve.openFail",pathname),e
t
"Could not close catalina properties file",ioe
"Must set '" + Globals.CATALINA_HOME_PROP + "' system property"
"Class loader creation threw exception",t
"Loading application class " + className
"Exception creating instance of " + className,t
"Identifying main() method"
"Exception locating main() method",t
"Calling main() method"
"Exception calling main() method",t
"Usage:  java org.apache.catalina.startup.Tool [<options>] <class> [<arguments>]"
sm.getString("engineConfig.cce",event.getLifecycle()),e
sm.getString("passwdUserDatabase.readFail"),e
sm.getString("hostConfig.cce",event.getLifecycle()),e
sm.getString("expandWar.copy",fileSrc,fileDest),e
sm.getString("expandWar.deleteFailed",dir.getAbsolutePath())
sm.getString("expandWar.deleteFailed",dir.getAbsolutePath())
sm.getString("listener.createFailed",className),e
sm.getString("listener.createFailed",className)
"Digester for server.xml created " + (t2SUBt1)
sm.getString("catalina.noCluster",e.getClass().getName() + ": " + e.getMessage()),e
sm.getString("catalina.noCluster",e.getClass().getName() + ": " + e.getMessage())
sm.getString("catalina.stopError"),e
sm.getString("catalina.stopError"),e
sm.getString("catalina.stopServer.connectException",s.getAddress(),String.valueOf(s.getPortWithOffset()),String.valueOf(s.getPort()),String.valueOf(s.getPortOffset()))
sm.getString("catalina.stopError"),ce
sm.getString("catalina.stopError"),e
sm.getString("catalina.stopServer")
sm.getString("catalina.configFail",file.getAbsolutePath()),e
sm.getString("catalina.incorrectPermissions")
sm.getString("catalina.initError"),e
sm.getString("catalina.init",Long.valueOf((t2SUBt1)SLASH1000000))
sm.getString("catalina.noServer")
sm.getString("catalina.serverStartFail"),e
"destroy() failed for failed Server ",e1
sm.getString("catalina.startup",Long.valueOf((t2SUBt1)SLASH1000000))
sm.getString("catalina.stopError"),e
sm.getString("embedded.notmp",temp)
sm.getString("catalina.noNatming")
"Setting naming prefix=" + value
"INITIAL_CONTEXT_FACTORY already set " + value
sm.getString("catalina.shutdownHookFail"),ex
sm.getString("versionLoggerListener.serverInfo.server.version",ServerInfo.getServerInfo())
sm.getString("versionLoggerListener.serverInfo.server.built",ServerInfo.getServerBuilt())
sm.getString("versionLoggerListener.serverInfo.server.number",ServerInfo.getServerNumber())
sm.getString("versionLoggerListener.os.name",System.getProperty("os.name"))
sm.getString("versionLoggerListener.os.version",System.getProperty("os.version"))
sm.getString("versionLoggerListener.os.arch",System.getProperty("os.arch"))
sm.getString("versionLoggerListener.java.home",System.getProperty("java.home"))
sm.getString("versionLoggerListener.vm.version",System.getProperty("java.runtime.version"))
sm.getString("versionLoggerListener.vm.vendor",System.getProperty("java.vm.vendor"))
sm.getString("versionLoggerListener.catalina.base",System.getProperty("catalina.base"))
sm.getString("versionLoggerListener.catalina.home",System.getProperty("catalina.home"))
sm.getString("versionLoggerListener.arg",arg)
sm.getString("versionLoggerListener.env",e.getKey(),e.getValue())
sm.getString("versionLoggerListener.prop",e.getKey(),e.getValue())
"Class loader creation threw exception",t
"Loading startup class"
"Setting startup class properties"
"Calling startup class " + method
"Bootstrap: command \"" + command + "\" does not exist."
sm.getString("connector.noSetExecutor",con)
sm.getString("connector.noSetSSLImplementationName",con)
sm.getString("contextConfig.cce",event.getLifecycle()),e
sm.getString("contextConfig.missingRealm")
sm.getString("contextConfig.authenticatorResources")
sm.getString("contextConfig.authenticatorMissing",loginConfig.getAuthMethod())
sm.getString("contextConfig.authenticatorInstantiate",authenticatorName),t
sm.getString("contextConfig.authenticatorConfigured",loginConfig.getAuthMethod())
sm.getString("contextConfig.badUrl",defaultContextXml),e
sm.getString("contextConfig.badUrl",hostContextFile),e
"Processing context [" + context.getName() + "] configuration file [" + contextXml + "]"
sm.getString("contextConfig.contextMissing",contextXml),e
"Successfully processed context [" + context.getName() + "] configuration file [" + contextXml + "]"
sm.getString("contextConfig.contextParse",context.getName()),e
sm.getString("contextConfig.defaultPosition","" + e.getLineNumber(),"" + e.getColumnNumber())
sm.getString("contextConfig.contextParse",context.getName()),e
sm.getString("contextConfig.contextClose"),e
"Anti locking context[" + context.getName() + "] setting docBase to " + antiLockingDocBase.getPath()
sm.getString("contextConfig.init")
sm.getString("contextConfig.fixDocBase",context.getName()),e
sm.getString("contextConfig.start")
sm.getString("contextConfig.xmlSettings",context.getName(),Boolean.valueOf(context.getXmlValidation()),Boolean.valueOf(context.getXmlNamespaceAware()))
"Pipeline Configuration:"
"  " + valvesLBRACKETiRBRACKET.getClass().getName()
"======================"
sm.getString("contextConfig.unavailable")
sm.getString("contextConfig.stop")
sm.getString("contextConfig.destroy")
sm.getString("contextConfig.role.auth",rolesLBRACKETjRBRACKET)
sm.getString("contextConfig.role.runas",runAs)
sm.getString("contextConfig.role.link",link)
sm.getString("contextConfig.effectiveWebXml",webXml.toXml())
"Skipping " + urlPattern + " , no servlet " + jspServletName
sm.getString("contextConfig.tomcatWebXmlError"),e
sm.getString("contextConfig.jspFile.warning",jspFile)
sm.getString("contextConfig.servletContainerInitializerFail",context.getName()),e
sm.getString("contextConfig.sci.debug",sci.getClass().getName()),e
sm.getString("contextConfig.sci.info",sci.getClass().getName())
sm.getString("contextConfig.resourceJarFail",url,context.getName())
sm.getString("contextConfig.resourceJarFail",url,context.getName())
sm.getString("contextConfig.altDDNotFound",altDDName)
sm.getString("contextConfig.applicationUrl")
sm.getString("contextConfig.applicationUrl")
sm.getString("contextConfig.applicationMissing") + " " + context
sm.getString("contextConfig.defaultError",filename,webXmlResource.getURI()),e
sm.getString("contextConfig.processAnnotationsWebDir.debug",webResource.getURL())
sm.getString("contextConfig.inputStreamWebResource",webResource.getWebappPath()),e
sm.getString("contextConfig.inputStreamWebResource",webResource.getWebappPath()),e
sm.getString("contextConfig.fileUrl",url),e
sm.getString("contextConfig.unknownUrlProtocol",url.getProtocol(),url)
sm.getString("contextConfig.processAnnotationsJar.debug",url)
sm.getString("contextConfig.inputStreamJar",entryName,url),e
sm.getString("contextConfig.inputStreamJar",entryName,url),e
sm.getString("contextConfig.jarFile",url),e
sm.getString("contextConfig.processAnnotationsDir.debug",file)
sm.getString("contextConfig.inputStreamFile",file.getAbsolutePath()),e
sm.getString("contextConfig.inputStreamFile",file.getAbsolutePath()),e
sm.getString("contextConfig.invalidSciHandlesTypes",className),e
sm.getString("contextConfig.invalidSciHandlesTypes",className),e
sm.getString("hostConfig.cce",event.getLifecycle()),e
sm.getString("hostConfig.ignorePath",appPath)
sm.getString("hostConfig.deployDescriptor.threaded.error"),e
sm.getString("hostConfig.deployDescriptor",contextXml.getAbsolutePath())
sm.getString("hostConfig.deployDescriptor.error",contextXml.getAbsolutePath()),e
sm.getString("hostConfig.deployDescriptor.path",context.getPath(),contextXml.getAbsolutePath())
sm.getString("hostConfig.deployDescriptor.hiddenWar",contextXml.getAbsolutePath(),war.getAbsolutePath())
sm.getString("hostConfig.deployDescriptor.hiddenDir",contextXml.getAbsolutePath(),dir.getAbsolutePath())
sm.getString("hostConfig.deployDescriptor.localDocBaseSpecified",docBase)
sm.getString("hostConfig.deployDescriptor.error",contextXml.getAbsolutePath()),t
sm.getString("hostConfig.deployDescriptor.finished",contextXml.getAbsolutePath(),Long.valueOf(System.currentTimeMillis()SUBstartTime))
sm.getString("hostConfig.deployWar.hiddenDir",dir.getAbsoluteFile(),war.getAbsoluteFile())
sm.getString("hostConfig.illegalWarName",filesLBRACKETiRBRACKET)
sm.getString("hostConfig.deployWar.threaded.error"),e
sm.getString("hostConfig.deployDescriptor.error",war.getAbsolutePath()),e
sm.getString("hostConfig.deployDescriptor.error",war.getAbsolutePath()),e
sm.getString("hostConfig.deployWar.error",war.getAbsolutePath()),t
sm.getString("hostConfig.deployWar",war.getAbsolutePath())
sm.getString("hostConfig.deployWar.error",war.getAbsolutePath()),t
sm.getString("hostConfig.deployWar.finished",war.getAbsolutePath(),Long.valueOf(System.currentTimeMillis()SUBstartTime))
sm.getString("hostConfig.deployDir.threaded.error"),e
sm.getString("hostConfig.deployDir",dir.getAbsolutePath())
sm.getString("hostConfig.deployDescriptor.error",xml),e
sm.getString("hostConfig.deployDir.error",dir.getAbsolutePath()),t
sm.getString("hostConfig.deployDir.finished",dir.getAbsolutePath(),Long.valueOf(System.currentTimeMillis()SUBstartTime))
"Ignoring non-existent WatchedResource '" + resource.getAbsolutePath() + "'"
"Watching WatchedResource '" + resource.getAbsolutePath() + "'"
"Checking context[" + app.name + "] redeploy resource " + resource
"Checking context[" + app.name + "] reload resource " + resource
sm.getString("hostConfig.reload",app.name)
sm.getString("hostConfig.context.restart",app.name),e
sm.getString("hostConfig.undeploy",app.name)
sm.getString("hostConfig.context.remove",app.name),t
"Delete " + current
"Delete " + current
sm.getString("hostConfig.resourceNotAbsolute",app.name,resource)
sm.getString("hostConfig.canonicalizing",resource.getParentFile(),app.name),e
sm.getString("hostConfig.canonicalizing",host.getAppBaseFile(),app.name),e
sm.getString("hostConfig.canonicalizing",host.getConfigBaseFile(),app.name),e
sm.getString("hostConfig.createDirs",dirsLBRACKETiRBRACKET)
sm.getString("hostConfig.start")
sm.getString("hostConfig.jmx.register",oname),e
sm.getString("hostConfig.appBase",host.getName(),host.getAppBaseFile().getPath())
sm.getString("hostConfig.stop")
sm.getString("hostConfig.jmx.unregister",oname),e
sm.getString("hostConfig.undeployVersion",previous.getName())
"Creating new class loader"
"  Including directory " + url
"  Including jar file " + file.getAbsolutePath()
"Creating new class loader"
"  Including URL " + url
"  Including directory " + url
"  Including jar file " + url
"  Including directory glob " + directory.getAbsolutePath()
"    Including glob jar file " + file.getAbsolutePath()
"  location " + i + " is " + arrayLBRACKETiRBRACKET
msg
msg
"Problem with JAR file [" + file + "], exists: [" + file.exists() + "], canRead: [" + file.canRead() + "]"
sm.getString("mapper.addHost.success",name)
sm.getString("mapper.addHost.sameHost",name)
sm.getString("mapper.duplicateHost",name,duplicate.getRealHostName())
sm.getString("mapper.addHostAlias.success",newAlias.name,newAlias.getRealHostName())
sm.getString("mapper.addHostAlias.sameHost",newAlias.name,newAlias.getRealHostName())
sm.getString("mapper.duplicateHostAlias",newAlias.name,newAlias.getRealHostName(),duplicate.getRealHostName())
sm.getString("mapper.addContext.noHost",hostName)
sm.getString("mapper.addContext.hostIsAlias",hostName)
sm.getString("mapper.findContext.noHostOrAlias",hostName)
sm.getString("mapper.findContext.noContext",contextPath)
sm.getString("mapper.findContext.noContextVersion",contextPath,version)
sm.getString("mapper.removeWrapper",context.name,path)
sm.getString("mapperListener.unknownDefaultHost",defaultHost,service)
sm.getString("mapperListener.registerHost",host.getName(),domain,service)
sm.getString("mapperListener.unregisterHost",hostname,domain,service)
sm.getString("mapperListener.unregisterWrapper",wrapperName,contextPath,service)
sm.getString("mapperListener.registerWrapper",container.getName(),contextPath,service)
sm.getString("mapperListener.registerContext",contextPath,service)
sm.getString("mapperListener.pauseContext",contextPath,service)
sm.getString("mapperListener.unregisterContext",contextPath,service)
sm.getString("mapperListener.registerWrapper",wrapper.getName(),contextPath,service)
msg
msg
msg
sm.getString("memoryUserDatabase.xmlFeatureEncoding"),e
sm.getString("memoryUserDatabase.fileNotFound",pathName)
sm.getString("memoryUserDatabase.readOnly")
sm.getString("memoryUserDatabase.notPersistable")
sm.getString("memoryUserDatabase.fileDelete",fileNew)
sm.getString("memoryUserDatabase.restoreOrig",fileOld)
sm.getString("memoryUserDatabase.reload",id,uri)
sm.getString("memoryUserDatabase.reloadError",id,uri),ioe
sm.getString("memoryUserDatabase.fileClose",pathname),ioe
sm.getString("pbeCredentialHandler.invalidKeySpec"),e
sm.getString("mdCredentialHandler.unknownEncoding",encodingName,encoding.name())
sm.getString("combinedRealm.authStart",username,realm.getClass().getName())
sm.getString("combinedRealm.authFail",username,realm.getClass().getName())
sm.getString("combinedRealm.authSuccess",username,realm.getClass().getName())
sm.getString("combinedRealm.authStart",username,realm.getClass().getName())
sm.getString("combinedRealm.authFail",username,realm.getClass().getName())
sm.getString("combinedRealm.authSuccess",username,realm.getClass().getName())
sm.getString("combinedRealm.authStart",username,realm.getClass().getName())
sm.getString("combinedRealm.authFail",username,realm.getClass().getName())
sm.getString("combinedRealm.authSuccess",username,realm.getClass().getName())
sm.getString("combinedRealm.realmStartFail",realm.getClass().getName()),e
sm.getString("combinedRealm.authStart",username,realm.getClass().getName())
sm.getString("combinedRealm.authFail",username,realm.getClass().getName())
sm.getString("combinedRealm.authSuccess",username,realm.getClass().getName())
sm.getString("realmBase.gssNameFail"),e
sm.getString("combinedRealm.authStart",username,realm.getClass().getName())
sm.getString("combinedRealm.authFail",username,realm.getClass().getName())
sm.getString("combinedRealm.authSuccess",username,realm.getClass().getName())
sm.getString("combinedRealm.unexpectedMethod"),uoe
sm.getString("combinedRealm.unexpectedMethod"),uoe
"MEMORY LOGIN MODULE"
"Abort"
"commit " + principal
"Init"
"login " + username + " " + principal
sm.getString("jaasMemoryLoginModule.noCatalinaBase",pathname)
sm.getString("jaasMemoryLoginModule.noConfig",file.getAbsolutePath())
sm.getString("jaasMemoryLoginModule.parseError",file.getAbsolutePath()),e
sm.getString("lockOutRealm.removeWarning",eldest.getKey(),Long.valueOf(timeInCache))
sm.getString("realmBase.gssNameFail"),e
sm.getString("memoryRealm.authenticateFailure",username)
sm.getString("memoryRealm.authenticateFailure",username)
sm.getString("memoryRealm.authenticateSuccess",username)
sm.getString("memoryRealm.authenticateFailure",username)
sm.getString("memoryRealm.xmlFeatureEncoding"),e
sm.getString("memoryRealm.loadPath",pathName)
"Digest : " + clientDigest + " Username:" + username + " ClientDigest:" + clientDigest + " nonce:" + nonce + " nc:" + nc + " cnonce:" + cnonce + " qop:" + qop + " realm:" + realm + "md5a2:" + md5a2 + " Server digest:" + serverDigest
"Authenticating client certificate chain"
" Checking validity for '" + certsLBRACKETiRBRACKET.getSubjectDN().getName() + "'"
"  Validity exception",e
sm.getString("realmBase.gssNameFail"),e
sm.getString("realmBase.delegatedCredentialFail",gssName),e
sm.getString("realmBase.credentialNotDelegated",gssName)
sm.getString("realmBase.gssContextNotEstablished")
"  No applicable constraints defined"
"  Checking constraint '" + constraintsLBRACKETiRBRACKET + "' against " + method + " " + uri + " --> " + constraintsLBRACKETiRBRACKET.included(uri,method)
"  Checking constraint '" + constraintsLBRACKETiRBRACKET + "' against " + method + " " + uri + " --> " + constraintsLBRACKETiRBRACKET.included(uri,method)
"  Checking constraint '" + constraintsLBRACKETiRBRACKET + "' against " + method + " " + uri + " --> " + constraintsLBRACKETiRBRACKET.included(uri,method)
"  Checking constraint '" + constraintsLBRACKETiRBRACKET + "' against " + method + " " + uri + " --> " + constraintsLBRACKETiRBRACKET.included(uri,method)
"  No applicable constraint located"
"  Checking roles " + principal
"Passing all authenticated users"
"No roles"
"Passing all access"
"  No user authenticated, cannot grant access"
"Role found:  " + rolesLBRACKETjRBRACKET
"No role found:  " + rolesLBRACKETjRBRACKET
"Checking for all roles mode: " + allRolesMode
"Granting access for role-name=*, auth-only"
"Granting access for role-name=*, strict auth-only"
sm.getString("realmBase.hasRoleSuccess",name,role)
sm.getString("realmBase.hasRoleFailure",name,role)
"  No applicable security constraint defined"
"  No applicable user data constraint defined"
"  User data constraint has no restrictions"
"  User data constraint already satisfied"
"  SSL redirect is disabled"
"  Redirecting to " + file.toString()
sm.getString("realmBase.gotX509Username",username)
sm.getString("jaasRealm.appName",appName)
sm.getString("jaasRealm.notPrincipal",classNamesLBRACKETiRBRACKET)
sm.getString("jaasRealm.classNotFound",classNamesLBRACKETiRBRACKET)
sm.getString("jaasRealm.beginLogin",username,appName)
sm.getString("jaasRealm.unexpectedError"),e
"Login context created " + username
sm.getString("jaasRealm.failedLogin",username)
sm.getString("jaasRealm.accountExpired",username)
sm.getString("jaasRealm.credentialExpired",username)
sm.getString("jaasRealm.failedLogin",username)
sm.getString("jaasRealm.loginException",username),e
sm.getString("jaasRealm.unexpectedError"),e
sm.getString("jaasRealm.loginContextCreated",username)
sm.getString("jaasRealm.authenticateFailure",username)
sm.getString("jaasRealm.authenticateSuccess",username,principal)
"error ",t
sm.getString("jaasRealm.checkPrincipal",principal,principalClass)
sm.getString("jaasRealm.userPrincipalSuccess",principal.getName())
sm.getString("jaasRealm.rolePrincipalAdd",principal.getName())
sm.getString("jaasRealm.userPrincipalFailure")
sm.getString("jaasRealm.rolePrincipalFailure")
sm.getString("jaasRealm.rolePrincipalFailure")
"Allocating non-STM instance"
"  Returning non-STM instance"
"  Returning allocated STM instance"
sm.getString("standardWrapper.jspMonitorError",instance)
sm.getString("standardWrapper.instantiate",servletClass),e
sm.getString("standardWrapper.waiting",countAllocated.toString(),getName())
sm.getString("standardWrapper.destroyInstance",getName()),t
sm.getString("standardService.engine.startFailed"),e
sm.getString("standardService.mapperListener.stopFailed"),e
sm.getString("standardService.mapperListener.startFailed"),e
sm.getString("standardService.engine.stopFailed"),e
sm.getString("standardService.connector.stopFailed",connectorsLBRACKETjRBRACKET),e
sm.getString("standardService.executor.start"),x
sm.getString("standardService.executor.stop"),e
sm.getString("standardService.start.name",this.name)
sm.getString("standardService.stop.name",this.name)
sm.getString("jreLeakListener.gcDaemonFail"),e
sm.getString("jreLeakListener.gcDaemonFail"),e
sm.getString("jreLeakListener.gcDaemonFail"),e
sm.getString("jreLeakListener.gcDaemonFail"),e
sm.getString("jreLeakListener.jarUrlConnCacheFail"),e
sm.getString("jreLeakListener.xmlParseFail"),e
sm.getString("jreLeakListener.ldapPoolManagerFail"),e
sm.getString("jreLeakListener.ldapPoolManagerFail"),e
sm.getString("jreLeakListener.classToInitializeFail",classNameToLoad),e
sm.getString("standardEngine.jvmRouteFail")
sm.getString("standardEngine.start",ServerInfo.getServerInfo())
sm.getString("standardServer.awaitSocket.fail",address,String.valueOf(getPortWithOffset()),String.valueOf(getPort()),String.valueOf(getPortOffset())),e
sm.getString("standardServer.accept.security"),ace
sm.getString("standardServer.accept.error"),e
sm.getString("standardServer.accept.readError"),e
sm.getString("standardServer.shutdownViaPort")
sm.getString("standardServer.invalidShutdownCommand",command.toString())
sm.getString("standardServer.storeConfig.notAvailable",sname)
sm.getString("standardServer.storeConfig.error"),t
sm.getString("standardServer.storeConfig.notAvailable",sname)
sm.getString("standardServer.storeConfig.contextError",context.getName()),t
sm.getString("standardServer.periodicEventError"),e
sm.getString("containerBase.cluster.stop"),e
sm.getString("containerBase.cluster.start"),e
sm.getString("containerBase.realm.stop"),e
sm.getString("containerBase.realm.start"),e
"Add child " + child + " " + this
sm.getString("containerBase.child.stop"),e
sm.getString("containerBase.child.destroy"),e
sm.getString("containerBase.threadedStartFailed"),e
sm.getString("containerBase.threadedStopFailed"),e
sm.getString("containerBase.backgroundProcess.cluster",cluster),e
sm.getString("containerBase.backgroundProcess.realm",realm),e
sm.getString("containerBase.backgroundProcess.valve",current),e
sm.getString("containerBase.backgroundProcess.error"),e
sm.getString("containerBase.backgroundProcess.error"),t
msg
sm.getString("aprListener.sslInit"),t
sm.getString("aprListener.aprDestroy")
sm.getString("aprListener.aprInitDebug",lnfe.getLibraryNames(),System.getProperty("java.library.path"),lnfe.getMessage()),lnfe
sm.getString("aprListener.aprInitError",t.getMessage()),t
sm.getString("aprListener.tcnInvalid",major + "." + minor + "." + patch,TCN_REQUIRED_MAJOR + "." + TCN_REQUIRED_MINOR + "." + TCN_REQUIRED_PATCH)
sm.getString("aprListener.currentFIPSMode",Integer.valueOf(fipsModeState))
sm.getString("aprListener.skipFIPSInitialization")
sm.getString("aprListener.initializingFIPS")
message
sm.getString("aprListener.initializeFIPSSuccess")
sm.getString("aprListener.initializedOpenSSL",SSL.versionString())
sm.getString("asyncContextImpl.onCompleteError",listener.getClass().getName()),t
sm.getString("asyncContextImpl.onTimeoutError",listener.getClass().getName()),t
sm.getString("asyncContextImpl.onStartAsyncError",listener.getClass().getName()),t
sm.getString("asyncContextImpl.onErrorError",listener.getClass().getName()),t2
msg,NEWDebugException()
msg
"Bound " + container
sm.getString("naming.namingContextCreationFailed",e)
sm.getString("naming.bindFailed",e)
sm.getString("naming.bindFailed",e)
"Creating JNDI naming context"
sm.getString("naming.bindFailed",e)
sm.getString("naming.bindFailed",e)
sm.getString("naming.bindFailed",e)
sm.getString("naming.invalidEnvEntryType",env.getName())
sm.getString("naming.invalidEnvEntryValue",env.getName())
sm.getString("naming.invalidEnvEntryValue",env.getName())
sm.getString("naming.addEnvEntry",env.getName())
sm.getString("naming.invalidEnvEntryValue",e)
"  Changing service ref wsdl file for /" + service.getWsdlfile()
sm.getString("naming.wsdlFailed",e)
"  Changing service ref jaxrpc file for /" + service.getJaxrpcmappingfile()
sm.getString("naming.wsdlFailed",e)
"  Adding service ref " + service.getName() + "  " + ref
sm.getString("naming.bindFailed",e)
"  Adding resource ref " + resource.getName() + "  " + ref
sm.getString("naming.bindFailed",e)
sm.getString("naming.jmxRegistrationFailed",e)
sm.getString("naming.addResourceEnvRef",resourceEnvRef.getName())
sm.getString("naming.bindFailed",e)
"  Adding resource link " + resourceLink.getName()
sm.getString("naming.bindFailed",e)
sm.getString("naming.unbindFailed",name),e
sm.getString("naming.unbindFailed",name),e
sm.getString("naming.unbindFailed",name),e
sm.getString("naming.unbindFailed",name),e
sm.getString("naming.unbindFailed",name),e
sm.getString("naming.unbindFailed",name),e
sm.getString("naming.unbindFailed",name),e
sm.getString("naming.unbindFailed",name),e
msg,e
msg,e
"Not renewing threads when the context is stopping. " + "It is not configured to do it."
sm.getString("standardHost.problematicAppBase",getName())
sm.getString("standardHost.invalidErrorReportValveClass",errorValve),t
sm.getString("standardContext.setLoader.stop"),e
sm.getString("standardContext.setLoader.start"),e
sm.getString("standardContext.setManager.stop"),e
sm.getString("standardContext.setManager.start"),e
sm.getString("standardContext.loginConfig.loginWarning",loginPage)
sm.getString("standardContext.loginConfig.errorWarning",errorPage)
sm.getString("standardContext.namingResource.destroy.fail"),e
sm.getString("standardContext.namingResource.init.fail"),e
sm.getString("standardContext.pathInvalid",path,this.path)
"Setting deployment descriptor public ID to '" + publicId + "'"
sm.getString("standardContext.workPath",getName()),e
sm.getString("standardContext.duplicateListener",listener)
sm.getString("standardContext.errorPage.warning",location)
sm.getString("standardContext.createWrapper.error"),t
sm.getString("standardContext.createWrapper.listenerError"),t
sm.getString("standardContext.createWrapper.containerListenerError"),t
sm.getString("standardContext.reloadingStarted",getName())
sm.getString("standardContext.stoppingContext",getName()),e
sm.getString("standardContext.startingContext",getName()),e
sm.getString("standardContext.reloadingCompleted",getName())
"Configuring application event listeners"
"Sending application stop events"
sm.getString("standardContext.resourcesStop"),t
"Starting " + getBaseName()
"Configuring default Resources"
sm.getString("standardContext.resourcesInit"),e
sm.getString("standardContext.extensionValidationError"),ioe
"Processing standard container startup"
sm.getString("standardContext.cluster.noManager",Boolean.valueOf((getCluster()BANGEQnull)),Boolean.valueOf(distributable))
sm.getString("standardContext.cluster.managerError"),ex
sm.getString("standardContext.manager",contextManager.getClass().getName())
sm.getString("standardContext.configurationFail")
sm.getString("standardContext.sciFail"),e
sm.getString("standardContext.listenerFail")
sm.getString("standardContext.managerFail"),e
sm.getString("standardContext.filterFail")
sm.getString("standardContext.servletFail")
"Starting completed"
sm.getString("standardContext.startFailed",getName())
sm.getString("standardContext.stop.asyncWaitInterrupted"),e
"Processing standard container shutdown"
"Error resetting context " + this + " " + ex,ex
"Stopping complete"
sm.getString("standardContext.backgroundProcess.loader",loader),e
sm.getString("standardContext.backgroundProcess.manager",manager),e
sm.getString("standardContext.backgroundProcess.resources",resources),e
sm.getString("standardContext.backgroundProcess.instanceManager",resources),e
"resetContext " + getObjectName()
sm.getString("standardContext.urlPattern.patternWarning",urlPattern)
sm.getString("standardContext.threadBindingListenerError",getName()),t
sm.getString("standardContext.threadBindingListenerError",getName()),t
sm.getString("standardContext.workCreateException",workDir,catalinaHomePath,getName()),e
sm.getString("standardContext.workCreateFail",dir,getName())
sm.getString("standardContext.suspiciousUrl",urlPattern,getName())
sm.getString("applicationFilterConfig.jmxRegisterFail",getFilterClass(),getFilterName()),ex
sm.getString("applicationFilterConfig.jmxUnregister",getFilterClass(),getFilterName())
sm.getString("applicationFilterConfig.jmxUnregisterFail",getFilterClass(),getFilterName()),ex
sm.getString("standardHost.clientAbort",realError.getCause().getMessage())
sm.getString("standardPipeline.basic.stop"),e
sm.getString("standardPipeline.basic.start"),e
sm.getString("standardPipeline.valve.start"),e
sm.getString("standardPipeline.valve.stop"),e
sm.getString("standardPipeline.valve.destroy"),e
sm.getString("jniLifecycleListener.load.name",libraryName)
sm.getString("jniLifecycleListener.load.path",libraryPath)
sm.getString(messageKey,resourceName)
sm.getString(messageKey,resourceName),ioe
sm.getString("defaultInstanceManager.restrictedWrongValue",resourceName,e.getKey(),e.getValue())
"Chat Error: " + t.toString(),t
"Chat Error: Failed to send message to client",e
"Caught to prevent timer from shutting down",e
"Unexpected exception: " + ex.toString(),ex
"Unexpected exception: " + ex.toString(),ex
"onError: " + t.toString(),t
"Unexpected exception: " + ex.toString(),ex
"Putting AsyncThread to sleep"
"Writing data."
"Async2",x
"Async2",x
"Async2",x
"Received dispatch, completing on the worker thread."
"After complete called started:" + req.isAsyncStarted()
"Putting AsyncThread to sleep"
"Dispatching"
"Async1",x
"Async1",x
"Putting AsyncThread to sleep"
"Dispatching to " + path
"Async1",x
"Async1",x
"AsyncStockServlet created"
statement
statement
"Re-running {} times to verify it isn't failing more often than it should.",rerunsOnFailure
"Test failed again, total num failures: {}",rerunsFailed,t
"Test failed in {} of the {} reruns.",rerunsFailed,rerunsOnFailure
"All reruns succeeded. Failure treated as flake."
"Simulating digest ACK response"
"OpenSSL not available in this application, so not testing the netty-openssl code paths"
"adding custom payload items {}",StringUtils.join(customPayload.keySet(),,)
"Loading legacy version: {}",legacyVersion
"Loading legacy version: {}",legacyVersion
"Loading legacy version: {}",legacyVersion
"Loading legacy version: {}",legacyVersion
"Loading legacy version: {}",legacyVersion
"Streaming legacy version {}",legacyVersion
"Truncating legacy version {}",legacyVersion
"Compacting legacy version {}",legacyVersion
"Preparing legacy version {}",legacyVersion
"for pk={} ck={}",pk,ck
"Read legacy_{}_clust_counter",legacyVersion
"Read legacy_{}_clust",legacyVersion
"Read legacy_{}_simple_counter",legacyVersion
"Read simple: legacy_{}_simple",legacyVersion
"Loading legacy table {}",table
"Seed {}",seed
"Seed : {}",seed
"forcing flush"
"begin manual replay"
"SSTables: {}",sstables
"Expected: {}",expected
"Originals: {}",result.txn.originals()
"Checking {}",sstable
"Originals: {}",result.txn.originals()
"Originals: {}",result.txn.originals()
"*** Note: java.util.UUID.compareTo() would have compared this differently"
"Corrupting sstable {} [{}] at pos {} / {}",currentSSTable,sstable.getFilename(),pos,raf.length()
"Boundaries for {} disks is {}",numDisks,Arrays.toString(boundaries)
"Index for SSTable {} on boundary {} is {}",reader.descriptor.generation,Arrays.toString(boundaries),index
"L1 or L2 has 0 sstables. Expected > 0 on both."
"L1: " + l1Count
"L2: " + l2Count
"expectCurrentCDCState violation! Expected state: {}. Found state: {}. Current CDC allocation: {}",expectedState,currentState,((CommitLogSegmentManagerCDC)CommitLog.instance.segmentManager).updateCDCTotalSize()
"Received matching message: {}",message
"Responding to intercepted message: {}",response
"seed: {}, decoder: {}",seed,decoder.getClass().getSimpleName()
"seed: {}, ratio: {}, version: {}",innerSeed,ratio,version
"seed: {}, iterations: {}, largeRatio: {}, messagingVersion: {}, decoder: {}",seed,count,largeRatio,messagingVersion,decoder.getClass().getSimpleName()
"inner seed: {}, iteration: {}",innerSeed,i
"Running {} {} -> {}",outbound.messagingVersion(),outbound.settings(),inboundSettings
"Protocol Version {} not supported by java driver",version
"Failed to cleanup and recreate directories."
"Failed to lookup host"
"Fatal exception in thread " + t,e
"Started Java Driver instance for protocol version {}",version
fullQuery
fullQuery
fullQuery
fullQuery
fullQuery
fullQuery
fullQuery
fullQuery
formattedQuery
formattedQuery
fullQuery
fullQuery
"Error performing schema change",e
"Executing: {} with values {}",query,formatAllValues(values)
"Executing: {}",query
"Got {} rows",rs.size()
"Extra row num {}: {}",i,str.toString()
"Error serializing query parameter {}:",value,ex
"Testing MV primary key: {}",mvPrimaryKeys.get(i)
"Testing MV primary key: {}",mvPrimaryKeys.get(i)
"Testing MV primary key: {}",mvPrimaryKeys.get(i)
"Testing MV primary key: {}",mvPrimaryKeys.get(i)
"Testing MV primary key: {}",mvPrimaryKeys.get(i)
"Testing MV primary key: {}",mvPrimaryKeys.get(i)
"Testing MV primary key: {}",mvPrimaryKeys.get(i)
"Testing MV primary key: {}",mvPrimaryKeys.get(i)
"Testing MV primary key: {}",mvPrimaryKeys.get(i)
"Testing MV primary key: {}",mvPrimaryKeys.get(i)
"No beta version found for testing"
"No beta version found for testing"
"Ranges to fetch with preferred endpoints"
result.toString()
"Ranges to stream by endpoint"
result.toString()
"Event time ranges are disjunctive - log invocations were made one after another"
"Time ranges {}, {}, disjunctive={}",firstThreadTimeRange,secondThreadTimeRange,disjunctive
"Both event lists contain more than one entry. First = {}, Second = {}",firstThreadEvents,secondThreadEvents
"No status log was recorded. First = {}, Second = {}",firstThreadEvents,secondThreadEvents
"Checking if logger was busy. First = {}, Second = {}",firstThreadEvents,secondThreadEvents
statement,param)
statement,param)
statement,param)
statement,param)
statement,param)
statement,param)
statement,param)
"Removed index entry for stale value {}",indexKey
"Inserted entry into index for value {}",valueKey
"Removed index entry for value {}",indexKey
"No SSTable data for {}.{} to build index {} from, marking empty index as built",baseCfs.metadata.keyspace,baseCfs.metadata.name,metadata.name
"Submitting index build of {} for data in {}",metadata.name,getSSTableNames(sstables)
"Index build of {} complete",metadata.name
"... everything looks good for {}",cfs.name
"adding node {} at {}",address,token
"{}: {}",testToken,replicas
"Bloom filter false positive: {}",fp
"Bloom filter false positive: {}",fp
"Bloom filter false positive: {}",fp
"Bloom filter mean false positive: {}",sumfpSLASH10
"Unexpected exception",t
"Syncing connections: {}",connections
"Sync'd connections: {}",connections
"{}",String.format(message,params)
"Connection: {}",currentConnection
"{}",String.format(message,params),t
"Connection: {}",currentConnection
"{}",String.format(message,params)
"Unexpected error:",t
String.format("%s: Executed %d barriers with %d operations. %.0f%% complete.",Thread.currentThread().getName(),count,opCount,100STAR(1SUB((untilSUBnow)SLASH(double)RUNTIME)))
"Received size changed after barrier finished: {} vs {}",checkCount,totalCount()
"Received an operation that was created after the barrier was issued."
"Missing registered operations. {} vs {}",TestOrdering.this.count.get(e.getKey()).intValue(),e.getValue().intValue()
"Overriding configured BufferPool.MEMORY_USAGE_THRESHOLD={} and enabling BufferPool.DEBUG",poolSize
"All threads did not free all memory in this time slot - skipping buffer recycle check"
"Reverting BufferPool.MEMORY_USAGE_THRESHOLD={}",prevPoolSize
"Got exception {}, current chunk {}",ex.getMessage(),BufferPool.unsafeCurrentChunk()
"Got throwable {}, current chunk {}",tr.getMessage(),BufferPool.unsafeCurrentChunk()
"Using seed {}",seed
"Looking for dtest jars in " + NEWFile("build").getAbsolutePath()
"Found " + e.getValue().stream().map(vARROWv.version).collect(Collectors.joining(", "))
"Attempting to configure full query logger path: {} Roll cycle: {} Blocking: {} Max queue weight: {} Max log size:{}, archive command: {}",path,rollCycle,blocking,maxQueueWeight,maxLogSize,archiveCommand
"Full query logger already configured. Ignoring requested configuration."
"Cleaning directory: {} as requested",path
"Reset (and deactivation) of full query log requested."
"Stopping full query log. Cleaning {}.",pathsToClean
"Full query log already deactivated. Cleaning {}.",pathsToClean
"Deactivation of full query log requested."
"Stopping full query log"
"Audit logging is enabled."
"Audit logging is disabled."
"Loading AuditLog filters"
"Prepared AEService trees of size {} for {}",trees.size(),desc
"Validated {} partitions for {}.  Partition sizes are:",validated,desc.sessionId
"Failed creating a merkle tree for {}, {} (see log for details)",desc,initiator
"{} Sending completed merkle tree to {} for {}.{}",previewKind.logPrefix(desc.sessionId),initiator,desc.keyspace,desc.columnFamily
"{} Local completed merkle tree for {} for {}.{}",previewKind.logPrefix(desc.sessionId),initiator,desc.keyspace,desc.columnFamily
"Failed to move local merkle tree for {} off heap",desc,e
"Preparing, {}",prepareMessage
"Snapshotting {}",desc
"Enqueuing response to snapshot request {} to {}",desc.sessionId,message.from()
"Validating {}",validationRequest
"Table {}.{} was dropped during snapshot phase of repair",desc.keyspace,desc.columnFamily
"Syncing {}",request
"Syncing {}",asymmetricSyncRequest
"cleaning up repair"
"Got error, removing parent repair session"
errorMessage
"[streaming task #{}] Performing streaming repair of {} ranges with {}",desc.sessionId,ranges.size(),dst
"[repair #{}] streaming task succeed, returning response to {}",desc.sessionId,initiator
"{} {} is fully synced",session.previewKind.logPrefix(session.getId()),desc.columnFamily
"{} {} sync failed",session.previewKind.logPrefix(session.getId()),desc.columnFamily
"diffs = {}",diffHolder
"{} is about to fetch {} from {}",address,toFetch,fetchFrom
"Node {} has nothing to stream",address
"{} {}",session.previewKind.logPrefix(desc.sessionId),message
"{} {}",session.previewKind.logPrefix(desc.sessionId),message
"Validating {}",address
"Validating {}",nextAddress
"{} {}",session.previewKind.logPrefix(desc.sessionId),message
"Validating {}",address
"Validating {}",nextAddress
"force flag set, removing dead endpoints"
"Removing a dead node from Repair due to -force {}",endpoint
"{} {}",previewKind.logPrefix(getId()),message
"{} Repair completed between {} and {} on {}",previewKind.logPrefix(getId()),nodes.coordinator,nodes.peer,desc.columnFamily
"{} new session: will sync {} on range {} for {}.{}",previewKind.logPrefix(getId()),repairedNodes(),commonRange,keyspace,Arrays.toString(cfnames)
"{} {}",previewKind.logPrefix(getId()),messageEQString.format("No neighbors to repair with on range %s: session completed",commonRange)
"{} {}",previewKind.logPrefix(getId()),message
"{} Session completed with the following error",previewKind.logPrefix(getId()),t
"{} session completed with the following error",previewKind.logPrefix(getId()),exception
"Validation of {} partitions (~{}) finished in {} msec, for {}",partitionCount,FBUtilities.prettyPrintMemory(estimatedTotalBytes),duration,validator.desc
"Error executing query " + fmtQry,t
"{} {}",previewKind.logPrefix(desc.sessionId),message
"{} {}",previewKind.logPrefix(desc.sessionId),message
"{} {}",previewKind.logPrefix(desc.sessionId),message
String.format(format,"are consistent")
"Repair {} failed:",parentSession,e
message
"Repair {} failed:",parentSession,e
"Repair {} failed:",parentSession,e
"Repair {} failed:",parentSession,t
"Repair result: {}",results
"Starting preview repair for {}",parentSession
message
message
"Error completing preview repair",t
"Error completing preview repair",t
"Preview repair {} completed",parentSession
"Starting RepairSession for {}",commonRange
message
message,t
message
"Setting coordinator state to {} for repair {}",state,sessionID
"Setting participant {} to state {} for repair {}",participant,state,sessionID
"Sending {} to {}",message,destination
"Beginning prepare phase of incremental repair session {}",sessionID
"{} failed the prepare phase for incremental repair session {}",participant,sessionID
"Successful prepare response received from {} for repair session {}",participant,sessionID
"Incremental repair session {} successfully prepared.",sessionID
"Proposing finalization of repair session {}",sessionID
"Incremental repair {} has failed, ignoring finalize promise from {}",sessionID,participant
"Finalization proposal of session {} rejected by {}. Aborting session",sessionID,participant
"Successful finalize promise received from {} for repair session {}",participant,sessionID
"Finalization proposal for repair session {} accepted by all participants.",sessionID
"Committing finalization of repair session {}",sessionID
"Incremental repair session {} completed",sessionID
"Incremental repair session {} failed",sessionID
"Beginning coordination of incremental repair session {}",sessionID
"Incremental repair {} prepare phase completed in {}",sessionID,formatDuration(sessionStart,repairStart)
"Incremental repair {} validation/stream phase completed in {}",sessionID,formatDuration(repairStart,finalizeStart)
"Incremental repair {} finalization phase completed in {}",sessionID,formatDuration(finalizeStart,System.currentTimeMillis())
"Incremental repair {} phase completed in {}",sessionID,formatDuration(sessionStart,System.currentTimeMillis())
"Incremental repair {} phase failed in {}",sessionID,formatDuration(sessionStart,System.currentTimeMillis())
"Cancelling local repair session {}",sessionID
"Unable to load malformed repair session {}, ignoring",row.has("parent_id")QUESrow.getUUID("parent_id"):null
"Running LocalSessions.cleanup"
"node not initialized, aborting local session cleanup"
"Auto failing timed out repair session {}",session
"Auto deleting repair session {}",session
"Skipping delete of LocalSession {} because it still contains sstables",session.sessionID
"sending {} to {}",message,destination
"Changing LocalSession state from {} -> {} for {}",session.getState(),state,session.sessionID
"Failing local repair session {}",sessionID
"Deleting local repair session {}",sessionID
"received {} from {}",request,from
"Error retrieving ParentRepairSession for session {}, responding with failure",sessionID
"Beginning local incremental repair session {}",session
"Prepare phase for incremental repair session {} completed",sessionID
"Session {} failed before anticompaction completed",sessionID
"Prepare phase for incremental repair session {} failed",sessionID,t
"Setting local incremental repair session {} to REPAIRING",session
"received {} from {}",propose,from
"Received FinalizePropose message for unknown repair session {}, responding with failure",sessionID
"Received FinalizePropose message for incremental repair session {}, responded with FinalizePromise",sessionID
String.format("Error handling FinalizePropose message for %s",session),e
"received {} from {}",commit,from
"Ignoring FinalizeCommit message for unknown repair session {}",sessionID
"Finalized local repair session {}",sessionID
"received {} from {}",msg,from
"Attempting to learn the outcome of unfinished local incremental repair session {}",session.sessionID
"received {} from {}",request,from
"Received status response message for unknown session {}",sessionID
"Responding to status response message for incremental repair session {} with local state {}",sessionID,session.getState()
"received {} from {}",response,from
"Received StatusResponse message for unknown repair session {}",sessionID
"Unfinished local incremental repair session {} set to state {}",sessionID,response.state
"Received StatusResponse for repair session {} with state {}, which is not actionable. Doing nothing.",sessionID,response.state
"Sequential repair disabled when memory-mapped I/O is configured on Windows. Reverting to parallel."
"Denormalizing range={} incoming={}",range,incoming
"denormalized {} to {}",range,newInput
"denormalized incoming to {}",incoming
"adding incoming range {} from {}",range,streamFromNode
"Configuration location: {}",url
"Loading settings from {}",url
"Node configuration:[{}]",Joiner.on("; ").join(configMap.entrySet())
"Syncing log with batch mode"
"Syncing log with a group window of {}",conf.commitlog_sync_period_in_ms
"Syncing log with a period of {}",conf.commitlog_sync_period_in_ms
"DiskAccessMode 'auto' determined to be {}, indexAccessMode is {}",conf.disk_access_mode,indexAccessMode
"DiskAccessMode is {}, indexAccessMode is {}",conf.disk_access_mode,indexAccessMode
"DiskAccessMode is {}, indexAccessMode is {}",conf.disk_access_mode,indexAccessMode
"concurrent_replicates has been deprecated and should be removed from cassandra.yaml"
"Global memtable on-heap threshold is enabled at {}MB",conf.memtable_heap_space_in_mb
"Global memtable off-heap threshold is disabled, HeapAllocator will be used instead"
"Global memtable off-heap threshold is enabled at {}MB",conf.memtable_offheap_space_in_mb
"repair_session_max_tree_depth has been deprecated and should be removed from cassandra.yaml. Use repair_session_space_in_mb instead"
"repair_session_max_tree_depth of " + conf.repair_session_max_tree_depth + " > 20 could lead to excessive memory usage"
"A repair_session_space_in_mb of " + conf.repair_session_space_in_mb + " megabytes is likely to cause heap pressure"
"Error checking disk space",e
"Small commitlog volume detected at {}; setting commitlog_total_space_in_mb to {}.  You can override this in cassandra.yaml",conf.commitlog_directory,minSize
"Error checking disk space",e
"Small cdc volume detected at {}; setting cdc_total_space_in_mb to {}.  You can override this in cassandra.yaml",conf.cdc_raw_directory,minSize
"cdc_enabled is true. Starting casssandra node with Change-Data-Capture enabled."
"Error checking disk space",e
"memtable_cleanup_threshold has been deprecated and should be removed from cassandra.yaml"
"memtable_cleanup_threshold is set very low [{}], which may cause performance degradation",conf.memtable_cleanup_threshold
"Internode encryption enabled. Disabling zero copy SSTable transfers for streaming."
"Back-pressure is {} with strategy {}.",backPressureEnabled()QUES"enabled":"disabled",conf.back_pressure_strategy
"found {}::{} less than lowest acceptable value {}, continuing with {}",property,actualValue,lowestAcceptedValue,lowestAcceptedValue
"repair_session_max_tree_depth of " + depth + " > 20 could lead to excessive memory usage"
"A repair_session_space_in_mb of " + conf.repair_session_space_in_mb + " megabytes is likely to cause heap pressure."
"Changing automatic_sstable_upgrade to {}",enabled
"Changing max_concurrent_automatic_sstable_upgrades to {}",value
"max_concurrent_automatic_sstable_upgrades ({}) is larger than concurrent_compactors ({})",value,getConcurrentCompactors()
"Setting use_offheap_merkle_trees to {}",value
"Reading token of {}",FBUtilities.prettyPrintMemory(size)
"Ignoring shutdown message from {} because gossip is disabled",message.from()
"Overriding max local pause time to {}ms",pause
"Overriding FD INITIAL_VALUE to {}ms",newvalue
"Unknown endpoint: " + ep,NEWIllegalArgumentException("")
"Average for {} is {}ns",ep,heartbeatWindow.mean()
"Not marking nodes down due to local pause of {}ns > {}ns",diff,MAX_LOCAL_PAUSE_IN_NANOS
"Still not marking nodes down due to local pause"
"PHI for {} : {}",ep,phi
"Node {} phi {} > {}; intervals: {} mean: {}ns",NEWObjectLBRACKETRBRACKETLBRACEep,PHI_FACTORSTARphi,getPhiConvictThreshold(),hbWnd,hbWnd.mean()RBRACE
"PHI for {} : {}",ep,phi
"PHI for {} : {}",ep,phi
"mean for {} : {}ns",ep,hbWnd.mean()
"Forcing conviction of {}",ep
"Overriding FD MAX_INTERVAL to {}ms",newvalue
"Reporting interval time of {}ns for {}",interArrivalTime,ep
"Ignoring interval time of {}ns for {}",interArrivalTime,ep
"Received a GossipDigestAck2Message from {}",from
"Ignoring GossipDigestAck2Message because gossip is disabled"
"Received a GossipDigestSynMessage from {}",from
"Ignoring GossipDigestSynMessage because gossip is disabled"
"ClusterName mismatch from {} {}!={}",from,gDigestMessage.clusterId,DatabaseDescriptor.getClusterName()
"Partitioner mismatch from {} {}!={}",from,gDigestMessage.partioner,DatabaseDescriptor.getPartitionerName()
"Ignoring non-empty GossipDigestSynMessage because currently in gossip shadow round"
"Received a shadow round syn from {}. Gossip is disabled but " + "currently also in shadow round, responding with a minimal ack",from
"Gossip syn digests are : {}",sb
"sending {} digests and {} deltas",deltaGossipDigestList.size(),deltaEpStateMap.size()
"Sending a GossipDigestAckMessage to {}",from
"My heartbeat is now {}",endpointStateMap.get(FBUtilities.getBroadcastAddressAndPort()).getHeartBeatState().getHeartBeatVersion()
"Gossip error",e
"Convicting {} with status {} - alive {}",endpoint,getGossipStatus(epState),epState.isAlive()
"evicting {} from gossip",endpoint
"removed {} from seeds, updated seeds list = {}",endpoint,seeds
"removing endpoint {}",endpoint
"Gossip Digests are : {}",sb
"Removing host: {}",hostId
"Sleeping for {}ms to ensure {} does not change",StorageService.RING_DELAY,endpoint
"Completing removal of {}",endpoint
"Gossiper.unsafeAssassinateEndpoint is deprecated and will be removed in the next release; use assassinateEndpoint instead"
"Assassinating {} via gossip",endpoint
"Sleeping for {}ms to ensure {} does not change",StorageService.RING_DELAY,endpoint
"Endpoint {} disappeared while trying to assassinate, continuing anyway",endpoint
"Finished assassinating {}",endpoint
"Sending a GossipDigestSyn to {} ...",to
"Performing status check ..."
"Gossip stage has {} pending tasks; skipping status check (no nodes will be marked down)",pending
"FatClient {} has been silent for {}ms, removing from gossip",endpoint,fatClientTimeout
"time is expiring for endpoint : {} ({})",endpoint,expireTime
"{} elapsed, {} gossip quarantine over",QUARANTINE_DELAY,entry.getKey()
"local heartbeat version {} greater than {} for {}",localHbVersion,version,forEndpoint
"Adding state {}: {}",key,value.value
"Clearing interval times for {} due to generation change",endpoint
"Sending ECHO_REQ to {}",addr
"marking as alive {}",addr
"removing expire time for endpoint : {}",addr
"InetAddress {} is now UP",addr
"Notified {}",subscribers
"marking as down {}",addr
"InetAddress {} is now DOWN",addr
"Notified {}",subscribers
"Node {} has restarted, now UP",ep
"Node {} is now part of the cluster",ep
"Adding endpoint state for {}",ep
"Not marking {} alive due to dead state",ep
"Ignoring gossip for {} because it is quarantined",ep
"{} local generation {}, remote generation {}",ep,localGeneration,remoteGeneration
"Updating heartbeat state generation to {} from {} for {}",remoteGeneration,localGeneration,ep
"Ignoring remote version {} <= {} for {}",remoteMaxVersion,localMaxVersion,ep
"Ignoring remote generation {} < {}",remoteGeneration,localGeneration
"Updating heartbeat state version to {} from {} for {} ...",localState.getHeartBeatState().getHeartBeatVersion(),oldVersion,addr
"Updating {} state version to {} for {}",entry.getKey().toString(),entry.getValue().version,addr
"requestAll for {}",gDigest.getEndpoint()
"gossip started with generation {}",localState.getHeartBeatState().getGeneration()
"Sending shadow round GOSSIP DIGEST SYN to known peers {}",peers
"Triggering reload of seed node list"
"Error while getting seed node list: {}",e.getLocalizedMessage()
"New seed node list is empty. Not updating seed list."
"New seed node list matches the existing list."
"New seed node list after reload {}",seeds
"Attempt to add self as saved endpoint"
"not replacing a previous epState for {}, but reusing it: {}",ep,epState
"Adding saved endpoint {} {}",ep,epState.getHeartBeatState().getGeneration()
"Announcing shutdown"
"No local state, state is in silent shutdown, or node hasn't joined, not announcing shutdown"
"Received an ack from {}, who isn't a seed. Ensure your seed list includes a live node. Exiting shadow round",respondent
"Received a regular ack from {}, can now exit shadow round",respondent
"All seeds are in a shadow round, clearing this node to exit its own"
"adding expire time for endpoint : {} ({})",endpoint,expireTime
"Waiting for gossip to settle..."
"Gossip looks settled."
"Gossip not settled after {} polls.",totalPolls
"Gossip not settled but startup forced by cassandra.skip_wait_for_gossip_to_settle. Gossip total polls: {}",totalPolls
"Gossip settled after {} extra polls; proceeding",totalPollsSUBGOSSIP_SETTLE_POLL_SUCCESSES_REQUIRED
"No gossip backlog; proceeding"
"Received a GossipDigestAckMessage from {}",from
"Ignoring GossipDigestAckMessage because gossip is disabled"
"Received ack with {} digests and {} states",gDigestList.size(),epStateMap.size()
"Received an ack from {}, which may trigger exit from shadow round",from
"Ignoring unrequested GossipDigestAck from {}",from
"Sending a GossipDigestAck2Message to {}",from
"Certificate for {} expired on {}",alias,expires
"Filtering out {} as it isn't supported by the socket",Iterables.toString(missing)
"Checking whether certificates have been updated {}",hotReloadableFiles
"SSL certificates have been updated. Reseting the ssl contexts for new connections."
"Failed to hot reload the SSL Certificates! Please check the certificate files.",e
"Initializing hot reloading SSLContext"
"initializing CipherFactory"
"loading secret key for alias {}",alias
"could not build cipher",e
"initializing keystore from file {}",options.get(PROP_KEYSTORE)
"Cannot fetch in memory data, we will fallback to read from disk ",e
"Completed loading ({} ms; {} keys) {} cache",TimeUnit.NANOSECONDS.toMillis(System.nanoTime()SUBstart),CacheService.instance.keyCache.size(),cacheType
"reading saved cache {}",dataPath
String.format("Non-fatal checksum error reading saved cache %s",dataPath.getAbsolutePath()),e
String.format("Harmless error reading saved cache %s",dataPath.getAbsolutePath()),t
"completed reading ({} ms; {} keys) saved cache {}",TimeUnit.NANOSECONDS.toMillis(System.nanoTime()SUBstart),count,dataPath
"Deleting old {} files.",cacheType
"Skipping {} save, cache is empty.",cacheType
"Unable to rename {} to {}",cacheFilePaths.left,cacheFile
"Unable to rename {} to {}",cacheFilePaths.right,crcFile
"Saved {} ({} items) in {} ms",cacheType,keysWritten,TimeUnit.NANOSECONDS.toMillis(System.nanoTime()SUBstart)
"Failed to delete {}",file.getAbsolutePath()
"Could not list files in {}",savedCachesDir
"Exception in response",cause
"Discarded request of size: {}. InflightChannelRequestPayload: {}, InflightEndpointRequestPayload: {}, InflightOverallRequestPayload: {}, Request: {}",frameSize,channelPayloadBytesInFlight,endpointAndGlobalPayloadsInFlight.endpoint().using(),endpointAndGlobalPayloadsInFlight.global().using(),request
"Received: {}, v={}",request,connection.getVersion()
"Responding: {}, v={}",response,connection.getVersion()
"Enabling optionally encrypted CQL connections between client and server"
"Enabling encrypted CQL connections between client and server"
"Starting listening for CQL clients on {} ({})...",socket,this.useSSLQUES"encrypted":"unencrypted"
"Stop listening for CQL clients"
"Closing client connection {} after timeout of {}ms",channel.remoteAddress(),idleTimeout
"Sending event for endpoint {}, rpc address {}",endpoint,event.nodeAddress()
"Topology changed event : {}, {}",endpoint,event.change
"Status changed event : {}, {}",endpoint,event.status
"Failed to get peer certificates for peer {}",channel().remoteAddress(),e
"Received invalid message for supported protocol version {}",version
"Unexpected error during query",e
"Unexpected exception during request",e
"Invalid compression/checksum options supplied. %s / %s",checksumType,compressor.getClass().getName()
"IO error during compression of frame body chunk",e
"IO error during decompression of frame body chunk",e
"Adding <{}> to trace events",message
"Waiting for up to {} seconds for {} trace events to complete", + WAIT_FOR_PENDING_EVENTS_TIMEOUT_SECS,pendingFutures.size()
"Failed to wait for tracing events to complete in {} seconds",WAIT_FOR_PENDING_EVENTS_TIMEOUT_SECS
"Got exception whilst waiting for tracing events to complete",t
"Failed to insert pending future, tracing synchronization may not work"
"Too many nodes are overloaded to save trace events"
"Using {} as tracing queries (as requested with -Dcassandra.custom_tracing_class)",customTracingClass
String.format("Cannot use class %s for tracing, ignoring by defaulting to normal tracing",customTracingClass),e
"request complete"
"failed to capture the tracing info for an outbound message to {}, ignoring",sendTo,e
"min_index_interval of {} is too low for {} expected keys of avg size {}; using interval of {} instead",minIndexInterval,expectedKeys,defaultExpectedKeySize,effectiveMinInterval
"Failed to delete snapshot [{}]. Will retry after further sstable deletions. Folder will be deleted on JVM shutdown or next node restart on crash.",path
"Successfully deleted snapshot {}.",path
"Redistributing index summaries"
"Beginning redistribution of index summaries for {} sstables with memory pool size {} MB; current spaced used is {} MB",redistribute.size(),memoryPoolBytesSLASH1024SLASH1024,totalSLASH1024.0SLASH1024.0
"Total reads/sec across all sstables in index summary resize process: {}",totalReadsPerSec
"Index summaries for compacting SSTables are using {} MB of space",(memoryPoolBytesSUBremainingBytes)SLASH1024.0SLASH1024.0
"Completed resizing of index summaries; current approximate memory used: {}",FBUtilities.prettyPrintMemory(total)
"min_index_interval changed from {} to {}, so the current sampling level for {} is effectively now {} (was {})",sstable.getMinIndexInterval(),minIndexInterval,sstable,effectiveSamplingLevel,currentSamplingLevel
"{} has {} reads/sec; ideal space for index summary: {} ({} entries); considering moving " + "from level {} ({} entries, {}) " + "to level {} ({} entries, {})",sstable.getFilename(),readsPerSec,FBUtilities.prettyPrintMemory(idealSpace),targetNumEntries,currentSamplingLevel,currentNumEntries,FBUtilities.prettyPrintMemory((long)(currentNumEntriesSTARavgEntrySize)),newSamplingLevel,numEntriesAtNewSamplingLevel,FBUtilities.prettyPrintMemory((long)(numEntriesAtNewSamplingLevelSTARavgEntrySize))
"Re-sampling index summary for {} from {}/{} to {}/{} of the original number of entries",sstable,sstable.getIndexSummarySamplingLevel(),Downsampling.BASE_SAMPLING_LEVEL,entry.newSamplingLevel,Downsampling.BASE_SAMPLING_LEVEL
"Using leftover space to keep {} at the current sampling level ({})",entry.sstable,entry.sstable.getIndexSummarySamplingLevel()
"Initializing index summary manager with a memory pool size of {} MB and a resize interval of {} minutes",indexSummarySizeInMB,interval
"Got exception during index summary redistribution",e
"Deleting sstable: {}",desc
"Missing component: {}",descriptor.filenameFor(component)
"Load metadata for {}",descriptor
"No sstable stats for {}",descriptor
"Mutating {} to level {}",descriptor.filenameFor(Component.STATS),newLevel
"Mutating {} to repairedAt time {} and pendingRepair {}",descriptor.filenameFor(Component.STATS),newRepairedAt,newPendingRepair
"Reading cardinality from Statistics.db failed for {}",sstable.getFilename()
"Reading cardinality from Statistics.db failed.",e
"Cardinality merge failed.",e
"Got a null cardinality estimator in: {}",sstable.getFilename()
"Could not read up compaction metadata for {}",sstable,e
"Estimated compaction gain: {}/{}={}",totalKeyCountAfter,totalKeyCountBefore,((double)totalKeyCountAfter)SLASHtotalKeyCountBefore
"Could not merge cardinalities",e
"Cannot open {}; partitioner {} does not match system partitioner {}.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.",descriptor,validationMetadata.partitioner,partitionerName
"Opening {} ({})",descriptor,FBUtilities.prettyPrintMemory(fileLength)
"Cannot open {}; partitioner {} does not match system partitioner {}.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.",descriptor,validationMetadata.partitioner,partitionerName
"Opening {} ({})",descriptor,FBUtilities.prettyPrintMemory(fileLength)
"INDEX LOAD TIME for {}: {} ms.",descriptor,TimeUnit.NANOSECONDS.toMillis(System.nanoTime()SUBstart)
"key cache contains {}/{} keys",sstable.getKeyCache().size(),sstable.getKeyCache().getCapacity()
"Corrupt sstable {}; skipping table",entry,ex
"Cannot read sstable {}; file system error, skipping table",entry,ex
"Cannot deserialize SSTable Summary File {}: {}",summariesFile.getPath(),e.getMessage()
"Cannot save SSTable Summary: ",e
"Cannot save SSTable bloomfilter: ",e
"Adding cache entry for {} -> {}",cacheKey,info
"Marking {} compacted",getFilename()
"Marking {} as a suspect for blacklisting.",getFilename()
"Running instance tidier for {} with setup {}",descriptor,setup
"Async instance tidier for {}, before barrier",descriptor
"Async instance tidier for {}, after barrier",descriptor
"Async instance tidier for {}, completed",descriptor
message
msg
"Renaming new SSTable {} to {}",oldDescriptor,newDescriptor
"Aborting import of sstables. {} was corrupt",newDescriptor
"wrote {} at {}",decoratedKey,dataEnd
"Key size {} exceeds maximum of {}, skipping row",key.getKey().remaining(),FBUtilities.MAX_UNSIGNED_SHORT
"Writing large partition {}/{}:{} ({}) to sstable {}",metadata.keyspace,metadata.name,keyString,FBUtilities.prettyPrintMemory(rowSize),getFilename()
"wrote index entry: {} at {}",indexEntry,indexStart
"Writing component {} to {} length {}",type,componentWriters.get(type).getPath(),prettyPrintMemory(size)
"Block Writing component to {} length {}",writer.getPath(),prettyPrintMemory(size)
"FATAL: Cannot initialize optimized memory deallocator. Some data, both in-memory and on-disk, may live longer due to garbage collection."
"Could not move file " + from + " to " + to,e
"Renaming {} to {}",from.getPath(),to.getPath()
"Could not do an atomic move",e
"Failed closing {}",c,e
"Failed closing {}",c,e
"Failed closing stream {}",c,ex
"Failed closing {}",c,ex
"Scheduling deferred deletion of file: {}",dir
"Error while getting {} folder size. {}",folder,e.getMessage()
"Failed to close mapped regions",err
"'{}' parameter is ignored when '{}' is '{}'",LZ4_HIGH_COMPRESSION_LEVEL,LZ4_COMPRESSOR_TYPE,LZ4_FAST_COMPRESSOR
"Cannot initialize native Snappy library. Compression on new sstables will be disabled."
"Creating Zstd Compressor with compression level={}",compressionLevel
"Failed to decode and apply a hint for {}: {} - table with id {} is unknown",address,hostId,message.payload.unknownTableID
"Failed to validate a hint for {}: {} - skipped",address,hostId
"Failed to apply hint",e
"Deleted hint file {}",descriptor.fileName()
"Failed to delete hint file {}",descriptor.fileName()
"Hint dispatch was interrupted",e
"Unable to sync directory {}",hintsDirectory.getAbsolutePath(),e
"Unable to open directory {}",hintsDirectory.getAbsolutePath()
"failed to create encyption context for hints file. ignoring encryption for hints.",ioe
"Failed to deserialize hints descriptor {}",path.toString(),e
"Unexpected EOF replaying hints ({}), likely due to unflushed hint file on shutdown; continuing",descriptor.fileName(),e
"Failed to read a hint for {}: {} - table with id {} is unknown in file {}",StorageService.instance.getEndpointForHostId(descriptor.hostId),descriptor.hostId,e.id,descriptor.fileName()
"Unexpected EOF replaying hints ({}), likely due to unflushed hint file on shutdown; continuing",descriptor.fileName(),e
"Transferring all hints to {}: {}",address,hostId
"Failed to transfer all hints to {}: {}; will retry in {} seconds",address,hostId,10
"Transferring all hints to {}: {}",address,hostId
"Failed to transfer all hints to {}: {}",address,hostId
String.format("Failed to dispatch hints file %s: file is corrupted",descriptor.fileName()),e
"Dispatching hints file {}",descriptor.fileName()
"Finished hinted handoff of file {} to endpoint {}: {}",descriptor.fileName(),address,hostId
"Finished hinted handoff of file {} to endpoint {}: {}, partially",descriptor.fileName(),address,hostId
"Finished converting hints file {}",descriptor.fileName()
"Unable to flush hint buffer: {}",e.getLocalizedMessage(),e
"Paused hints dispatch"
"Resumed hints dispatch"
"scheduling flush in {} ms",period
"Could not set new local compaction strategy",t
"Initializing {}.{}",keyspace.getName(),name
"Disabling compaction strategy by setting compaction thresholds to 0 is deprecated, set the compaction option 'enabled' to 'false' instead."
"Exception caught while calculating speculative retry threshold for {}: {}",metadata(),e
"Removing temporary or obsoleted files from unfinished operations for table {}",metadata.name
"Further extra check for orphan sstable files for {}",metadata.name
"could not delete {}",file.getAbsolutePath()
"User Requested secondary index re-build for {}/{} indexes: {}",ksName,cfName,Joiner.on(,).join(idxNames)
"Enqueuing flush of {}: {}",name,String.format("%s (%.0f%%) on-heap, %s (%.0f%%) off-heap",FBUtilities.prettyPrintMemory(onHeapTotal),onHeapRatioSTAR100,FBUtilities.prettyPrintMemory(offHeapTotal),offHeapRatioSTAR100)
"forceFlush requested but everything is clean in {}",name
"Flushed to {} ({} sstables, {}), biggest {}, smallest {}",sstables,sstables.size(),FBUtilities.prettyPrintMemory(totalBytesOnDisk),FBUtilities.prettyPrintMemory(maxBytesOnDisk),FBUtilities.prettyPrintMemory(minBytesOnDisk)
"Flushing largest {} to free up room. Used total: {}, live: {}, flushing: {}, this: {}",largest.cfs,ratio(usedOnHeap,usedOffHeap),ratio(liveOnHeap,liveOffHeap),ratio(flushingOnHeap,flushingOffHeap),ratio(thisOnHeap,thisOffHeap)
"Checking for sstables overlapping {}",sstables
"Rebuilding index for {} because of <{}>",name,failure.getMessage()
"Snapshot for {} keyspace data file {} created in {}",keyspace,ssTable.getFilename(),snapshotDirectory
"Created ephemeral snapshot marker file on {}.",ephemeralSnapshotMarker.getAbsolutePath()
String.format("Could not create marker file %s for ephemeral snapshot %s. " + "In case there is a failure in the operation that created " + "this snapshot, you may need to clean it manually afterwards.",ephemeralSnapshotMarker.getAbsolutePath(),snapshot),e
"Clearing ephemeral snapshot {} leftover from previous session.",ephemeralSnapshot
"using snapshot sstable {}",entries.getKey()
"using active sstable {}",entries.getKey()
"Discarding sstable data for truncated CF + indexes"
"cleaning out row cache"
"Truncate of {}.{} is complete",keyspace.getName(),name
"Cancelling in-progress compactions for {}",metadata.name
"Unable to cancel in-progress compactions for {}.  Perhaps there is an unusually large row in progress somewhere, or the system is simply overloaded.",metadata.name
"Compactions successfully cancelled"
"Changing neverPurgeTombstones for {}.{} from {} to {}",keyspace.getName(),getTableName(),neverPurgeTombstones,value
"Not changing neverPurgeTombstones for {}.{}, it is {}",keyspace.getName(),getTableName(),neverPurgeTombstones
"Node is not part of the ring; not recording size estimates"
"Recording size estimates"
"Spent {} milliseconds on estimating {}.{} size",TimeUnit.NANOSECONDS.toMillis(passed),table.metadata.keyspace,table.metadata.name
"Using stored Gossip Generation {} as it is greater than current system time {}.  See CASSANDRA-3654 if you experience problems",storedGeneration,now
"No host ID found, created {} (Note: This should happen exactly once per node).",hostId
"Detected version upgrade from {} to {}, snapshotting system keyspaces",previous,next
"Found unreadable versions info in pre 1.2 system.Versions table"
"stored prepared statement for logged keyspace '{}': '{}'",loggedKeyspace,cql
msg
"Couldn't find a defined index on {}.{} with the id {}. " + "If an index was just created, this is likely due to the schema not " + "being fully propagated. Local read will proceed without using the " + "index. Please wait for schema agreement after index creation.",metadata.keyspace,metadata.name,e.indexId
"Applying forwarded {}",cm
"Not a directory {}",dataDir
"Doesn't have execute permissions for {} directory",dataDir
"Doesn't have read permissions for {} directory",dataDir
"Doesn't have write permissions for {} directory",dataDir
"Moving index file {} to {}",indexFile,destFile
"Removing temporary directory {}",tmpDir
"removing blacklisted candidate {}",dataDir.location
"removing candidate {}, usable={}, requested={}",candidate.dataDirectory.location,candidate.availableSpace,writeSize
"Removing snapshot directory {}",snapshotDir
"Could not calculate the size of {}. {}",input,e.getMessage()
"Skipping invalid directory found in .toDelete: {}. Only %TEMP% or data file subdirectories are valid.",f
"Discovered obsolete snapshot. Deleting directory [{}]",snapshotDirectory
"Failed to open {}. Obsolete snapshots from previous runs will not be deleted.",TODELETEFILE,e
"Refreshing disk boundary cache for {}.{}",cfs.keyspace.getName(),cfs.getTableName()
"Updating boundaries from {} to {} for {}.{}",oldBoundaries,diskBoundaries,cfs.keyspace.getName(),cfs.getTableName()
"Got local ranges {} (ringVersion = {})",localRanges,ringVersion
"Invalid expiration date overflow policy: {}. Using default: {}",policyAsString,ExpirationDateOverflowPolicy.REJECT.name()
"Error in truncation",e
"{} applied.  Enqueuing response to {}@{} ",t,message.id(),message.from()
"Initializing {}.{}",getName(),cfm.name
"Creating replication strategy " + ksm.name + " params " + ksm.params
"New replication settings for keyspace {} - invalidating disk boundary caches",ksm.name
"Could not acquire lock for {} and table {}",ByteBufferUtil.bytesToHex(mutation.key().getKey()),columnFamilyStores.get(tableId).name
"Attempting to mutate non-existant table {} ({}.{})",upd.metadata().id,upd.metadata().keyspace,upd.metadata().name
String.format("Unknown exception caught while attempting to update MaterializedView! %s",upd.metadata().toString()),t
"Operation not allowed on secondary Index table ({})",cfName
"adding secondary index table {} to operation",indexCfs.metadata.name
"Loading new SSTables for {}/{}: {}",cfs.keyspace.getName(),cfs.getTableName(),options
"Failed verifying sstable {} in directory {}",descriptor,dir,t
"Failed verifying sstable {}",descriptor,t
"Failed importing sstables in directory {}",dir,t
"Failed importing sstables from data directory - renamed sstables are: {}",movedSSTables
"No new SSTables were found for {}/{}",cfs.keyspace.getName(),cfs.getTableName()
"Loading new SSTables and building secondary indexes for {}/{}: {}",cfs.keyspace.getName(),cfs.getTableName(),newSSTables
"Done loading load new SSTables for {}/{}",cfs.keyspace.getName(),cfs.getTableName()
"Moving sstable {} back to {}",movedSSTable.newDescriptor.filenameFor(Component.DATA),movedSSTable.oldDescriptor.filenameFor(Component.DATA)
"Writing {}, flushed range = ({}, {}]",Memtable.this.toString(),from,to
"Completed flushing {} ({}) for commitlog position {}",writer.getFilename(),FBUtilities.prettyPrintMemory(bytesFlushed),commitLogUpperBound
"High update contention in {}/{} partitions of {} ",heavilyContendedRowCount,toFlush.size(),Memtable.this
"Received a read request from {} for a range that is not owned by the current replica {}.",message.from(),command
"{} table was empty, migrating legacy {}, if this fails you should fix the issue and then truncate {} to have it try again.",peersName,legacyPeersName,peersName
"Migrating rows from legacy {} to {}",legacyPeersName,peersName
"Transferring row {}",transferred
"Migrated {} rows from legacy {} to {}",transferred,legacyPeersName,peersName
"{} table was empty, migrating legacy {} to {}",peerEventsName,legacyPeerEventsName,peerEventsName
"Transferring row {}",transferred
"Migrated {} rows from legacy {} to {}",transferred,legacyPeerEventsName,peerEventsName
"{} table was empty, migrating legacy {} to {}",transferredRangesName,legacyTransferredRangesName,transferredRangesName
"Transferring row {}",transferred
"Migrated {} rows from legacy {} to {}",transferred,legacyTransferredRangesName,transferredRangesName
"{} table was empty, migrating legacy {} to {}",availableRangesName,legacyAvailableRangesName,availableRangesName
"Transferring row {}",transferred
"Migrated {} rows from legacy {} to {}",transferred,legacyAvailableRangesName,availableRangesName
"Blacklisting {} for reads",directory
"Blacklisting {} for writes",directory
"Could not mark compacting for {} (sstables = {}, compacting = {})",sessionID,sstables,cfs.getTracker().getCompacting()
e.getMessage()
"Got exception trying to acquire sstables",e
"acquiring sstables for pending anti compaction on session {}",sessionID
"Session {} failed acquiring sstables: {}, retrying every {}ms for another {}s",sessionID,e.getMessage(),acquireSleepMillis,TimeUnit.SECONDS.convert(delay + startSUBSystem.currentTimeMillis(),TimeUnit.MILLISECONDS)
"{} Timed out waiting to acquire sstables",sessionID,e
"Got exception disabling compactions for session {}",sessionID,t
"Releasing acquired sstables for {}.{}",result.cfs.metadata.keyspace,result.cfs.metadata.name
message
"Could not reference sstables"
"Scheduling monitoring task with report interval of {} ms, max operations {}",reportIntervalMillis,maxOperations
"{} operations timed out in the last {} msecs:{}{}",failedOperations.num(),NANOSECONDS.toMillis(elapsedNanos),LINE_SEPARATOR,failedOperations.getLogMessage()
"{} operations were slow in the last {} msecs:{}{}",slowOperations.num(),NANOSECONDS.toMillis(approxElapsedNanos),LINE_SEPARATOR,slowOperations.getLogMessage()
"Committing transaction over {} staged: {}, logged: {}",originals,staged,logged
"Aborting transaction over {} staged: {}, logged: {}",originals,staged,logged
"Obsoleting {}",obsolete
"Checkpointing staged {}",staged
"Staging for obsolescence {}",reader
"Staging for obsolescence {}",originals
"Cancelling {} from transaction",cancel
"Splitting {} into new transaction",readers
"Failed to open log replica {}",file,e
"Added log file replica {} ",file
"Created new file replica {}",replica
"Failed to create log replica {}/{}",directory,fileName,e
"Failed to add record '{}' to some replicas '{}'",record,this
"Failed to classify files in {}\n" + "Some old files are missing but the txn log is still there and not completed\n" + "Files in folder:\n{}\nTxn: {}",folder,files.isEmpty()QUES"\t-":String.join("\n",files.keySet().stream().map(fARROWString.format("\t%s",f)).collect(Collectors.toList())),txnFile.toString(true)
"adding {} to list of files tracked for {}.{}",sstable.descriptor,cfstore.keyspace.getName(),cfstore.name
"removing {} from list of files tracked for {}.{}",sstable.descriptor,cfstore.keyspace.getName(),cfstore.name
"Failed to read records for transaction log {}",this
"Failed to sync file {}",file,e
"Failed to sync directory descriptor {}",directoryDescriptor,e
"Created transaction logs with id {}",txnFile.id()
"Deleting {}",file
"Unable to delete {} as it does not exist, see debug log file for stack trace",file
"Unable to delete {} as it does not exist, stack trace:\n {}",file,baos
"Unable to delete {}",file,e
"Removing files for transaction log {}",data
"Transaction log {} indicates txn was not completed, trying to abort it now",data
"Failed to abort transaction log {}",data,err
"Failed deleting files for transaction log {}, we'll retry after GC and on on server restart",data,err
"Closing transaction log {}",data
"SSTableTidier ran with no existing data file for an sstable that was not new"
"Failed deletion for {}, we'll retry after GC and on server restart",desc
"Failed to complete file transaction id {}",id(),t
"Failed to remove unfinished transaction leftovers for transaction log {}",txn.toString(true),failure
"Unexpected disk state: failed to read transaction log {}",txn.toString(true)
"[{}] Logging iterator on {}.{}, partition key={}, reversed={}",id,metadata.keyspace,metadata.name,metadata.partitionKeyType.getString(iterator.partitionKey().getKey()),iterator.isReverseOrder()
"[{}] {}",id,row.toString(metadata)
"[{}] {}",id,row.toString(metadata)
"[{}] Logging iterator on {}.{}, partition key={}, reversed={}, deletion={}",id,metadata.keyspace,metadata.name,metadata.partitionKeyType.getString(iterator.partitionKey().getKey()),iterator.isReverseOrder(),iterator.partitionLevelDeletion().markedForDeleteAt()
"[{}] {}",id,row.toString(metadata,fullDetails)
"[{}] {}",id,row.toString(metadata,fullDetails)
"[{}] {}",id,marker.toString(metadata)
"Changing from DateType to TimestampType is allowed, but be wary that they sort differently for pre-unix-epoch timestamps " + "(negative timestamp values) and thus this change will corrupt your data if you have such negative timestamp. So unless you " + "know that you don't have *any* pre-unix-epoch timestamp you should change back to DateType"
"Changing from TimestampType to DateType is allowed, but be wary that they sort differently for pre-unix-epoch timestamps " + "(negative timestamp values) and thus this change will corrupt your data if you have such negative timestamp. There is no " + "reason to switch from DateType to TimestampType except if you were using DateType in the first place and switched to " + "TimestampType by mistake."
"View already marked built for {}.{}",ksName,view.name
"Starting build of view({}.{}). Flushing base table {}.{}",ksName,view.name,ksName,baseCfs.name
"Stopped build for view({}.{}) after covering {} keys",ksName,view.name,keysBuilt
"Interrupted build for view({}.{}) after covering {} keys",ksName,view.name,keysBuilt
"Materialized View failed to complete, sleeping 5 minutes before restarting",t
"Marking view({}.{}) as built after covering {} keys ",ksName,view.name,keysBuilt
"Failed to update the distributed status of view, sleeping 5 minutes before retrying",e
"Stopping current view builder due to schema change"
"Not submitting build tasks for views in keyspace {} as " + "storage service is not initialized",keyspace.getName()
"Not adding view {} because the base table {} is unknown",definition.name(),definition.baseTableId
"Skipping {}, view query filters",key
"Starting new view build for range {}",range
"Resuming view build for range {} from token {} with {} covered keys",range,prevToken,keysBuilt
"Failed to get schema to converge before building view {}.{}",baseCfs.keyspace.getName(),view.name
"Completed build of view({}.{}) for range {} after covering {} keys ",ksName,view.name,range,keysBuilt
"Stopped build for view({}.{}) for range {} after covering {} keys",ksName,view.name,range,keysBuilt
"Registered user defined expression type {} and serializer {} with identifier {}",expressionClass.getName(),deserializer.getClass().getName(),id
"Error setting compaction strategy options ({}), defaults will be used",e.getMessage()
"Max sstable size of {}MB is configured for {}.{}; having a unit of compaction this large is probably a bad idea",configuredMaxSSTableSize,cfs.name,cfs.getTableName()
"Max sstable size of {}MB is configured for {}.{}.  Testing done for CASSANDRA-5727 indicates that performance improves up to 160MB",configuredMaxSSTableSize,cfs.name,cfs.getTableName()
"Created {}",manifest
"No compaction necessary for {}",this
"Could not acquire references for compacting SSTables {} which is not a problem per se," + "unless it happens frequently, in which case it must be reported. Will retry later.",candidate.sstables
"Unable to mark {} for compaction; probably a background compaction got to it first.  You can disable background compactions temporarily if this is a problem",sstables
"Live sstable {} from level {} is not on corresponding level in the leveled manifest." + " This is not a problem per se, but may indicate an orphaned sstable due to a failed" + " compaction not cleaned up properly.",sstable.getFilename(),level
"Compacting ({}) {}",taskId,ssTableLoggerMsg
String.format("Compacted (%s) %d sstables to [%s] to level=%d.  %s to %s (~%d%% of original) in %,dms.  Read Throughput = %s, Write Throughput = %s, Row Throughput = ~%,d/s.  %,d total partitions merged to %,d.  Partition merge counts were {%s}",taskId,transaction.originals().size(),newSSTableNames.toString(),getLevel(),FBUtilities.prettyPrintMemory(startsize),FBUtilities.prettyPrintMemory(endsize),(int)(ratioSTAR100),dTime,FBUtilities.prettyPrintMemoryPerSecond(startsize,durationInNano),FBUtilities.prettyPrintMemoryPerSecond(endsize,durationInNano),(int)totalSourceCQLRowsSLASH(TimeUnit.NANOSECONDS.toSeconds(durationInNano) + 1),totalSourceRows,totalKeysWritten,mergeSummary)
"CF Total Bytes Compacted: {}",FBUtilities.prettyPrintMemory(CompactionTask.addToTotalBytesCompacted(endsize))
"Actual #keys: {}, Estimated #keys:{}, Err%: {}",totalKeysWritten,estimatedKeys,((double)(totalKeysWrittenSUBestimatedKeys)SLASHtotalKeysWritten)
"Compaction space check is disabled"
msg
"Not enough space for compaction, {}MB estimated.  Reducing scope.",(float)expectedWriteSizeSLASH1024SLASH1024
"Disabling tombstone compactions for TWCS"
"Enabling tombstone compactions for TWCS"
"Could not acquire references for compacting SSTables {} which is not a problem per se," + "unless it happens frequently, in which case it must be reported. Will retry later.",latestBucket
"TWCS expired check sufficiently far in the past, checking for fully expired SSTables"
"TWCS skipping check for fully expired SSTables"
"Including expired sstables: {}",expired
"buckets {}, max timestamp {}",buckets,maxTimestamp
"Key {}, now {}",key,now
"Using STCS compaction for first window of bucket: data files {} , options {}",pairs,stcsOptions
"bucket size {} >= 2 and not in current bucket, compacting what's here: {}",bucket.size(),bucket
"No compaction necessary for bucket size {} , key {}, now {}",bucket.size(),key,now
"Unable to mark {} for compaction; probably a background compaction got to it first.  You can disable background compactions temporarily if this is a problem",sstables
"Creating {}.{} compaction strategy for pending repair: {}",cfs.metadata.keyspace,cfs.metadata.name,id
"Removing compaction strategy for pending repair {} on  {}.{}",sessionID,cfs.metadata.keyspace,cfs.metadata.name
"Obsoleting transient repaired ssatbles"
"Setting repairedAt to {} on {} for {}",repairedAt,transaction.originals(),sessionID
"You are running with -Dcassandra.never_purge_tombstones=true, this is dangerous!"
"not refreshing overlaps - running with -D{}=true",NEVER_PURGE_TOMBSTONES_PROPERTY
"not refreshing overlaps - running with ignoreOverlaps activated"
"not refreshing overlaps for {}.{} - neverPurgeTombstones is enabled",cfs.keyspace.getName(),cfs.getTableName()
"Checking droppable sstables in {}",cfStore
"Dropping overlap ignored expired SSTable {} (maxLocalDeletionTime={}, gcBefore={})",candidate,candidate.getSSTableMetadata().maxLocalDeletionTime,gcBefore
"Dropping expired SSTable {} (maxLocalDeletionTime={}, gcBefore={})",candidate,candidate.getSSTableMetadata().maxLocalDeletionTime,gcBefore
"Not compacting {}, level is already {}",sstable,level
"Changing level on {} from {} to {}",sstable,metadataBefore.sstableLevel,level
"Could not add sstable {} in level {} - dropping to 0",reader,reader.getSSTableLevel()
"Could not change sstable level - adding it at level 0 anyway, we will find it at restart.",e
"Replacing [{}]",toString(removed)
"Adding [{}]",toString(added)
"At level {}, {} [{}, {}] overlaps {} [{}, {}].  This could be caused by a bug in Cassandra 1.1.0 .. 1.1.3 or due to the fact that you have dropped sstables from another node into the data directory. " + "Sending back to L0.  If you didn't drop in sstables, and have not yet run scrub, you should do so since you may also have rows out-of-order within an sstable",level,previous,previous.first,previous.last,current,current.first,current.last
"Bootstrapping - doing STCS in L0"
"Compaction score for level {} is {}",i,score
"Compaction candidates for L{} are {}",i,toString(candidates)
"No compaction candidates for L{}",i
"L0 is too far behind, performing size-tiering there first"
"CompactionCounter: {}: {}",j,compactionCounterLBRACKETjRBRACKET
"Adding high-level (L{}) {} to candidates",sstable.getSSTableLevel(),sstable
"L{} contains {} SSTables ({}) in {}",i,getLevel(i).size(),FBUtilities.prettyPrintMemory(SSTableReader.getTotalBytes(getLevel(i))),this
"Choosing candidates for L{}",level
"Estimating {} compactions to do for {}.{}",Arrays.toString(estimated),cfs.keyspace.getName(),cfs.name
"Autocompaction is disabled"
"Background compaction is still running for {}.{} ({} remaining). Skipping",cfs.keyspace.getName(),cfs.name,count
"Scheduling a background task check for {}.{} with {}",cfs.keyspace.getName(),cfs.name,cfs.getCompactionStrategyManager().getName()
"Failed to wait for compaction executors shutdown"
"Interrupted while waiting for tasks to be terminated",e
"Checking {}.{}",cfs.keyspace.getName(),cfs.name
"Aborting compaction for dropped CF"
"Checking for upgrade tasks {}.{}",cfs.keyspace.getName(),cfs.getTableName()
"No tasks available"
"Starting {} for {}.{}",operationType,cfs.keyspace.getName(),cfs.getTableName()
"No sstables to {} for {}.{}",operationType.name(),cfs.keyspace.getName(),cfs.name
"Finished {} for {}.{} successfully",operationType,cfs.keyspace.getName(),cfs.getTableName()
"Failed to cleanup lifecycle transactions ({} for {}.{})",operationType,cfs.keyspace.getName(),cfs.getTableName(),fail
"Cleanup cannot run before a node has joined the ring"
"Skipping {} ([{}, {}]) for cleanup; all rows should be kept. Needs cleanup full ranges: {} Needs cleanup transient ranges: {} Repaired: {}",sstable,sstable.first.getToken(),sstable.last.getToken(),needsCleanupFull,needsCleanupTransient,sstable.isRepaired()
"Skipping cleanup for {}/{} sstables for {}.{} since they are fully contained in owned ranges (full ranges: {}, transient ranges: {})",skippedSStables,totalSSTables,cfStore.keyspace.getName(),cfStore.getTableName(),fullRanges,transientRanges
"Garbage collecting {}",txn.originals()
"Partitioner does not support splitting"
"Relocate cannot run before a node has joined the ring"
"Relocating {}",txn.originals()
"{} Starting anticompaction for {}.{} on {}/{} sstables",PreviewKind.NONE.logPrefix(sessionID),cfs.keyspace.getName(),cfs.getTableName(),validatedForRepair.size(),cfs.getLiveSSTables().size()
"{} Starting anticompaction for ranges {}",PreviewKind.NONE.logPrefix(sessionID),replicas
"{} Completed anticompaction successfully",PreviewKind.NONE.logPrefix(sessionID)
message
"{} SSTable {} fully contained in range {}, mutating repairedAt instead of anticompacting",PreviewKind.NONE.logPrefix(parentRepairSession),sstable,r
"{} SSTable {} ({}) will be anticompacted on range {}",PreviewKind.NONE.logPrefix(parentRepairSession),sstable,sstableBounds,r
"Major compaction will not result in a single sstable - repaired and unrepaired data is kept separate and compaction runs per data_file_directory."
"No sstables found for the provided token range"
"Schema does not exist for file {}. Skipping.",filename
"Schema does not exist for file {}. Skipping.",filename
"Cleanup cannot run before a node has joined the ring"
"Will not clean {}, it is not an active sstable",entry.getValue()
"forceUserDefinedCleanup failed: {}",e.getLocalizedMessage()
"Will not compact {}: it is not an active sstable",desc
"No files to compact for user defined compaction"
"SSTable {} ([{}, {}]) does not intersect the owned ranges ({}), dropping it",sstable,sstable.first.getToken(),sstable.last.getToken(),allRanges
"Expected bloom filter size : {}",expectedBloomFilterSize
"Cleaning up {}",sstable
String.format(format,finished.get(0).getFilename(),FBUtilities.prettyPrintMemory(startsize),FBUtilities.prettyPrintMemory(endsize),(int)(ratioSTAR100),totalkeysWritten,dTime)
"Performing anticompaction on {} sstables",originalCount
format,originalCount,antiCompactedSSTableCount
"No valid anticompactions for this group, All sstables were compacted and are no longer available"
"Anticompacting {}",txn
"Anticompaction has been canceled for session {}",pendingRepair
e.getMessage(),e
"Error anticompacting " + txn + " for " + pendingRepair,e
"Cache flushing was already in progress: skipping {}",writer.getCompactionInfo()
t.getMessage()
"Interruption of compaction encountered exceptions:",t
"Full interruption stack trace:",t
"Executor has been shut down, not submitting {}",name
"Executor has shut down, could not submit {}",name
"Failed to submit {}",name,ex
"Disabling tombstone compactions for DTCS"
"Enabling tombstone compactions for DTCS"
"Could not acquire references for compacting SSTables {} which is not a problem per se," + "unless it happens frequently, in which case it must be reported. Will retry later.",latestBucket
"Including expired sstables: {}",expired
"Compaction buckets are {}",buckets
"Got sstables {} for STCS from {}",s,sstables
"Unable to mark {} for compaction; probably a background compaction got to it first.  You can disable background compactions temporarily if this is a problem",sstables
"Using a non-default timestamp_resolution {} - are you really doing inserts with USING TIMESTAMP <non_microsecond_timestamp> (or driver equivalent)?",timestampResolution
"{} subscribed to the data tracker.",this
"Running automatic sstable upgrade for {}",sstable
"Recreating compaction strategy - compaction parameters changed for {}.{}",cfs.keyspace.getName(),cfs.getTableName()
"Recreating compaction strategy - disk boundaries are out of date for {}.{}.",cfs.keyspace.getName(),cfs.getTableName()
"SSTable repairedAt/pendingRepaired values changed while getting scanners"
"Switching local compaction strategy from {} to {}}",this.params,params
"Using a non-default timestamp_resolution {} - are you really doing inserts with USING TIMESTAMP <non_microsecond_timestamp> (or driver equivalent)?",timestampResolution
"You are running with sstables overlapping checks disabled, it can result in loss of data"
"Compaction buckets are {}",buckets
"Could not acquire references for compacting SSTables {} which is not a problem per se," + "unless it happens frequently, in which case it must be reported. Will retry later.",hottestBucket
"Unable to mark {} for compaction; probably a background compaction got to it first.  You can disable background compactions temporarily if this is a problem",sstables
"Switching write location from {} to {}",locations.get(prevIdx),locations.get(locationIndex)
"All sstables not from the same disk - putting results in {}",directory
"putting compaction results in {}",directory
"Switching writer, currentBytesToWrite = {}",currentBytesToWrite
"Switching writer, currentPartitionsToWrite = {}",currentPartitionsToWrite
"invalid global counter shard detected; ({}, {}, {}) and ({}, {}, {}) differ only in " + "count; will pick highest to self-heal on compaction",leftState.getCounterId(),leftClock,leftCount,rightState.getCounterId(),rightClock,rightCount
"invalid remote counter shard detected; ({}, {}, {}) and ({}, {}, {}) differ only in " + "count; will pick highest to self-heal on compaction",leftState.getCounterId(),leftClock,leftCount,rightState.getCounterId(),rightClock,rightCount
"Error while reading compressed input stream.",e
"[Stream #{}] Start streaming sstable {} to {}, repairedAt = {}, totalSize = {}",session.planId(),sstable.getFilename(),session.peer,sstable.getSSTableMetadata().repairedAt,prettyPrintMemory(totalSize)
"[Stream #{}] Finished streaming {}.{} gen {} component {} to {}, xfered = {}, length = {}, totalSize = {}",session.planId(),sstable.getKeyspaceName(),sstable.getColumnFamilyName(),sstable.descriptor.generation,component,session.peer,prettyPrintMemory(bytesWritten),prettyPrintMemory(length),prettyPrintMemory(totalSize)
"[Stream #{}] Finished streaming sstable {} to {}, xfered = {}, totalSize = {}",session.planId(),sstable.getFilename(),session.peer,prettyPrintMemory(progress),prettyPrintMemory(totalSize)
"[Stream #{}] Start receiving file #{} from {}, repairedAt = {}, size = {}, ks = '{}', table = '{}', pendingRepair = '{}'.",session.planId(),fileSeqNum,session.peer,repairedAt,totalSize,cfs.keyspace.getName(),cfs.getTableName(),pendingRepair
"[Stream #{}] Finished receiving file #{} from {} readBytes = {}, totalSize = {}",session.planId(),fileSeqNum,session.peer,FBUtilities.prettyPrintMemory(in.getBytesRead()),FBUtilities.prettyPrintMemory(totalSize)
"[Stream {}] Error while reading partition {} from stream on ks='{}' and table='{}'.",session.planId(),partitionKey,cfs.keyspace.getName(),cfs.getTableName(),e
"Incoming stream entireSSTable={} components={}",streamHeader.isEntireSSTable,streamHeader.componentManifest
"[Stream #{}] Invalidated {} row cache entries on table {}.{} after stream " + "receive task completed.",session.planId(),invalidatedKeys,cfs.keyspace.getName(),cfs.getTableName()
"[Stream #{}] Invalidated {} counter cache entries on table {}.{} after stream " + "receive task completed.",session.planId(),invalidatedKeys,cfs.keyspace.getName(),cfs.getTableName()
"ViewFilter for {}/{} sstables",sstables.size(),Iterables.size(view.select(SSTableSet.CANONICAL))
"[Stream #{}] Start streaming file {} to {}, repairedAt = {}, totalSize = {}",session.planId(),sstable.getFilename(),session.peer,sstable.getSSTableMetadata().repairedAt,totalSize
"[Stream #{}] Writing section {} with length {} to stream.",session.planId(),sectionIdxPLUSPLUS,length
"[Stream #{}] Finished streaming file {} to {}, bytesTransferred = {}, totalSize = {}",session.planId(),sstable.getFilename(),session.peer,FBUtilities.prettyPrintMemory(progress),FBUtilities.prettyPrintMemory(totalSize)
"[Stream #{}] Start receiving file #{} from {}, repairedAt = {}, size = {}, ks = '{}', pendingRepair = '{}', table = '{}'.",session.planId(),fileSeqNum,session.peer,repairedAt,totalSize,cfs.keyspace.getName(),pendingRepair,cfs.getTableName()
"[Stream #{}] Reading section {} with length {} from stream.",session.planId(),sectionIdxPLUSPLUS,sectionLength
"[Stream #{}] Finished receiving file #{} from {} readBytes = {}, totalSize = {}",session.planId(),fileSeqNum,session.peer,FBUtilities.prettyPrintMemory(cis.getTotalCompressedBytesRead()),FBUtilities.prettyPrintMemory(totalSize)
"[Stream {}] Error while reading partition {} from stream on ks='{}' and table='{}'.",session.planId(),partitionKey,cfs.keyspace.getName(),cfs.getTableName()
"[Stream #{}] Start streaming file {} to {}, repairedAt = {}, totalSize = {}",session.planId(),sstable.getFilename(),session.peer,sstable.getSSTableMetadata().repairedAt,totalSize
"[Stream #{}] Finished streaming file {} to {}, bytesTransferred = {}, totalSize = {}",session.planId(),sstable.getFilename(),session.peer,FBUtilities.prettyPrintMemory(progress),FBUtilities.prettyPrintMemory(totalSize)
"[Stream #{}] Started receiving sstable #{} from {}, size = {}, table = {}",session.planId(),fileSequenceNumber,session.peer,prettyPrintMemory(totalSize),cfs.metadata()
"[Stream #{}] Started receiving {} component from {}, componentSize = {}, readBytes = {}, totalSize = {}",session.planId(),component,session.peer,prettyPrintMemory(length),prettyPrintMemory(bytesRead),prettyPrintMemory(totalSize)
"[Stream #{}] Finished receiving {} component from {}, componentSize = {}, readBytes = {}, totalSize = {}",session.planId(),component,session.peer,prettyPrintMemory(length),prettyPrintMemory(bytesRead),prettyPrintMemory(totalSize)
"[Stream {}] Error while reading sstable from stream for table = {}",session.planId(),cfs.metadata(),e
"[Table #{}] {} Components to write: {}",cfs.metadata(),desc.filenameFor(Component.DATA),components
"Restore point in time is before latest truncation of table {}.{}. Clearing truncation record.",cfs.metadata.keyspace,cfs.metadata.name
"Global replay position is {} from columnfamilies {}",globalPosition,FBUtilities.toString(cfPersisted)
"Skipped {} mutations from unknown (probably removed) CF with id {}",entry.getValue(),entry.getKey()
"Finished waiting on mutations from recovery"
"Ignoring commit log replay error likely due to incomplete flush to disk",exception
"Ignoring commit log replay error",exception
"Replay stopped. If you wish to override this error and continue starting the node ignoring " + "commit log replay problems, specify -D" + IGNORE_REPLAY_ERRORS_PROPERTY + "=true " + "on the command line"
"(Unopened) CDC segment {} is no longer needed and will be deleted now",cdcFile
"created a new encrypted commit log segment: {}",logFile
"Skipping playback of empty log: {}",file.getName()
"Finished reading {}",file
"Reading {} (CL version {}, messaging version {}, compression {})",file.getPath(),desc.version,desc.getMessagingVersion(),desc.compression
"Skipping read of fully-flushed {}",file
"Reading mutation at {}",mutationStart
"Not enough bytes left for another mutation in this CommitLog section, continuing"
"Encountered end of segment marker at {}",reader.getFilePointer()
"Read mutation for {}.{}: {}",mutation.getKeyspaceName(),mutation.key(),"{" + StringUtils.join(mutation.getPartitionUpdates().iterator(),", ") + "}"
"No commitlog_archiving properties found; archive + pitr will be disabled"
"Archiving file {} failed, file may have already been archived.",name,e
"Looks like the archiving of file {} failed earlier, cassandra is going to ignore this segment for now.",name,e.getCause().getCause()
"Skipping restore of archive {} as the segment already exists in the restore location {}",fromFile.getPath(),toFile.getPath()
"Will update the commitlog markers every {}ms and flush every {}ms",markerIntervalMillis,syncIntervalMillis
"No commitlog files found; skipping replay"
"Replaying {}",StringUtils.join(files,", ")
"Log replay complete, {} replayed mutations",replayed
"discard completed log segments for {}-{}, table {}",lowerBound,upperBound,id
"Commit log segment {} is unused",segment
"Not safe to delete{} commit log segment {}; dirty is {}",(iter.hasNext()QUES"":" active"),segment,segment.dirtyString()
String.format("%s. Commit disk failure policy is %s; terminating thread",message,DatabaseDescriptor.getCommitFailurePolicy()),t
message,t
"No segments in reserve; creating a fresh one"
"Failed to force-recycle all segments; at least one segment is still in use with dirty CFs."
"Total active commitlog segment space used is {} out of {}",currentSize,total
"CLSM closing and clearing existing commit log segments..."
"CLSM done with closing and clearing existing commit log segments."
"cqlQuery {}",cqlQuery
"created {}",rowIterator
"Finished scanning {} rows (estimate was: {})",rowIterator.totalRead,totalRowCount
"Some hosts failed: {}",loader.getFailedHosts()
"partitioner is {}",partitioner
"adding {}",split
"Error closing connection",t
"Invalid replica host name: {}, skipping it",replica
"Created instance with the following replicas: {}",Arrays.asList(replicas)
"Initialized with replica hosts: {}",replicaHosts
"Using the following hosts order for the new query plan: {} | {}",local,remote
"Added a new host {}",host
"The host {} is now up",host
"The host {} is now down",host
"Removed the host {}",host
"Could not retrieve local network interfaces.",e
"Failed to authorize {} for super-user permission",role.getRoleName()
"Failed to authorize {} for login permission",role.getRoleName()
"(Re)initializing {} (validity period/update interval/max entries) ({}/{}/{})",name,getValidity(),getUpdateInterval(),getMaxEntries()
"Decoding credentials from client token"
"An invalid value has been detected in the {} table for role {}. If you are " + "unable to login, you may need to disable authentication and confirm " + "that values in that table are accurate",AuthKeyspace.ROLES,row.getString("role")
"Failed to authorize {} for super-user permission",role.getRoleName()
"Failed to authorize {} for login permission",role.getRoleName()
"Created default superuser role '{}'",DEFAULT_SUPERUSER_NAME
"CassandraRoleManager skipped default role setup: some nodes were not ready"
"Setup task failed with error, rescheduling"
"No CallbackHandler available for authentication"
"Unexpected exception processing authentication callbacks",e
"Configuration options credentials_update_interval_in_ms, credentials_validity_in_ms and " + "credentials_cache_max_entries may not be applicable for the configured authenticator ({})",authenticator.getClass().getName()
"Failed to authorize {} for {}",user,resource
String.format("CassandraAuthorizer failed to revoke all permissions of %s",revokee.getRoleName()),e
String.format("CassandraAuthorizer failed to revoke all permissions on %s",droppedResource),e
"Authentication exception",e
"Authorizing JMX method invocation {} for {}",methodName,subjectEQEQnullQUES"":subject.toString().replaceAll("\\n"," ")
"Auth setup is not complete, refusing access"
"Access denied to blacklisted method {}",methodName
"JMX invocation of {} on MBeanServer requires permission {}",methodName,Permission.DESCRIBE
"JMX invocation of {} on {} requires permission {}",methodName,targetBean,requiredPermission
"Permissions for JMX resource contains invalid ObjectName {}",resource.getObjectName()
"Subject does not have sufficient permissions on all MBeans matching the target pattern {}",target
"Permissions for JMX resource contains invalid ObjectName {}",resource.getObjectName()
"Subject does not have sufficient permissions on target MBean {}",target
"Access denied, method name {} does not map to any defined permission",methodName
"Unexpected error when executing deferred lock-intending functions",t
msg,message.id(),message.from()
"{} dropping message of type {} due to error",id(),message.verb(),t
"{} channel closed by provider",id(),cause
"{} channel in potentially inconsistent state after error; closing",id(),cause
"Unexpected exception in {}.exceptionCaught",this.getClass().getSimpleName(),t
"{} successfully connected, version = {}, framing = {}, encryption = {}",id(true),success.messagingVersion,settings.framing,encryptionLogStatement(settings.encryption)
"{} incorrect legacy peer version predicted; reconnecting",id()
"{} updating connection settings",id()
"Problem closing channel {}",closeIfIs,future.cause()
"Failed to close connection cleanly:",t
"creating outbound bootstrap to {}, requestVersion: {}",settings,requestMessagingVersion
"creating outbound netty SslContext: context={}, engine={}",sslContext.getClass().getName(),sslHandler.engine().getClass().getName()
"starting handshake with peer {}, msg = {}",settings.connectTo,msg
"received second handshake message from peer {}, msg = {}",settings.connectTo,msg
"Failed to connect to peer {}",settings.to,cause
"Failed to handshake with peer {}",settings.to,cause
"Unexpected exception in {}.exceptionCaught",this.getClass().getSimpleName(),t
"Initialized back-pressure with high ratio: {}, factor: {}, flow: {}, window size: {}.",highRatio,factor,flow,windowSize
"Back-pressure state for {}: incoming rate {}, outgoing rate {}, ratio {}, rate limiting {}",backPressure.getHost(),incomingRate,outgoingRate,actualRatio,limiter.getRate()
"Failed to invoke listener {} to {}",listener,future,t
"using netty {} event loop for pool prefix {}",name(),threadNamePrefix
"Creating SSL handler for {}:{}",peer.getHostString(),peer.getPort()
"{} sending {} to {}@{}",FBUtilities.getBroadcastAddressAndPort(),message.verb(),message.id(),to
"Message-to-self {} going over MessagingService",message
"Waiting for messaging service to quiesce"
"Writing {} bytes at position {} of {}",toWrite,bytesTransferred,length
"creating inbound netty SslContext: context={}, engine={}",sslContext.getClass().getName(),sslHandler.engine().getClass().getName()
"Listening on {}",initializer.settings
"Timeout handshaking with {} (on {})",SocketFactory.addressId(initiate.from,(InetSocketAddress)ctx.channel().remoteAddress()),settings.bindAddress
"connection from peer {}, protocol = {}, cipher suite = {}",ctx.channel().remoteAddress(),session.getProtocol(),session.getCipherSuite()
"Received handshake initiation message from peer {}, message = {}",ctx.channel().remoteAddress(),initiate
"Connection version {} (min {}) from {}",initiate.acceptVersions.max,initiate.acceptVersions.min,initiate.from
"peer {} only supports messaging versions higher ({}) than this node supports ({})",ctx.channel().remoteAddress(),initiate.acceptVersions.min,current_version
"peer {} only supports messaging versions lower ({}) than this node supports ({})",ctx.channel().remoteAddress(),initiate.acceptVersions.max,minimum_version
"Connection version {} from {}",version,ctx.channel().remoteAddress()
"Received stream using protocol version {} (my version {}). Terminating connection",version,settings.acceptStreaming.max
"Received third handshake message from peer {}, message = {}",ctx.channel().remoteAddress(),confirmOutboundPre40
"Failed to properly handshake with peer {}. Closing the channel.",channel.remoteAddress(),cause
"Unexpected exception in {}.exceptionCaught",this.getClass().getSimpleName(),t
"{} connection established, version = {}, framing = {}, encryption = {}",handler.id(true),useMessagingVersion,initiate.framing,pipeline.get("ssl")BANGEQnullQUESencryptionLogStatement(settings.encryption):"disabled"
"{} unexpected exception caught while deserializing a message",id(),t
"Unexpected exception in {}.exceptionCaught",this.getClass().getSimpleName(),t
"{} invalid, unrecoverable CRC mismatch detected while reading messages - closing the connection",id()
"{} unexpected exception caught while processing inbound messages; terminating connection",id(),cause
"{} unexpected exception caught while deserializing a message",id(),t
"{} exception caught while reactivating a handler",handler.id(),t
"Setting version {} for {}",version,endpoint
"Resetting version for {}",endpoint
"Expired {} entries",n
"setting the block-for-peers timeout (in seconds) to {} might be a bit excessive, but using it nonetheless",timeoutSecs
"Blocking coordination until only a single peer is DOWN in the local datacenter, timeout={}s",TimeUnit.NANOSECONDS.toSeconds(timeoutNanos)
"Blocking coordination until only a single peer is DOWN in each datacenter, timeout={}s",TimeUnit.NANOSECONDS.toSeconds(timeoutNanos)
"Ensured sufficient healthy connections with {} after {} milliseconds",numDown.keySet(),TimeUnit.NANOSECONDS.toMillis(System.nanoTime()SUBstartNanos)
"Timed out after {} milliseconds, was waiting for remaining peers to connect: {}",TimeUnit.NANOSECONDS.toMillis(System.nanoTime()SUBstartNanos),numDown
"{} prepared statements discarded in the last minute because cache limit reached ({} MB)",count,DatabaseDescriptor.getPreparedStatementsCacheSizeMB()
"Initialized prepared statement caches with {} MB",DatabaseDescriptor.getPreparedStatementsCacheSizeMB()
"prepared statement recreation error: {}",useKeyspaceAndCQL.right,e
"Preloaded {} prepared statements",count
"Process {} @CL.{}",statement,options.getConsistency()
"Statement {} executed with NODE_LOCAL consistency level.",statement
"[{}] '{}'",i + 1,variables.get(i)
String.format("The statement: [%s] could not be parsed.",queryStr),re
"Column definitions for {}.{} changed, invalidating related prepared statements",ksName,cfName
"Keyspace {} was dropped, invalidating related prepared statements",ksName
"Table {}.{} was dropped, invalidating related prepared statements",ksName,cfName
format,tableNames,FBUtilities.prettyPrintMemory(size),FBUtilities.prettyPrintMemory(failThreshold),FBUtilities.prettyPrintMemory(sizeSUBfailThreshold)," (see batch_size_fail_threshold_in_kb)"
format,tableNames,FBUtilities.prettyPrintMemory(size),FBUtilities.prettyPrintMemory(warnThreshold),FBUtilities.prettyPrintMemory(sizeSUBwarnThreshold),""
"Ignoring obsolete property {}",name
msg
"Failed to compile function '{}' for language {}: ",name,language,e
"The function 'dateof' is deprecated." + " Use the function 'toTimestamp' instead."
"The function 'unixtimestampof' is deprecated." + " Use the function 'toUnixTimestamp' instead."
"access denied: resource {}",resource
"access denied: resource {}",resource
"Invocation of user-defined function '{}' failed",this,t
"Invocation of user-defined function '{}' failed",this,t
warn
"Compiling Java source UDF '{}' as class '{}' using source:\n{}",name,targetClassName,javaSource
String.format("Could not compile function '%s' from Java source:%n%s",name,javaSource),e
"Publishing: {}",event
"Adding subscriber: {}",consumer
"Total subscribers: {}",subscribersByClass.values().size()
"Returning {} {} events for key {} (limit {}) (includeKey {})",ret.size(),eventClazz,key,limit,includeKey
"Enabling events: {}",eventClazz
"Disabling events: {}",eventClazz
"Persisting received {} event",cls.getName()
"Compaction strategy {} does not have a static validateOptions method. Validation ignored",klass.getName()
"Received schema pull request from {}",message.from()
"Received schema version request from {}",message.from()
COMPACT_STORAGE_DEPRECATION_MESSAGE,builder.keyspace,builder.name,builder.flags,DEPRECATED_CS_FLAGS,flags
"Not pulling schema because release version in Gossip is not major version {}, it is {}",ourMajorVersion,releaseVersion
"Not pulling schema from {}, because local schema version is not known yet",endpoint
"Not pulling schema from {}, because schema versions match ({})",endpoint,Schema.schemaVersionToString(theirVersion)
"Not pulling schema from {}, because versions match ({}/{}), or shouldPullSchemaFrom returned false",endpoint,Schema.instance.getVersion(),theirVersion
"epState vanished for {}, not submitting migration task",endpoint
"Not submitting migration task for {} because our versions match ({})",endpoint,epSchemaVersion
"Submitting migration task for {}, schema version mismatch: local={}, remote={}",endpoint,Schema.schemaVersionToString(Schema.instance.getVersion()),Schema.schemaVersionToString(epSchemaVersion)
"Migration task failed to complete"
"Migration task was interrupted"
"Create new Keyspace: {}",ksm
"Create new table: {}",cfm
"Update Keyspace '{}' From {} To {}",ksm.name,oldKsm,ksm
"Update table '{}/{}' From {} To {}",current.keyspace,current.name,current,updated
"Drop Keyspace '{}'",oldKsm.name
"Drop table '{}/{}'",tm.keyspace,tm.name
"Starting local schema reset..."
"Truncating schema tables..."
"Clearing local schema keyspace definitions..."
"Requesting schema from {}",node
"Local schema reset is complete."
errorMsg,"",exc
errorMsg,"restart cassandra with -Dcassandra.ignore_corrupted_schema_tables=true and "
"Skipping duplicate compilation of already existing UDF {}",name
String.format("Cannot load function '%s' from schema: this function won't be available (on this node)",name),e
"Can't send schema pull request: node {} is down.",endpoint
"Skipped sending a migration request: node {} has a higher major version now.",endpoint
"Configuration exception merging remote schema",e
"Indexer {} does not have a static validateOptions method. Validation ignored",indexerClass.getName()
CRC_CHECK_CHANCE_WARNING
"The {} option has been deprecated. You should use {} instead",CHUNK_LENGTH_KB,CHUNK_LENGTH_IN_KB
"The {} option has been deprecated. You should use {} instead",SSTABLE_COMPRESSION,CLASS
"Received schema push request from {}",message.from()
String.format("Uncaught exception on thread %s",Thread.currentThread()),t
"ScheduledThreadPoolExecutor has shut down as part of C* shutdown"
"Error in ThreadPoolExecutor",t
"Task cancelled",e
"Failed to execute task, unexpected exception killed worker",t
"Unexpected exception killed worker",t
"Interrupted while executing {}, but not shutdown; continuing with loop",runnable,ie
"Exception thrown by runnable, continuing with loop",t
"Removing {} in parent repair sessions",toRemove
"JMX settings in cassandra-env.sh have been bypassed as the JMX connector server is " + "already initialized. Please refer to cassandra-env.(sh|ps1) for JMX configuration info"
"Exception in thread " + t,e
"Exception in thread " + t,e2
"Error while loading schema: ",e
"opening keyspace {}",keyspaceName
"Error loading key or row cache",t
"Unable to start GCInspector (currently only supported on the Sun JVM)"
"Trying to load metrics-reporter-config from file: {}",metricsReporterConfigFile
"Failed to load metrics-reporter-config, file does not exist: {}",metricsReporterConfigFile
"Failed to load metrics-reporter-config, metric sinks will not be activated",e
"Completed submission of build tasks for any materialized views defined at startup"
"Hostname: {}",InetAddress.getLocalHost().getHostName() + ":" + DatabaseDescriptor.getStoragePort() + ":" + DatabaseDescriptor.getSSLStoragePort()
"Could not resolve local host"
"JVM vendor/version: {}/{}",System.getProperty("java.vm.name"),System.getProperty("java.version")
"Heap size: {}/{}",FBUtilities.prettyPrintMemory(Runtime.getRuntime().totalMemory()),FBUtilities.prettyPrintMemory(Runtime.getRuntime().maxMemory())
"{} {}: {}",pool.getName(),pool.getType(),pool.getPeakUsage()
"Classpath: {}",System.getProperty("java.class.path")
"JVM Arguments: {}",ManagementFactory.getRuntimeMXBean().getInputArguments()
"Not starting client transports in write_survey mode as it's bootstrapping or " + "auth is enabled"
"Not starting client transports as bootstrap has not completed"
"Not starting native transport as requested. Use JMX (StorageService->startNativeTransport()) or nodetool (enablebinary) to start it"
"Error shutting down local JMX server: ",e
"Exception encountered during startup",e
"Exception encountered during startup: {}",e.getMessage()
message,cause
"Initializing key cache with capacity of {} MBs.",DatabaseDescriptor.getKeyCacheSizeInMB()
"Initializing row cache with capacity of {} MBs",DatabaseDescriptor.getRowCacheSizeInMB()
"Initializing counter cache with capacity of {} MBs",DatabaseDescriptor.getCounterCacheSizeInMB()
"Scheduling counter cache save to every {} seconds (going to save {} keys).",DatabaseDescriptor.getCounterCacheSavePeriod(),keysToSaveEQEQInteger.MAX_VALUEQUES"all":keysToSave
"submitting cache saves"
"cache saves completed"
"Enqueuing response to snapshot request {} to {}",command.snapshot_name,message.from()
"Sending ECHO_RSP to {}",message.from()
"Need to stream {}, current endpoints {}, new endpoints {}",toStream,oldEndpoints,newEndpoints
"Current tmd: {}, Updated tmd: {}",tokenMetaClone,tokenMetaCloneAllSettled
"Calculating ranges to stream and request for keyspace {}",keyspace
"Endpoint ranges to stream to " + rangesToStream
"Will stream range {} of keyspace {} to endpoint {}",rangesToStream.get(address),keyspace,address
"Will request range {} of keyspace {} from endpoint {}",rangesToFetch.get(address),keyspace,address
"Keyspace {}: work map {}.",keyspace,rangesToFetch
"Calculating toStream"
"Calculating toFetch"
"To stream {}",toStream
"To fetch {}",toFetch
"Comparing {} and {}",src,dst
"    Doesn't intersect adding {}",src
"    Intersects adding {}",remainder
"Netty using native Epoll event loop"
"Netty using Java NIO event loop"
"epoll not available",Epoll.unavailabilityCause()
"Got failure from {}",from
"Failed paxos prepare locally",ex
"Failed paxos propose locally",ex
"Failed to apply paxos commit locally : ",ex
"Received base materialized view mutation for key {} that does not belong " + "to this node. There is probably a range movement happening (move or decommission)," + "but this node hasn't updated its ring metadata yet. Adding mutation to " + "local batchlog to be replayed later.",mutation.key()
"Error applying local view update to keyspace {}: {}",mutation.getKeyspaceName(),mutation
"Exception occurred updating coordinatorWriteLatency metric",ex
"Sending batchlog store request {} to {} for {} mutations",batch.id,replica,batch.size()
"Sending batchlog remove request {} to {}",uuid,target
"Adding FWD message to {}@{}",message.id(),replica
"Sending message to {}@{}",message.id(),targets.get(0)
"Failed to apply mutation locally : ",ex
"Failed to apply mutation locally : ",ex
t.getMessage()
"Didn't get enough response rows; actual rows per range: {}; remaining rows: {}, new concurrent requests: {}",rowsPerRange,remainingRows,concurrencyFactor
"Estimated result rows per range: {}; requested rows: {}, ranges.size(): {}; concurrent range requests: {}",resultsPerRange,command.limits().count(),ranges.rangeCount(),concurrencyFactor
"Hosts not in agreement. Didn't get a response from everybody: {}",StringUtils.join(results.get(UNREACHABLE),",")
"{} disagrees ({})",host,entry.getKey()
"Schemas are in agreement."
"Starting a blocking truncate operation on keyspace {}, CF {}",keyspace,cfname
"Cannot perform truncate, some hosts are down"
"Some hints were not written before shutdown.  This is not supposed to happen.  You should (a) run repair, and (b) file a bug report"
"Discarding hint for endpoint not part of ring: {}",target
"Adding hints for {}",validTargets
"Using {} as query handler for native protocol queries (as requested with -Dcassandra.custom_query_handler_class)",customHandlerClass
"Cannot use class {} as query handler ({}), ignoring by defaulting on normal query handling",customHandlerClass,e.getMessage()
"Overriding RING_DELAY to {}ms",newdelay
"Setting tokens to {}",tokens
"Stopping gossip by operator request"
"Starting gossip by operator request"
"Stopping gossiper"
"Stopping native transport"
"Gathering node replacement information for {}",replaceAddress
"Skipping endpoint collision check as cassandra.allow_unsafe_join=true"
"Starting shadow gossip round to check for endpoint collision"
"Unable to gossip with any peers but continuing anyway since node is in its own seed list"
"Populating token metadata from system tables"
"Token metadata: {}",tokenMetadata
"Cassandra version: {}",FBUtilities.getReleaseVersionString()
"CQL version: {}",QueryProcessor.CQL_VERSION
"Native protocol supported versions: {} (default: {})",StringUtils.join(ProtocolVersion.supportedVersions(),", "),ProtocolVersion.CURRENT
"Not starting gossip as requested."
"Error loading counter cache",t
"Not joining ring as requested. Use JMX (StorageService->joinRing()) to initiate ring joining"
"Loading persisted ring state"
"Replace address on first boot requested; this node is already bootstrapped"
"This node was decommissioned, but overriding by operator request."
"Starting up server gossip"
"current schema version: {}",Schema.instance.getVersion()
"Bootstrap variables: {} {} {} {}",DatabaseDescriptor.isAutoBootstrap(),SystemKeyspace.bootstrapInProgress(),SystemKeyspace.bootstrapComplete(),DatabaseDescriptor.getSeeds().contains(FBUtilities.getBroadcastAddressAndPort())
"This node will not auto bootstrap because it is configured to be a seed node."
"Detected previous bootstrap failure; retrying"
"... got ring + schema info"
"Using saved tokens {}",bootstrapTokens
"Some data streaming failed. Use nodetool to check bootstrap state and resume. For more, see `nodetool help bootstrap`. {}",SystemKeyspace.getBootstrapState()
"Startup complete, but write survey mode is active, not becoming an active ring member. Use JMX (StorageService->joinRing()) to finalize ring joining."
"Some data streaming failed. Use nodetool to check bootstrap state and resume. For more, see `nodetool help bootstrap`. {}",SystemKeyspace.getBootstrapState()
"Joining ring by operator request"
"Leaving write survey mode and joining ring at operator request"
"Can't join the ring because in write_survey mode and bootstrap hasn't completed"
"Attempted to create new keyspace {}, but it already exists",ksm.name
"rebuild from dc: {}, {}, {}",sourceDcEQEQnullQUES"(any dc)":sourceDc,keyspaceEQEQnullQUES"(All keyspaces)":keyspace,tokensEQEQnullQUES"(All tokens)":tokens
"adding range: ({},{}]",startToken,endToken
"set rpc timeout to {} ms",value
"set read rpc timeout to {} ms",value
"set range rpc timeout to {} ms",value
"set write rpc timeout to {} ms",value
"set internode tcp connect timeout to {} ms",value
"set internode tcp user timeout to {} ms",value
"set counter write rpc timeout to {} ms",value
"set cas contention rpc timeout to {} ms",value
"set truncate rpc timeout to {} ms",value
"setstreamthroughput: throttle set to {}",value
"setinterdcstreamthroughput: throttle set to {}",value
logMsg
logMsg
"Resetting bootstrap progress to start fresh"
"Bootstrap completed! for the tokens {}",tokens
"Error during bootstrap.",e
"Error while waiting on bootstrap to complete. Bootstrap will have to be restarted.",e
"Resuming bootstrap..."
"Startup complete, but write survey mode is active, not becoming an active ring member. Use JMX (StorageService->joinRing()) to finalize ring joining."
"Resume complete"
message,e
"Resuming bootstrap is requested, but the node is already bootstrapped."
"Ignoring state change for dead or unknown endpoint: {}",endpoint
"Node {} state bootstrapping, token {}",endpoint,tokens
"Node {} state jump to bootstrap",endpoint
"Node {} tried to replace malformed endpoint {}.",newNode,piecesLBRACKET1RBRACKET,e
"Node {} is replacing {}, tokens {}",newNode,oldNode,tokens
"Node {} state {}, tokens {}",endpoint,status,tokens
"Node {} state jump to {}",endpoint,status
"Node {} '{}' token mismatch. Long network partition?",endpoint,status
"New node {} at token {}",endpoint,token
"Nodes {} and {} have the same token {}. {} is the new owner",endpoint,currentOwner,token,endpoint
"Nodes () and {} have the same token {}.  Ignoring {}",endpoint,currentOwner,token,endpoint
"Node {} state {}, token {}",endpoint,status,tokens
"Node {} state jump to {}",endpoint,status
"Node {} is in state normal but it has no tokens, state: {}",endpoint,Gossiper.instance.getEndpointStateForEndpoint(endpoint)
"Node {} will complete replacement of {} for tokens {}",endpoint,replacingNode.get(),tokens
"Node {} cannot complete replacement of alive node {}.",endpoint,replacingNode.get()
"Node {} is currently being replaced by node {}.",endpoint,replacementNode.get()
"Not updating token metadata for {} because I am replacing it",endpoint
"Not updating host ID {} for {} because it's mine",hostId,endpoint
"Host ID collision for {} between {} and {}; {} is the new owner",hostId,existing,endpoint,endpoint
"Host ID collision for {} between {} and {}; ignored {}",hostId,existing,endpoint,endpoint
"Node {} state left, tokens {}",endpoint,tokens
"Node {} state moving, new token {}",endpoint,token
"Received removenode gossip about myself. Is this node rejoining after an explicit removenode?"
"Tokens {} removed manually (endpoint was {})",removeTokens,endpoint
"Removing tokens {} for {}",tokens,endpoint
"Getting new source replicas for {}",leavingReplicas
"Possible replicas for newReplica {} are {}",ourReplica,possibleReplicas
"Sorted possible replicas starts as {}",sortedPossibleReplicas
"Skipping down replica {}",possibleReplica
"Didn't find live replica to restore replication for " + ourReplica
"Notifying {} of replication completion\n",remote
"Restoring replica count for keyspace {}",keyspaceName
"Changed replicas for leaving {}, myNewReplicas {}",changedReplicas,myNewReplicas
"Requesting keyspace {} sources",keyspaceName
"Source and our replicas are {}",fetchReplicas
"Requesting from {} full replicas {} transient replicas {}",sourceAddress,StringUtils.join(full,", "),StringUtils.join(transientReplicas,", ")
"Streaming to restore replica count failed",t
"Node {} replicas [{}]",endpoint,StringUtils.join(replicas,", ")
"Replica {} already in all replicas",replica
"Replica {} will be responsibility of {}",replica,StringUtils.join(newReplicaEndpoints,", ")
"Verifying {}.{} with options = {}",keyspaceName,Arrays.toString(tableNames),options
"Cleared out snapshot directories"
"Forcing flush on keyspace {}, CF {}",keyspaceName,cfStore.name
"computing ranges for {}",StringUtils.join(sortedTokens,", ")
"DECOMMISSIONING"
"failed to shutdown message service: {}",ioe
"Error while decommissioning node ",e.getCause()
"Announcing that I have left the ring for {}ms",delay
"Ranges needing transfer are [{}]",StringUtils.join(rangesMM.keySet(),",")
"stream acks all received."
"Unable to stream hints since no live endpoints seen"
"Invalid request to move(Token); This node has more than one token and cannot be moved thusly."
"Successfully moved to new token {}",getLocalTokens().iterator().next()
"Removal not confirmed for for {}",StringUtils.join(this.replicatingNodes,",")
"No nodes to force removal on, call 'removenode' first"
"Node {} is already being removed, continuing removal anyway",endpoint
"Endpoint {} is down and will not receive data for re-replication of {}",ep,endpoint
"Received unexpected REPLICATION_FINISHED message from {}. Was this node recently a removal coordinator?",node
"Cannot drain node (did it already happen?)"
"Attempting to continue draining after pre-shutdown hooks returned exception",preShutdownHookThrowable
"Batchlog manager timed out shutting down",t
"Failed to wait for non periodic tasks to shutdown"
"Caught an exception while draining ",t
"Post-shutdown hooks returned exception",postShutdownHookThrowable
"Created new dynamic snitch {} with update-interval={}, reset-interval={}, badness-threshold={}",((DynamicEndpointSnitch)newSnitch).subsnitch.getClass().getName(),DatabaseDescriptor.getDynamicUpdateInterval(),DatabaseDescriptor.getDynamicResetInterval(),DatabaseDescriptor.getDynamicBadnessThreshold()
"Created new non-dynamic snitch {}",newSnitch.getClass().getName()
"Applying config change to dynamic snitch {} with update-interval={}, reset-interval={}, badness-threshold={}",((DynamicEndpointSnitch)oldSnitch).subsnitch.getClass().getName(),DatabaseDescriptor.getDynamicUpdateInterval(),DatabaseDescriptor.getDynamicResetInterval(),DatabaseDescriptor.getDynamicBadnessThreshold()
"Skipping transferred range {} of keyspace {}, endpoint {}",local,keyspace,remote
"Updated batch_size_fail_threshold_in_kb to {}",threshold
"Updated batch_size_warn_threshold_in_kb to {}",threshold
"Updated hinted_handoff_throttle_in_kb to {}",throttleInKB
"Cleared connection history"
"Auditlog is disabled"
"AuditLog is enabled with logger: [{}], included_keyspaces: [{}], excluded_keyspaces: [{}], " + "included_categories: [{}], excluded_categories: [{}], included_users: [{}], " + "excluded_users: [{}], archive_command: [{}]",loggerName,auditLogOptions.included_keyspaces,auditLogOptions.excluded_keyspaces,auditLogOptions.included_categories,auditLogOptions.excluded_categories,auditLogOptions.included_users,auditLogOptions.excluded_users,auditLogOptions.archive_command
"Setting corrupted tombstone strategy to {}",strategy
"Error accessing field of java.nio.Bits",t
sb.toString()
sb.toString()
sb.toString()
"Error accessing field of java.nio.Bits",t
"Exiting forcefully due to file system exception on startup, disk failure policy \"{}\"",DatabaseDescriptor.getDiskFailurePolicy(),t
"jemalloc shared library could not be preloaded to speed up memory allocations"
"jemalloc preload explicitly disabled"
"jemalloc seems to be preloaded from {}",jemalloc
"JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info."
"cassandra.jmx.local.port missing from cassandra-env.sh, unable to start local JMX service."
"JMX is enabled to receive remote connections on port: {}",jmxPort
"Use of com.sun.management.jmxremote.port at startup is deprecated. " + "Please use cassandra.jmx.remote.port instead."
"32bit JVM detected.  It is recommended to run Cassandra on a 64bit JVM for better performance."
"Non-Oracle JVM detected.  Some features, such as immediate unmap of compacted SSTables, may not work as intended"
"The JVM is not configured to stop on OutOfMemoryError which can cause data corruption." + " Use one of the following JVM options to configure the behavior on OutOfMemoryError: " + " -XX:+ExitOnOutOfMemoryError, -XX:+CrashOnOutOfMemoryError, or -XX:OnOutOfMemoryError=\"<cmd args>;<cmd args>\""
"The JVM is not configured to stop on OutOfMemoryError which can cause data corruption." + " Either upgrade your JRE to a version greater or equal to 8u92 and use -XX:+ExitOnOutOfMemoryError/-XX:+CrashOnOutOfMemoryError" + " or use -XX:OnOutOfMemoryError=\"<cmd args>;<cmd args>\" on your current JRE."
"Unable to parse {}.",path,e
"IO exception while reading file {}.",path,e
"Maximum number of memory map areas per process (vm.max_map_count) {} " + "is too low, recommended value: {}, you can change it with sysctl.",maxMapCount,EXPECTED_MAX_MAP_COUNT
"Checking directory {}",dataDir
"Directory {} doesn't exist",dataDir
"Finished PendingRangeTask for {} keyspaces in {}ms",keyspaces.size(),System.currentTimeMillis()SUBstart
"Disseminating load info ..."
"Requesting {} extra rows from {} for short read protection",toQuery,source
"responsesMatch: {} ms.",TimeUnit.NANOSECONDS.toMillis(System.nanoTime()SUBstart)
"Encountered error while trying to preprocess the message {}, in command {}, replica plan: {}",message,command,replicaPlan
"reading {} locally",readCommand.isDigestQuery()QUES"digest":"data"
"speculating read retry on {}",extraReplica
"Timed out waiting on digest mismatch repair requests"
"Blockfor is {}; setting up requests to {}",blockFor,this.replicaPlan
"{}; received {} of {} responses{}",NEWObjectLBRACKETRBRACKETLBRACE(failedQUES"Failed":"Timed out"),received,blockFor,gotDataRBRACE
"Timeout while read-repairing after receiving all {} data and digest responses",blockFor
"Encountered an oversized ({}/{}) read repair mutation for table {}, key {}, node {}",mutationSize,maxMutationSize,metadata,metadata.partitionKeyType.getString(key.getKey()),destination
"Encountered an oversized ({}/{}) read repair mutation for table {}, key {}, node {}",mutationSize,maxMutationSize,metadata,metadata.partitionKeyType.getString(key.getKey()),destination
"Propose response {} from {}",msg.payload,msg.from()
"Prepare response {} from {}",response,message.from()
"JNA not found. Native methods will be disabled."
"Failed to link the C library against JNA. Native methods will be unavailable.",e
"Obsolete version of JNA present; unable to register C library. Upgrade to JNA 3.2.7 or later"
"Configured JMX server at: {}",url
"JMX SSL configuration. { protocols: [{}], cipher_suites: [{}], require_client_auth: {} }",serverFactory.getEnabledProtocols()EQEQnullQUES"'JVM defaults'":Arrays.stream(serverFactory.getEnabledProtocols()).collect(Collectors.joining("','","'","'")),serverFactory.getEnabledCipherSuites()EQEQnullQUES"'JVM defaults'":Arrays.stream(serverFactory.getEnabledCipherSuites()).collect(Collectors.joining("','","'","'")),serverFactory.getNeedClientAuth()
"Initializing SIGAR library"
"Could not initialize SIGAR library {} ",e.getMessage()
"Could not initialize SIGAR library {} ",linkError.getMessage()
"Could not determine if max processes was acceptable. Error message: {}",sigarException
"Could not determine if max open file handle limit is correctly configured. Error message: {}",sigarException
"Could not determine if VirtualMemoryMax was acceptable. Error message: {}",sigarException
"Could not determine if swap configuration is acceptable. Error message: {}",sigarException
"Cassandra server running in degraded mode. Is swap disabled? : {},  Address space adequate? : {}, " + " nofile limit adequate? : {}, nproc limit adequate? : {} ",BANGswapEnabled,goodAddressSpace,goodFileLimits,goodProcNumber
"Checked OS settings and found them configured for optimal performance."
"Sigar could not be initialized, test for checking degraded mode omitted."
"Digest mismatch detected among leaf nodes {}, {}",lnode,rnode
"Digest mismatch detected, traversing trees [{}, {}]",ltree,rtree
"Range {} fully inconsistent",active
"({}) Hashing sub-ranges [{}, {}] for {} divided by midpoint {}",active.depth,left,right,active,midpoint
"({}) Inconsistent digest on left sub-range {}: [{}, {}]",active.depth,left,lnode,rnode
"({}) Left sub-range fully inconsistent {}",active.depth,left
"({}) Inconsistent digest on right sub-range {}: [{}, {}]",active.depth,right,lnode,rnode
"({}) Right sub-range fully inconsistent {}",active.depth,right
"({}) Adding left sub-range to diff as fully inconsistent {}",active.depth,left
"({}) Adding right sub-range to diff as fully inconsistent {}",active.depth,right
"({}) Range {} partially inconstent",active.depth,active
"Configuration requests off-heap merkle trees, but partitioner does not support it. Ignoring."
"Allocating direct buffer of size {} for an off-heap merkle tree",size
"Will try to load mx4j now, if it's in the classpath"
"mx4j successfuly loaded"
"Will not load MX4J, mx4j-tools.jar is not in the classpath"
"Could not start register mbean in JMX",e
msg
msg
msg
msg,th
"Scheduling approximate time-check task with a precision of {} milliseconds",ALMOST_NOW_UPDATE_INTERVAL_MS
"Scheduling approximate time conversion task with an interval of {} milliseconds",ALMOST_SAME_TIME_UPDATE_INTERVAL_MS
"StatusLogger is busy"
String.format("%-28s%10s%10s%15s%10s%18s",tpool.poolName,tpool.activeTasks.getValue(),tpool.pendingTasks.getValue(),tpool.completedTasks.getValue(),tpool.currentBlocked.getCount(),tpool.totalBlocked.getCount())
String.format("%-25s%10s%10s","MessagingService","n/a",pendingLargeMessages + "/" + pendingSmallMessages)
String.format("%-25s%10s%25s%25s","Cache Type","Size","Capacity","KeysToSave")
String.format("%-25s%10s%25s%25s","KeyCache",keyCache.weightedSize(),keyCache.getCapacity(),keyCacheKeysToSaveEQEQInteger.MAX_VALUEQUES"all":keyCacheKeysToSave)
String.format("%-25s%10s%25s%25s","RowCache",rowCache.weightedSize(),rowCache.getCapacity(),rowCacheKeysToSaveEQEQInteger.MAX_VALUEQUES"all":rowCacheKeysToSave)
String.format("%-25s%20s",cfs.keyspace.getName() + "." + cfs.name,cfs.metric.memtableColumnsCount.getValue() + "," + cfs.metric.memtableLiveDataSize.getValue())
"Error in MBean wrapper: ",e
String.format(formatstr,namesLBRACKETiRBRACKET,count)
String.format("Timed run of %s failed.",callback.getClass()),t
"Creating IntervalNode from {}",toBisect
"Trigger directory doesn't exist, please create it and try again."
"Unable to load version.properties",e
"JNA not found. winmm.dll cannot be registered. Performance will be negatively impacted on this node."
"Failed to register winmm.dll. Performance will be negatively impacted on this node."
"Failed to set timer to : {}. Performance will be degraded.",period
"Failed to end accelerated timer period. System timer will remain set to: {} ms.",period
"Underlying string constructor threw an error: {}",causeEQEQnullQUESite.getMessage():cause.getMessage()
"JNA not found. Native methods will be disabled."
"Failed to link the C library against JNA. Native methods will be unavailable.",e
"Obsolete version of JNA present; unable to register C library. Upgrade to JNA 3.2.7 or later"
"Using custom clock implementation: {}",sclock
e.getMessage(),e
"Using custom clock implementation: {}",sclock
e.getMessage(),e
"Scheduling approximate time conversion task with an interval of {} milliseconds",UPDATE_INTERVAL_MS
"Scheduling approximate time-check task with a precision of {} milliseconds",UPDATE_INTERVAL_MS
"Skip fsync enabled due to property {} and environment {}",skipSyncProperty,skipSyncEnv
"Cannot provide an optimal BloomFilter for {} elements ({}/{} buckets per element).",numElements,bucketsPerElement,targetBucketsPerElem
"Some JRE information could not be retrieved for the JRE version: " + jreVersion,e
"OutOfMemory error letting the JVM handle the error:",t
"Exiting due to error while processing commit log during initialization.",t
t.getMessage()
"JVM state determined to be unstable.  Exiting forcefully due to:",t
"JNA not found. Native methods will be disabled."
"Failed to link the Windows/Kernel32 library against JNA. Native methods will be unavailable.",e
"Obsolete version of JNA present; unable to register Windows/Kernel32 library. Upgrade to JNA 3.2.7 or later"
"Trying to log the heap histogram using jcmd"
"The process ID could not be retrieved. Skipping heap histogram generation."
"The heap histogram could not be generated due to the following error: ",e
builder.toString()
"the current operating system, {}, is unsupported by cassandra",osName
"Obsolete version of JNA present; unable to read errno. Upgrade to JNA 3.2.7 or later"
"JNA mlockall successful"
"Unable to lock JVM memory (ENOMEM)." + " This can result in part of the JVM being swapped out, especially with mmapped I/O enabled." + " Increase RLIMIT_MEMLOCK or run Cassandra as root."
"Could not skip cache",e
"posix_fadvise({}, {}) failed, errno ({}).",fd,offset,errno(e)
"fcntl({}, {}, {}) failed, errno ({}).",fd,command,flags,errno(e)
"open({}, O_RDONLY) failed, errno ({}).",path,errno(e)
errMsg
errMsg
"Unable to read fd field from FileChannel"
"Unable to read fd field from FileDescriptor"
"Failed to get PID from JNA",e
"Got error archiving {}, retrying in {} minutes",toArchive.file,TimeUnit.MINUTES.convert(retryDelayMs,TimeUnit.MILLISECONDS),t
"Max retries {} reached for {}, leaving on disk",toArchive.retries,toArchive.file,t
"Got error waiting for files to archive",t
"Exiting archiver thread"
"BinLog file released: {}",file
"Archiving existing file {}",f
"Got error archiving existing file {}",f,e
"Executing archive command: {}",cmd
"Unexpected exception in binary log thread",t
"Chronicle store file {} rolled file size {}",file.getPath(),file.length()
"Failed to delete chronicle store file: {} store file size: {} bytes in store files: {}. " + "You will need to clean this up manually or reset full query logging.",toDelete.getPath(),toDeleteLength,bytesInStoreFiles
"Deleted chronicle store file: {} store file size: {} bytes in store files: {} max log size: {}.",file.getPath(),toDeleteLength,bytesInStoreFiles,maxLogSize
"BAD RELEASE: attempted to release a reference ({}) that has already been released",id
"LEAK DETECTED: a reference ({}) to {} was not released before the reference was garbage collected",id,globalState
"Error when closing {}",globalState,fail
"Allocate trace {}:\n{}",id,print(allocateThread,allocateTrace)
"Deallocate trace {}:\n{}",id,print(deallocateThread,deallocateTrace)
"Strong reference leak candidates detected: {}",names
"Global buffer pool is disabled, allocating {}",ALLOCATE_ON_HEAP_WHEN_EXAHUSTEDQUES"on heap":"off heap"
"Global buffer pool is enabled, when pool is exhausted (max is {}) it will allocate {}",prettyPrintMemory(MEMORY_USAGE_THRESHOLD),ALLOCATE_ON_HEAP_WHEN_EXAHUSTEDQUES"on heap":"off heap"
"Requested buffer size {} is bigger than {}; allocating directly",prettyPrintMemory(size),prettyPrintMemory(NORMAL_CHUNK_SIZE)
"Requested buffer size {} has been allocated directly due to lack of capacity",prettyPrintMemory(size)
"{} regions now allocated in {}",regionCount,this
"You are using Cassandra with an unsupported deployment. The intended logging implementation library logback is not used by slf4j. Detected slf4j logger factory: {}. " + "You will not be able to dynamically manage log levels via JMX and may have performance or other issues.",loggerFactoryClass
"set log level to {} for classes under '{}' (if the level doesn't look like '{}' then the logger couldn't parse '{}')",level,classQualifier,rawLevel,rawLevel
"The log level was not changed, because you are using an unsupported slf4j logging implementation for which this functionality was not implemented."
"An empty map of logger names and their logging levels was returned, because you are using an unsupported slf4j logging implementation for which this functionality was not implemented."
"No defined indexes with the supplied names: {}",Joiner.on(,).join(indexNames)
"Submitting index build of {} for data in {}",indexes.stream().map(iARROWi.getIndexMetadata().name).collect(Collectors.joining(",")),sstables.stream().map(SSTableReaderCOLCOLtoString).collect(Collectors.joining(","))
"Index build of {} completed",getIndexNames(groupedIndexes)
"Index flush of {} failed",indexNames
"Index flush of {} completed",indexNames
"Index build of {} failed. Please run full index rebuild to fix it.",getIndexNames(indexes),indexBuildFailure
"Index build of {} failed. Please run full index rebuild to fix it.",getIndexNames(indexes)
"Executing pre-join{} tasks for: {}",hadBootstrapQUES" post-bootstrap":"",this.baseCfs
"Indexing partition {}",baseCfs.metadata().partitionKeyType.getString(key.getKey())
"Calculated page size {} for indexing {}.{} ({}/{}/{}/{})",pageSize,baseCfs.metadata.keyspace,baseCfs.metadata.name,meanPartitionSize,meanCellsPerPartition,meanRowsPerPartition,meanRowSize
"Command contains a custom index expression, using target index {}",customExpression.getTargetIndex().name
"No applicable indexes found"
"Registered index {}",name
removedEQEQnullQUES"Index {} was not registered":"Removed index {} from registry",name
"failed to flush indexes: {} because flush task is missing.",indexers
"Removed index entry for stale value {}",indexKey
"Inserted entry into index for value {}",valueKey
"Removed index entry for value {}",indexKey
"No SSTable data for {}.{} to build index {} from, marking empty index as built",baseCfs.metadata.keyspace,baseCfs.metadata.name,metadata.name
"Submitting index build of {} for data in {}",metadata.name,getSSTableNames(sstables)
"Index build of {} complete",metadata.name
"Search Concurrency Factor is set to {} for {}",concurrencyFactor,currentThread
String.format("Failed search an index %s, skipping.",index.getPath()),e1
String.format("Failed to release index %s",index.getPath()),e
"Can't cast value for {} to size accepted by {}, value size is {}.",index.getColumnName(),validator,FBUtilities.prettyPrintMemory(size)
"Failed to create new instance of analyzer with class [{}]",analyzerClass.getName(),e
"failed to parse {} option, defaulting to 'false'.",INDEX_IS_LITERAL_OPTION
"SSTableIndex.open(column: {}, minTerm: {}, maxTerm: {}, minKey: {}, maxKey: {}, sstable: {})",columnIndex.getColumnName(),columnIndex.getValidator().getString(index.minTerm()),columnIndex.getValidator().getString(index.maxTerm()),keyValidator.getString(index.minKey()),keyValidator.getString(index.maxKey()),index.getSSTable()
"Can't open index file at " + indexFile.getAbsolutePath() + ", skipping.",t
"Rejecting value (size {}, maximum {}) for column {} (analyzed {}) at {} SSTable.",FBUtilities.prettyPrintMemory(term.remaining()),FBUtilities.prettyPrintMemory(OnDiskIndexBuilder.MAX_TERM_SIZE),columnIndex.getColumnName(),columnIndex.getMode().isAnalyzed,descriptor
"({}) Failed to add {} to index for key: {}, value size was {}, validator is {}.",outputFile,columnIndex.getColumnName(),keyValidator.getString(key.getKey()),FBUtilities.prettyPrintMemory(size),columnIndex.getValidator()
"Failed to build index segment {}",segmentFile,e
"Flushed index segment {}, took {} ms.",segmentFile,TimeUnit.NANOSECONDS.toMillis(System.nanoTime()SUBstart)
"Scheduling index flush to {}",outputFile
"Failed to flush index {}.",outputFile,e
"Index flush to {} took {} ms.",outputFile,TimeUnit.NANOSECONDS.toMillis(System.nanoTime()SUBstart1)
"Rejecting value (value size {}, maximum size {}).",FBUtilities.prettyPrintMemory(term.remaining()),FBUtilities.prettyPrintMemory(Short.MAX_VALUE)
"Failed to deserialize value with " + validator,e
"Failed to get stemmer constructor",e
"Failed to create new SnowballStemmer instance " + "for language [{}]",locale.getLanguage(),e
"Failed to populate Stop Words Cache for language [{}]",locale.getLanguage(),e
"Failed to retrieve Stop Terms resource for language [{}]",language,e
"An unhandled exception to occurred while processing " + "pipeline [{}]",task.getName(),e
"Can't add term of column {} to index for key: {}, term size {}, max allowed size {}, use analyzed = true (if not yet set) for that column.",columnIndex.getColumnName(),keyValidator.getString(key.getKey()),FBUtilities.prettyPrintMemory(term.remaining()),FBUtilities.prettyPrintMemory(OnDiskIndexBuilder.MAX_TERM_SIZE)
"Can't add column {} to index for key: {}, value size {}, validator: {}.",index.columnIndex.getColumnName(),index.columnIndex.keyValidator().getString(key.getKey()),FBUtilities.prettyPrintMemory(size),validator
"Beginning bootstrap process"
"manually specified tokens override automatic allocation"
"Picking random token for a single vnode.  You should probably add more vnodes and/or use the automatic token allocation mechanism."
"tokens manually specified as {}",initialTokens
"Generated random tokens. tokens are {}",tokens
"Using other DC endpoints for streaming for range: {} and keyspace {}",trivialRange,keyspace
"Not optimising trivial range {} for keyspace {}",range,keyspace
"Using other DC endpoints for streaming for range: {} and keyspace {}",range,keyspace
"Not adding ranges for Local Strategy keyspace={}",keyspaceName
"{}: range {} exists on {} for keyspace {}",description,entry.getKey(),entry.getValue(),keyspaceName
"{}: range source {} local range {} for keyspace {}",description,r.remote,r.local,keyspaceName
"Keyspace: {}",keyspace
"To fetch RN: {}",fetchRanges
"Fetch ranges: {}",rangeAddresses
"To fetch {}",toFetch
"Old endpoints {}",oldEndpoints
"New endpoints {}",newEndpoints
"Endpoints to fetch for {} are {}",toFetch,sources
"A node required to move the data consistently is down"
"Unable to find sufficient sources for streaming range {} in keyspace {} with RF=1. " + "Keyspace might be missing data.",toFetch,keyspace
"A node required to move the data consistently is down"
"Work map {}",workMap
"Output from RangeFetchMapCalculator for keyspace {}",keyspace
"Streaming range {} from endpoint {} for keyspace {}",entry.getValue(),entry.getKey(),keyspace
"Keyspace {} Sources {}",keyspace,sources
"Some ranges of {} are already available. Skipping streaming those ranges. Skipping {}. Fully available {} Transiently available {}",fetchReplicas,skipped,available.full,available.trans
"{}ing from {} ranges {}",description,source,StringUtils.join(remaining,", ")
"Source and our replicas {}",fetchReplicas
"Source {} Keyspace {}  streaming full {} transient {}",source,keyspace,full,transientReplicas
"Using NoReplicationTokenAllocator."
"Using ReplicationAwareTokenAllocator."
"Selected tokens {}",tokens
"Replicated node load in datacenter before allocation {}",statToString(os)
"Replicated node load in datacenter after allocation {}",statToString(ns)
"Unexpected growth in standard deviation after allocation."
"Loading new jar {}",inputJar.getAbsolutePath()
"Class not found using parent class loader,",ex
"[Stream #{}] Executing streaming plan for {}",planId,streamOperation.getDescription()
"[Stream #{} ID#{}] Creating new streaming plan for {} from {} channel.remote {} channel.local {}" + " channel.id {}",planId,sessionIndex,streamOperation.getDescription(),from,channel.remoteAddress(),channel.localAddress(),channel.id()
"[Stream #{}, ID#{}] Received streaming plan for {} from {} channel.remote {} channel.local {} channel.id {}",planId,sessionIndex,streamOperation.getDescription(),from,channel.remoteAddress(),channel.localAddress(),channel.id()
"[Stream #{} ID#{}] Prepare completed. Receiving {} files({}), sending {} files({})",session.planId(),session.sessionIndex(),sessionInfo.getTotalFilesToReceive(),FBUtilities.prettyPrintMemory(sessionInfo.getTotalSizeToReceive()),sessionInfo.getTotalFilesToSend(),FBUtilities.prettyPrintMemory(sessionInfo.getTotalSizeToSend())
"[Stream #{}] Session with {} is complete",session.planId(),session.peer
"[Stream #{}] Stream failed",planId
"[Stream #{}] All sessions completed",planId
"Connecting next session {} with {}.",next.planId(),next.peer.toString()
"Finished connecting all sessions"
"[Stream #{}, ID#{}] Beginning stream session with {}",session.planId(),session.sessionIndex(),session.peer
"[{}] Received stream {} on already finished stream received task. Aborting stream.",session.planId(),stream.getName()
"received {} of {} total files {} of total bytes {}",remoteStreamsReceived,totalStreams,bytesReceived,totalSize
"Creating stream session to {}",template
"[Stream #{}] Session does not have any tasks.",planId()
"[Stream #{}] Starting streaming to {}{}",planId(),peer,template.connectToEQEQnullQUES"":" through " + template.connectTo
"failed to abort some streaming tasks",e
"[Stream #{}] Did not receive response from peer {}{} for {} secs. Is peer down? " + "If not, maybe try increasing streaming_keep_alive_period_in_secs.",planId(),peer.getHostAddress(true),template.connectToEQEQnullQUES"":" through " + template.connectTo.getHostAddress(true),2STARDatabaseDescriptor.getStreamingKeepAlivePeriod(),e
"[Stream #{}] Streaming error occurred on session with peer {}{}",planId(),peer.getHostAddress(true),template.connectToEQEQnullQUES"":" through " + template.connectTo.getHostAddress(true),e
"handling Complete message, state = {}, completeSent = {}",state,completeSent
"[Stream #{}] Remote peer {} failed stream session.",planId(),peer.toString()
"[Stream #{}] Session failed because remote peer {} has left.",planId(),peer.toString()
"[Stream #{}] Session failed because remote peer {} was restarted.",planId(),peer.toString()
"recevied sequenceNumber {}, remaining files {}",sequenceNumber,streams.keySet()
"Replying to {}@{}",msg.id(),msg.from()
"{} Scheduling keep-alive task with {}s period.",createLogTag(session,channel),keepAlivePeriod
"Creating channel id {} local {} remote {}",channel.id(),channel.localAddress(),channel.remoteAddress()
"{} Sending {}",createLogTag(session,null),message
"{} Sending {}",createLogTag(session,channel),message
"{} failed to send a stream message/data to peer {}: msg = {}",createLogTag(session,channel),template.to,msg,future.cause()
"{} waiting to acquire a permit to begin streaming {}. This message logs every {} minutes",createLogTag(session,null),ofm.getName(),logInterval
"{} Sending keep-alive to {}.",createLogTag(session,channel),session.peer
"{} Could not send keep-alive message (perhaps stream session is finished?).",createLogTag(session,channel),future.cause()
"{} Closing stream connection channels on {}",createLogTag(session,null),template.to
"connection problem while streaming",cause
"exception occurred while in processing streaming data",cause
"{} Received {}",createLogTag(session,channel),message
"{} Received {}",createLogTag(session,channel),message
"{} stream operation from {} failed",createLogTag(session,channel),remoteAddress,t
"Token {} changing ownership from {} to {}",token,prev,endpoint
"Changing {}'s host ID from {} to {}",endpoint,storedId,hostId
"Replacing {} with {}",newNode,oldNode
"Node {} failed during replace.",endpoint
"Updating topology for {}",endpoint
"Updating topology for all endpoints that have changed"
"No bootstrapping, leaving or moving nodes -> empty pending ranges for {}",keyspaceName
"No bootstrapping, leaving or moving nodes -> empty pending ranges for {}",keyspaceName
"Starting pending range calculation for {}",keyspaceName
"Pending range calculation for {} completed (took: {}ms)",keyspaceName,took
"Calculated pending ranges for {}:\n{}",keyspaceName,(pendingRanges.isEmpty()QUES"<empty>":printPendingRanges())
"Error in getting the IP address resolved: ",e
"InternodeAuthenticator said don't reconnect to {} on {}",publicAddress,localAddress
"Initiated reconnect to an Internal IP {} for the {}",localAddress,publicAddress
"clearing cached endpoints"
"Ignoring {}",e.getMessage()
String.format("Local replicas %s are insufficient to satisfy LOCAL_QUORUM requirement of %d live replicas and %d full replicas in '%s'",allLive.filter(InOurDcTester.replicas()),blockFor,blockForFullReplicas,DatabaseDescriptor.getLocalDataCenter())
"Live nodes {} do not satisfy ConsistencyLevel ({} required)",Iterables.toString(allLive),blockFor
"EC2Snitch using region: {}, zone: {}.",ec2region,ec2zone
"This ec2-enabled snitch appears to be using the {} naming scheme for regions, " + "but existing nodes in cluster are using the opposite: region(s) = {}, availability zone(s) = {}. " + "Please check the {} property in the {} configuration file for more details.",usingLegacyNamingQUES"legacy":"standard",datacenters,racks,SNITCH_PROP_NAMING_SCHEME,SnitchProperties.RACKDC_PROPERTY_FILENAME
"EC2Snitch using publicIP as identifier: {}",localPublicAddress
"broadcast_rpc_address unset, broadcasting public IP as rpc_address: {}",localPublicAddress
"Configured datacenter replicas are {}",FBUtilities.toString(datacenters)
"GCESnitch using region: {}, zone: {}.",gceRegion,gceZone
"Loaded {} for compatibility",PropertyFileSnitch.SNITCH_PROPERTIES_FILENAME
"Unable to load {}; compatibility mode disabled",PropertyFileSnitch.SNITCH_PROPERTIES_FILENAME
"{} found, but does not look like a plain file. Will not watch it for changes",SNITCH_PROPERTIES_FILENAME
"Could not find end point information for {}, will use default",endpoint
"Loaded network topology from property file: {}",StringUtils.removeEnd(sb.toString(),", ")
"Cannot update data center or rack from {} to {} for live host {}, property file NOT RELOADED",origValue,updateValue,host
"Started replayFailedBatches"
"Replay cancelled as there are no peers in the ring."
"Finished replayFailedBatches"
"Updating batchlog replay throttle to {} KB/s, {} KB/s per endpoint",throttleInKB,endpointThrottleInKB
"Skipped batch replay of {} due to {}",id,e.getMessage()
String.format("Encountered %d unexpected exceptions while sending out batches",skipped),caughtException
"Replaying batch {}",id
"Failed replaying a batched mutation to a node, will write a hint"
"Failure was : {}",e.getMessage()
"Failure serializing partition key.",e
"Fatal error parsing partition: {}",key,e
"Fatal error parsing row.",e
"Failure parsing tombstone.",e
"Failure parsing ColumnData.",e
"Failure parsing cell.",e
GT0)StatusLogger.log(
"{} has {} dropped hints, because node is down past configured hint window.",entry.getKey(),difference
"Invalid negative latency in hint delivery delay: {}",delay
"Failure to offer sample",e
sb.toString(),mismatchUUID,query
"ERROR notifying listener",t
sb.toString(),mismatchUUID,query
"ERROR notifying listener",t
"Query {} against {} failure: {}",query,targetHosts.get(i),results.get(i).getFailureException().getMessage()
"Executing query: {}",query
"QUERY %s got exception: %s",query,t.getMessage()
String.format("%d queries, rate = %.2f",timer.getCount(),timer.getOneMinuteRate())
"Switching keyspace from {} to {}",session.getLoggedKeyspace(),query.keyspace()
"USE {} failed: {}",query.keyspace(),t.getMessage()
"Could not close connection",t
