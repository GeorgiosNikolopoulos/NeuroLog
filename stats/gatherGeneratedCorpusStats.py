import argparse
from graph_pb2 import Graph
import json
from pathlib import Path
from tqdm import tqdm


def main():
    with open(jsonLoc) as json_file:
        logs = json.load(json_file)
        calculateImportLeaks(logs)

# Examines how many log levels are "leaked" as imports
def calculateImportLeaks(logs):
    graphLocs = map(lambda log: corpusLocation / Path(log["fileLoc"] + ".proto"), logs)
    # remove duplicates
    graphLocs = list(set(graphLocs))
    totalNodes = 0
    debug = error = fatal = info = trace = warn = 0
    for graphLoc in tqdm(graphLocs, unit="graph file"):
        with open(graphLoc, "rb") as graphFile:
            g = Graph()
            g.ParseFromString(graphFile.read())
            totalNodes += len(g.node)
            for node in g.node:
                # using "." and "log" and " " not in node.contents does not work well
                if "." in node.contents and "log" in node.contents and " " not in node.contents:
                    if "trace" in node.contents:
                        trace += 1
                    if "debug" in node.contents:
                        debug += 1
                    if "info" in node.contents:
                        info += 1
                    if "error" in node.contents:
                        error += 1
                    if "fatal" in node.contents:
                        fatal += 1
    totalImportDangers = trace + debug + info + error + fatal
    print("-----------------------------------------------")
    print("    INFORMATION ON IMPORT LOG LEAKS FOLLOW     ")
    print("-----------------------------------------------")
    print(f"Analyzed a total of {totalNodes} nodes, out of them:")
    print(f"{trace} trace import statements were found ({(trace / totalNodes) * 100}%)")
    print(f"{debug} debug import statements were found ({(debug / totalNodes) * 100}%)")
    print(f"{info} info import statements were found ({(info / totalNodes) * 100}%)")
    print(f"{error} error import statements were found ({(error / totalNodes) * 100}%)")
    print(f"{fatal} fatal import statements were found ({(fatal / totalNodes) * 100}%)")
    print(f"Total of {totalImportDangers} potentially dangerous import statements found")
    print(f"Note: there are a total of {len(logs)} logs in the JSON file, meaning that "
          f"{totalImportDangers / len(logs) * 100}% could be at risk")

# Remnants of a debug era
def getWordInstances(nodes):
    debug = error = fatal = info = trace = warn = 0
    for node in nodes:
        if "debug" in node.contents:
            debug += 1
        if "error" in node.contents:
            error += 1
        if "fatal" in node.contents:
            fatal += 1
        if "info" in node.contents:
            info += 1
        if "trace" in node.contents:
            trace += 1
        if "warn" in node.contents:
            warn += 1
    return debug, error, fatal, info, trace, warn


if __name__ == "__main__":
    # use argparser to set up all argument parsing
    parser = argparse.ArgumentParser(
        description="Calculates various statistics for the generated corpus")
    parser.add_argument("input_json", help="Location of json file. Please use full path",
                        type=str)
    parser.add_argument("modified_corpus_location",
                        help="Root folder location of the corpus generated by nodeParsing.py")
    args = parser.parse_args()
    jsonLoc = Path(args.input_json)
    corpusLocation = Path(args.modified_corpus_location)
    main()
